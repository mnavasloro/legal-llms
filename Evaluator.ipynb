{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import pandas as pd\n",
    "from gatenlp import Document\n",
    "from gatenlp.corpora import ListCorpus\n",
    "from GatenlpUtils import loadCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Set your pipeline results folder here\n",
    "PIPELINE_RESULTS_FOLDER = \"output/pipeline_results_20250725_111753\"\n",
    "\n",
    "print(f\"📁 Using pipeline results folder: {PIPELINE_RESULTS_FOLDER}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a482b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Path Structure\n",
    "print(\"🔍 VERIFYING PATH STRUCTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check the pipeline results folder structure\n",
    "folder_path = Path(PIPELINE_RESULTS_FOLDER)\n",
    "print(f\"Target folder: {folder_path}\")\n",
    "print(f\"Folder exists: {folder_path.exists()}\")\n",
    "\n",
    "if folder_path.exists():\n",
    "    print(f\"\\n📁 Contents of {folder_path.name}:\")\n",
    "    \n",
    "    # Count different file types\n",
    "    json_files = list(folder_path.glob(\"*.json\"))\n",
    "    txt_files = list(folder_path.glob(\"*.txt\"))\n",
    "    csv_files = list(folder_path.glob(\"*.csv\"))\n",
    "    subdirs = [item for item in folder_path.iterdir() if item.is_dir()]\n",
    "    \n",
    "    print(f\"  📄 JSON files: {len(json_files)}\")\n",
    "    print(f\"  📋 TXT files: {len(txt_files)}\")\n",
    "    print(f\"  📊 CSV files: {len(csv_files)}\")\n",
    "    print(f\"  📁 Subdirectories: {len(subdirs)}\")\n",
    "    \n",
    "    # Show result JSON files (exclude system files)\n",
    "    result_jsons = [f for f in json_files if not f.name.startswith(\"pipeline_results_\")]\n",
    "    print(f\"\\n📊 Result JSON files ({len(result_jsons)}):\")\n",
    "    for json_file in result_jsons[:10]:  # Show first 10\n",
    "        print(f\"  📄 {json_file.name}\")\n",
    "    if len(result_jsons) > 10:\n",
    "        print(f\"  ... and {len(result_jsons) - 10} more\")\n",
    "    \n",
    "    # Show subdirectories\n",
    "    if subdirs:\n",
    "        print(f\"\\n📁 Subdirectories:\")\n",
    "        for subdir in subdirs:\n",
    "            print(f\"  📁 {subdir.name}\")\n",
    "            # Check if it's the annotated corpus folder\n",
    "            if \"annotated_corpus\" in subdir.name:\n",
    "                corpus_files = list(subdir.glob(\"*\"))\n",
    "                print(f\"    Contains {len(corpus_files)} files\")\n",
    "    \n",
    "    print(f\"\\n✅ Path structure looks correct!\")\n",
    "    print(f\"Expected structure:\")\n",
    "    print(f\"📁 {folder_path.name}/\")\n",
    "    print(f\"  📄 *.json (result files)\")\n",
    "    print(f\"  📋 *.txt (reports)\")\n",
    "    print(f\"  📊 *.csv (data)\")\n",
    "    print(f\"  📁 annotated_corpus_with_predictions/\")\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ Folder doesn't exist: {folder_path}\")\n",
    "    \n",
    "    # Check what's in the output directory\n",
    "    output_dir = Path(\"output\")\n",
    "    if output_dir.exists():\n",
    "        print(f\"\\n📁 Available folders in output/:\")\n",
    "        for item in output_dir.iterdir():\n",
    "            if item.is_dir() and \"pipeline\" in item.name:\n",
    "                print(f\"  📁 {item.name}\")\n",
    "    else:\n",
    "        print(\"❌ Output directory doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEventResultsEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates LLM event annotation results by comparing predictions against \n",
    "    gold standard consensus annotations in GateNLP documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"output\", corpus=None):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.corpus = corpus if corpus is not None else loadCorpus()\n",
    "        self.evaluation_results = []\n",
    "        self.runtime_results = []  # Track runtime per model and document\n",
    "        self.annotation_counts = []  # Track annotation counts per document and model\n",
    "        \n",
    "        # Event-related annotation types to evaluate based on consensus annotation set\n",
    "        self.event_annotation_types = [\n",
    "            \"Event\",      # General event annotation\n",
    "            \"Event_who\",  # Who annotation \n",
    "            \"Event_what\", # What annotation\n",
    "            \"Event_when\"  # When annotation\n",
    "        ]\n",
    "        \n",
    "    def find_result_folders(self) -> List[Path]:\n",
    "        \"\"\"Find all timestamped result folders in output directory.\"\"\"\n",
    "        folders = []\n",
    "        if self.output_dir.exists():\n",
    "            # Check if output_dir itself is a timestamped folder (direct path to pipeline_results_*)\n",
    "            if re.search(r'pipeline_results_\\d{8}_\\d{6}', self.output_dir.name):\n",
    "                # We're pointing directly to a timestamped folder\n",
    "                folders.append(self.output_dir)\n",
    "                print(f\"✅ Using direct pipeline results folder: {self.output_dir}\")\n",
    "            else:\n",
    "                # Look for timestamped folders within the output directory\n",
    "                for item in self.output_dir.iterdir():\n",
    "                    if item.is_dir() and re.search(r'pipeline_results_\\d{8}_\\d{6}', item.name):\n",
    "                        folders.append(item)\n",
    "                        print(f\"✅ Found pipeline results folder: {item}\")\n",
    "        else:\n",
    "            print(f\"❌ Output directory doesn't exist: {self.output_dir}\")\n",
    "        \n",
    "        print(f\"📊 Total pipeline result folders found: {len(folders)}\")\n",
    "        return sorted(folders)\n",
    "    \n",
    "    def find_result_jsons(self, folder: Path) -> List[Path]:\n",
    "        \"\"\"Find all result JSON files (excluding pipeline_results_*.json and other system files).\"\"\"\n",
    "        jsons = []\n",
    "        for json_file in folder.glob(\"*.json\"):\n",
    "            # Exclude system files like pipeline_results_*.json\n",
    "            if not json_file.name.startswith(\"pipeline_results_\"):\n",
    "                jsons.append(json_file)\n",
    "        \n",
    "        print(f\"📄 Found {len(jsons)} result JSON files in {folder.name}\")\n",
    "        for json_file in jsons:\n",
    "            print(f\"  📄 {json_file.name}\")\n",
    "        \n",
    "        return jsons\n",
    "    \n",
    "    def extract_doc_name_from_path(self, file_path: str) -> str:\n",
    "        \"\"\"Extract document name from file path for matching with corpus.\"\"\"\n",
    "        return Path(file_path).stem\n",
    "    \n",
    "    def find_corpus_document(self, doc_identifier: str) -> Document:\n",
    "        \"\"\"Find corresponding document in corpus.\"\"\"\n",
    "        for doc in self.corpus:\n",
    "            doc_name = doc.features.get(\"gate.SourceURL\", \"\")\n",
    "            if (doc_identifier in doc_name or \n",
    "                doc_name.endswith(f\"{doc_identifier}.xml\") or\n",
    "                Path(doc_name).stem == doc_identifier):\n",
    "                return doc\n",
    "        return None\n",
    "    \n",
    "    def parse_llm_event_predictions(self, result_data: Dict[str, Any]) -> Dict[str, Dict[str, List[Dict]]]:\n",
    "        \"\"\"\n",
    "        Parse LLM event predictions from result JSON.\n",
    "        Maps JSON events to consensus annotation types for proper comparison:\n",
    "        - \"event\" in JSON -> \"Event\" annotation with the event text as annotation span\n",
    "        - \"event_who\" in JSON -> \"Event_who\" annotation with the who text as annotation span\n",
    "        - \"event_what\" in JSON -> \"Event_what\" annotation with the what text as annotation span  \n",
    "        - \"event_when\" in JSON -> \"Event_when\" annotation with the when text as annotation span\n",
    "        - \"event_type\" in JSON -> type metadata of \"Event\" annotation (remove \"event_\" prefix)\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        # Check if annotations array exists\n",
    "        if \"annotations\" not in result_data:\n",
    "            print(\"No 'annotations' key found in result data\")\n",
    "            return predictions\n",
    "        \n",
    "        # Process each model's annotations\n",
    "        for model_annotation in result_data[\"annotations\"]:\n",
    "            if not isinstance(model_annotation, dict):\n",
    "                continue\n",
    "                \n",
    "            model_name = model_annotation.get(\"model_name\", \"unknown_model\")\n",
    "            predictions[model_name] = {}\n",
    "            \n",
    "            # Initialize all event annotation types\n",
    "            for ann_type in self.event_annotation_types:\n",
    "                predictions[model_name][ann_type] = []\n",
    "            \n",
    "            # Extract events from this model's results\n",
    "            if \"events\" in model_annotation:\n",
    "                events = model_annotation[\"events\"]\n",
    "                for event in events:\n",
    "                    if isinstance(event, dict):\n",
    "                        source_text = event.get(\"source_text\", \"\")\n",
    "                        \n",
    "                        # Map \"event\" field to \"Event\" annotation\n",
    "                        if \"event\" in event and event[\"event\"].strip():\n",
    "                            event_type = event.get(\"event_type\", \"\")\n",
    "                            # Remove \"event_\" prefix from event_type to match consensus type metadata\n",
    "                            if event_type.startswith(\"event_\"):\n",
    "                                event_type = event_type[6:]  # Remove \"event_\" prefix\n",
    "                            elif event_type.startswith(\"Event_\"):\n",
    "                                event_type = event_type[6:]  # Remove \"Event_\" prefix\n",
    "                            \n",
    "                            # Use the event text as the annotation span\n",
    "                            event_text = event[\"event\"].strip()\n",
    "                            ann_dict = {\n",
    "                                \"text\": event_text,  # The actual text to annotate\n",
    "                                \"start\": 0,  # Will be calculated from event text\n",
    "                                \"end\": 0,    # Will be calculated from event text\n",
    "                                \"features\": {\n",
    "                                    \"source_text\": source_text,\n",
    "                                    \"type\": event_type  # This will be compared to consensus Event type metadata\n",
    "                                }\n",
    "                            }\n",
    "                            predictions[model_name][\"Event\"].append(ann_dict)\n",
    "                        \n",
    "                        # Map \"event_who\" field to \"Event_who\" annotation\n",
    "                        if \"event_who\" in event and event[\"event_who\"].strip():\n",
    "                            who_text = event[\"event_who\"].strip()\n",
    "                            ann_dict = {\n",
    "                                \"text\": who_text,  # The actual text to annotate\n",
    "                                \"start\": 0,\n",
    "                                \"end\": 0,\n",
    "                                \"features\": {\n",
    "                                    \"source_text\": source_text\n",
    "                                }\n",
    "                            }\n",
    "                            predictions[model_name][\"Event_who\"].append(ann_dict)\n",
    "                        \n",
    "                        # Map \"event_what\" field to \"Event_what\" annotation\n",
    "                        if \"event_what\" in event and event[\"event_what\"].strip():\n",
    "                            what_text = event[\"event_what\"].strip()\n",
    "                            ann_dict = {\n",
    "                                \"text\": what_text,  # The actual text to annotate\n",
    "                                \"start\": 0,\n",
    "                                \"end\": 0,\n",
    "                                \"features\": {\n",
    "                                    \"source_text\": source_text\n",
    "                                }\n",
    "                            }\n",
    "                            predictions[model_name][\"Event_what\"].append(ann_dict)\n",
    "                        \n",
    "                        # Map \"event_when\" field to \"Event_when\" annotation\n",
    "                        if \"event_when\" in event and event[\"event_when\"].strip():\n",
    "                            when_text = event[\"event_when\"].strip()\n",
    "                            ann_dict = {\n",
    "                                \"text\": when_text,  # The actual text to annotate\n",
    "                                \"start\": 0,\n",
    "                                \"end\": 0,\n",
    "                                \"features\": {\n",
    "                                    \"source_text\": source_text\n",
    "                                }\n",
    "                            }\n",
    "                            predictions[model_name][\"Event_when\"].append(ann_dict)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def extract_runtime_info(self, result_data: Dict[str, Any], doc_name: str):\n",
    "        \"\"\"Extract runtime information from result JSON.\"\"\"\n",
    "        if \"annotations\" not in result_data:\n",
    "            return\n",
    "        \n",
    "        # Process each model's runtime\n",
    "        for model_annotation in result_data[\"annotations\"]:\n",
    "            if not isinstance(model_annotation, dict):\n",
    "                continue\n",
    "                \n",
    "            model_name = model_annotation.get(\"model_name\", \"unknown_model\")\n",
    "            runtime_seconds = model_annotation.get(\"runtime_seconds\", 0)\n",
    "            \n",
    "            # Store runtime information\n",
    "            runtime_info = {\n",
    "                \"document\": doc_name,\n",
    "                \"model\": model_name,\n",
    "                \"runtime_seconds\": runtime_seconds,\n",
    "                \"runtime_minutes\": runtime_seconds / 60 if runtime_seconds else 0\n",
    "            }\n",
    "            self.runtime_results.append(runtime_info)\n",
    "    \n",
    "    def collect_annotation_counts(self, doc: Document, doc_name: str):\n",
    "        \"\"\"Collect annotation counts for consensus and all model prediction sets.\"\"\"\n",
    "        \n",
    "        # Get consensus annotation counts\n",
    "        consensus_annset = doc.annset(\"consensus\")\n",
    "        consensus_counts = {\n",
    "            \"document\": doc_name,\n",
    "            \"annotation_set\": \"consensus\",\n",
    "            \"model\": \"consensus\"\n",
    "        }\n",
    "        \n",
    "        for ann_type in self.event_annotation_types:\n",
    "            ann_count = len(list(consensus_annset.with_type(ann_type)))\n",
    "            consensus_counts[ann_type] = ann_count\n",
    "        \n",
    "        self.annotation_counts.append(consensus_counts)\n",
    "        \n",
    "        # Get model prediction annotation counts\n",
    "        pred_annset_names = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "        \n",
    "        for annset_name in pred_annset_names:\n",
    "            model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "            pred_annset = doc.annset(annset_name)\n",
    "            \n",
    "            model_counts = {\n",
    "                \"document\": doc_name,\n",
    "                \"annotation_set\": annset_name,\n",
    "                \"model\": model_name\n",
    "            }\n",
    "            \n",
    "            for ann_type in self.event_annotation_types:\n",
    "                ann_count = len(list(pred_annset.with_type(ann_type)))\n",
    "                model_counts[ann_type] = ann_count\n",
    "            \n",
    "            self.annotation_counts.append(model_counts)\n",
    "    \n",
    "    def calculate_text_positions(self, target_text: str, document_text: str) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Calculate start and end positions of target_text within document_text.\n",
    "        Returns: (start, end) positions or (0, 0) if not found.\n",
    "        \"\"\"\n",
    "        if not target_text or not document_text:\n",
    "            return (0, 0)\n",
    "        \n",
    "        # Clean up the target text for better matching\n",
    "        cleaned_target = target_text.strip()\n",
    "        \n",
    "        # Try to find the text in the document (case-insensitive)\n",
    "        start_pos = document_text.lower().find(cleaned_target.lower())\n",
    "        if start_pos != -1:\n",
    "            end_pos = start_pos + len(cleaned_target)\n",
    "            return (start_pos, end_pos)\n",
    "        \n",
    "        # If exact match fails, try with normalized whitespace\n",
    "        import re\n",
    "        normalized_target = re.sub(r'\\s+', ' ', cleaned_target)\n",
    "        normalized_doc = re.sub(r'\\s+', ' ', document_text)\n",
    "        \n",
    "        start_pos = normalized_doc.lower().find(normalized_target.lower())\n",
    "        if start_pos != -1:\n",
    "            # Find the actual positions in the original text\n",
    "            # This is a simplified approach - might need refinement\n",
    "            end_pos = start_pos + len(normalized_target)\n",
    "            return (start_pos, end_pos)\n",
    "        \n",
    "        # If still not found, try partial matching with the first few words\n",
    "        words = cleaned_target.split()\n",
    "        if len(words) > 1:\n",
    "            # Try with first 3 words\n",
    "            partial_text = \" \".join(words[:min(3, len(words))])\n",
    "            start_pos = document_text.lower().find(partial_text.lower())\n",
    "            if start_pos != -1:\n",
    "                end_pos = start_pos + len(partial_text)\n",
    "                return (start_pos, end_pos)\n",
    "        \n",
    "        return (0, 0)\n",
    "    \n",
    "    def extract_annotations_from_response(self, response_text: str) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Extract event annotations from LLM response text.\n",
    "        Adjust this method based on your actual LLM response format.\n",
    "        \"\"\"\n",
    "        annotations = {}\n",
    "        for ann_type in self.event_annotation_types:\n",
    "            annotations[ann_type] = []\n",
    "        \n",
    "        # Example parsing - adjust based on your actual response format\n",
    "        # This assumes annotations are in JSON format within the response\n",
    "        try:\n",
    "            # Try to find JSON blocks in the response\n",
    "            json_matches = re.findall(r'\\{.*?\\}', response_text, re.DOTALL)\n",
    "            for json_str in json_matches:\n",
    "                try:\n",
    "                    parsed_json = json.loads(json_str)\n",
    "                    if isinstance(parsed_json, dict):\n",
    "                        for ann_type in self.event_annotation_types:\n",
    "                            if ann_type in parsed_json:\n",
    "                                annotations[ann_type].extend(parsed_json[ann_type])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing annotations from response: {e}\")\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    def add_prediction_annotations(self, doc: Document, model_name: str, \n",
    "                                predictions: Dict[str, List[Dict]]):\n",
    "        \"\"\"Add prediction annotations to document as new annotation set.\"\"\"\n",
    "        # Create BOTH the temporary _predictions set (for evaluation) AND the clean set (for viewing)\n",
    "        \n",
    "        # 1. Create temporary prediction set for evaluation\n",
    "        temp_annset_name = f\"{model_name}_predictions\"\n",
    "        temp_annset = doc.annset(temp_annset_name)\n",
    "        temp_annset.clear()\n",
    "        \n",
    "        # 2. Create clean permanent set for viewing/saving\n",
    "        clean_annset = doc.annset(model_name)\n",
    "        clean_annset.clear()\n",
    "        \n",
    "        # Add predicted event annotations to BOTH sets\n",
    "        for ann_type, ann_list in predictions.items():\n",
    "            for ann_dict in ann_list:\n",
    "                try:\n",
    "                    # Extract the target text and features from annotation dict\n",
    "                    target_text = ann_dict.get(\"text\", \"\")\n",
    "                    start = ann_dict.get(\"start\", 0)\n",
    "                    end = ann_dict.get(\"end\", 0)\n",
    "                    features = ann_dict.get(\"features\", {})\n",
    "                    \n",
    "                    # If start/end positions are not provided or are 0, calculate from target text\n",
    "                    if (start == 0 and end == 0) and target_text:\n",
    "                        start, end = self.calculate_text_positions(target_text, doc.text)\n",
    "                    \n",
    "                    # Only add annotation if we found valid positions\n",
    "                    if start < end:\n",
    "                        # Ensure boundaries are within document bounds\n",
    "                        start = max(0, min(start, len(doc.text)))\n",
    "                        end = max(start, min(end, len(doc.text)))\n",
    "                        \n",
    "                        # Features for temporary set (with extra metadata)\n",
    "                        temp_features = dict(features)\n",
    "                        temp_features.update({\n",
    "                            \"source\": \"llm_prediction\",\n",
    "                            \"model\": model_name\n",
    "                        })\n",
    "                        \n",
    "                        # Features for clean set (without extra metadata)\n",
    "                        clean_features = dict(features)\n",
    "                        clean_features.pop(\"source\", None)\n",
    "                        clean_features.pop(\"model\", None)\n",
    "                        \n",
    "                        # Add to both sets\n",
    "                        temp_annset.add(start, end, ann_type, temp_features)\n",
    "                        clean_annset.add(start, end, ann_type, clean_features)\n",
    "                        \n",
    "                        print(f\"    Added {ann_type} annotation: '{doc.text[start:end]}' at {start}-{end}\")\n",
    "                    else:\n",
    "                        print(f\"    Skipped {ann_type} annotation: text '{target_text}' not found in document\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error adding annotation {ann_dict}: {e}\")\n",
    "    \n",
    "    def calculate_annotation_overlap_metrics(self, gold_annset, pred_annset, ann_type: str, document: Document = None) -> Dict[str, float]:\n",
    "        \"\"\"Calculate precision, recall, F1 for a specific annotation type.\"\"\"\n",
    "        gold_anns = list(gold_annset.with_type(ann_type))\n",
    "        pred_anns = list(pred_annset.with_type(ann_type))\n",
    "        \n",
    "        if not gold_anns and not pred_anns:\n",
    "            return {\"precision\": 1.0, \"recall\": 1.0, \"f1\": 1.0, \"gold_count\": 0, \"pred_count\": 0}\n",
    "        \n",
    "        if not pred_anns:\n",
    "            return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"gold_count\": len(gold_anns), \"pred_count\": 0}\n",
    "        \n",
    "        if not gold_anns:\n",
    "            return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"gold_count\": 0, \"pred_count\": len(pred_anns)}\n",
    "        \n",
    "        # Get the document text to extract annotation text\n",
    "        # Use the document parameter or try to get it from annotation set\n",
    "        if document is None:\n",
    "            # Try to get document from annotation set's owner\n",
    "            document = getattr(gold_annset, '_owner', None)\n",
    "            if document is None:\n",
    "                print(f\"Warning: Could not access document for annotation comparison\")\n",
    "                return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"gold_count\": len(gold_anns), \"pred_count\": len(pred_anns)}\n",
    "        \n",
    "        # For Event annotations, consider both position and type metadata matching\n",
    "        if ann_type == \"Event\":\n",
    "            matches = 0\n",
    "            matched_gold = set()\n",
    "            matched_pred = set()\n",
    "            \n",
    "            for i, pred_ann in enumerate(pred_anns):\n",
    "                pred_type = pred_ann.features.get(\"type\", \"\")\n",
    "                pred_text = document.text[pred_ann.start:pred_ann.end].strip().lower()\n",
    "                \n",
    "                for j, gold_ann in enumerate(gold_anns):\n",
    "                    if j in matched_gold:\n",
    "                        continue\n",
    "                        \n",
    "                    gold_type = gold_ann.features.get(\"type\", \"\")\n",
    "                    gold_text = document.text[gold_ann.start:gold_ann.end].strip().lower()\n",
    "                    \n",
    "                    # Check for text similarity and type matching\n",
    "                    text_match = (pred_text == gold_text or \n",
    "                                pred_text in gold_text or \n",
    "                                gold_text in pred_text or\n",
    "                                self.calculate_text_similarity(pred_text, gold_text) > 0.7)\n",
    "                    \n",
    "                    type_match = (pred_type == gold_type or \n",
    "                                (not pred_type and not gold_type))\n",
    "                    \n",
    "                    if text_match and type_match:\n",
    "                        matches += 1\n",
    "                        matched_gold.add(j)\n",
    "                        matched_pred.add(i)\n",
    "                        break\n",
    "            \n",
    "            precision = matches / len(pred_anns) if pred_anns else 0\n",
    "            recall = matches / len(gold_anns) if gold_anns else 0\n",
    "            \n",
    "        else:\n",
    "            # For other annotation types (Event_who, Event_what, Event_when), use text similarity\n",
    "            matches = 0\n",
    "            matched_gold = set()\n",
    "            \n",
    "            for i, pred_ann in enumerate(pred_anns):\n",
    "                pred_text = document.text[pred_ann.start:pred_ann.end].strip().lower()\n",
    "                \n",
    "                for j, gold_ann in enumerate(gold_anns):\n",
    "                    if j in matched_gold:\n",
    "                        continue\n",
    "                        \n",
    "                    gold_text = document.text[gold_ann.start:gold_ann.end].strip().lower()\n",
    "                    \n",
    "                    # Check for text similarity\n",
    "                    text_match = (pred_text == gold_text or \n",
    "                                pred_text in gold_text or \n",
    "                                gold_text in pred_text or\n",
    "                                self.calculate_text_similarity(pred_text, gold_text) > 0.7)\n",
    "                    \n",
    "                    if text_match:\n",
    "                        matches += 1\n",
    "                        matched_gold.add(j)\n",
    "                        break\n",
    "            \n",
    "            precision = matches / len(pred_anns) if pred_anns else 0\n",
    "            recall = matches / len(gold_anns) if gold_anns else 0\n",
    "        \n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"gold_count\": len(gold_anns),\n",
    "            \"pred_count\": len(pred_anns),\n",
    "            \"matches\": matches\n",
    "        }\n",
    "    \n",
    "    def calculate_text_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate simple text similarity score between two strings.\"\"\"\n",
    "        if not text1 or not text2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Simple word-based similarity\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "    \n",
    "    def evaluate_document(self, doc: Document, doc_name: str, result_folder: str) -> List[Dict]:\n",
    "        \"\"\"Evaluate all model predictions for a single document against consensus annotations.\"\"\"\n",
    "        gold_annset = doc.annset(\"consensus\")  # Use consensus annotation set as gold standard\n",
    "        \n",
    "        if not gold_annset:\n",
    "            print(f\"Warning: No 'consensus' annotation set found in document {doc_name}\")\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Get all prediction annotation sets\n",
    "        pred_annset_names = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "        \n",
    "        for annset_name in pred_annset_names:\n",
    "            model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "            pred_annset = doc.annset(annset_name)\n",
    "            \n",
    "            # Evaluate each event annotation type\n",
    "            for ann_type in self.event_annotation_types:\n",
    "                metrics = self.calculate_annotation_overlap_metrics(gold_annset, pred_annset, ann_type, doc)\n",
    "                \n",
    "                result = {\n",
    "                    \"result_folder\": result_folder,\n",
    "                    \"document\": doc_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"annotation_type\": ann_type,\n",
    "                    **metrics\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_result_json(self, json_path: Path, result_folder: str):\n",
    "        \"\"\"Process a single result JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                result_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {json_path}: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Extract document identifier\n",
    "        doc_path = result_data.get(\"Document\", \"\")\n",
    "        doc_name = self.extract_doc_name_from_path(doc_path)\n",
    "        \n",
    "        # Find corresponding corpus document\n",
    "        corpus_doc = self.find_corpus_document(doc_name)\n",
    "        if corpus_doc is None:\n",
    "            print(f\"Warning: Could not find corpus document for {doc_name}\")\n",
    "            return\n",
    "        \n",
    "        # Parse LLM event predictions\n",
    "        predictions = self.parse_llm_event_predictions(result_data)\n",
    "        \n",
    "        # Extract runtime information\n",
    "        self.extract_runtime_info(result_data, doc_name)\n",
    "        \n",
    "        if not predictions:\n",
    "            print(f\"Warning: No predictions found in {json_path.name}\")\n",
    "            return\n",
    "        \n",
    "        # Debug: Print what we found\n",
    "        print(f\"Found predictions for models: {list(predictions.keys())}\")\n",
    "        for model_name, model_preds in predictions.items():\n",
    "            total_events = sum(len(events) for events in model_preds.values())\n",
    "            print(f\"  {model_name}: {total_events} total events\")\n",
    "            for ann_type, events in model_preds.items():\n",
    "                if events:\n",
    "                    print(f\"    {ann_type}: {len(events)} events\")\n",
    "        \n",
    "        # Add prediction annotations to document\n",
    "        for model_name, model_predictions in predictions.items():\n",
    "            self.add_prediction_annotations(corpus_doc, model_name, model_predictions)\n",
    "        \n",
    "        # Collect annotation counts after adding predictions\n",
    "        self.collect_annotation_counts(corpus_doc, doc_name)\n",
    "        \n",
    "        # Evaluate predictions against consensus annotations\n",
    "        doc_results = self.evaluate_document(corpus_doc, doc_name, result_folder)\n",
    "        self.evaluation_results.extend(doc_results)\n",
    "        \n",
    "        print(f\"Processed {json_path.name}: {len(predictions)} models, {len(doc_results)} evaluations\")\n",
    "    \n",
    "    def run_evaluation(self) -> pd.DataFrame:\n",
    "        \"\"\"Run complete evaluation on all result folders.\"\"\"\n",
    "        result_folders = self.find_result_folders()\n",
    "        \n",
    "        if not result_folders:\n",
    "            print(\"No result folders found in output directory\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Found {len(result_folders)} result folders\")\n",
    "        \n",
    "        for folder in result_folders:\n",
    "            print(f\"\\nProcessing folder: {folder.name}\")\n",
    "            result_jsons = self.find_result_jsons(folder)\n",
    "            \n",
    "            for json_path in result_jsons:\n",
    "                self.process_result_json(json_path, folder.name)\n",
    "        \n",
    "        # Convert results to DataFrame\n",
    "        results = {}\n",
    "        if self.evaluation_results:\n",
    "            results['evaluation'] = pd.DataFrame(self.evaluation_results)\n",
    "        else:\n",
    "            results['evaluation'] = pd.DataFrame()\n",
    "        \n",
    "        if self.runtime_results:\n",
    "            results['runtime'] = pd.DataFrame(self.runtime_results)\n",
    "        else:\n",
    "            results['runtime'] = pd.DataFrame()\n",
    "            \n",
    "        if self.annotation_counts:\n",
    "            results['annotation_counts'] = pd.DataFrame(self.annotation_counts)\n",
    "        else:\n",
    "            results['annotation_counts'] = pd.DataFrame()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_summary_report(self, results_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate summary report of evaluation results.\"\"\"\n",
    "        if results_df.empty:\n",
    "            return \"No results to summarize\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"LLM Event Annotation Evaluation Summary\")\n",
    "        report.append(\"=\" * 60)\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_evaluations = len(results_df)\n",
    "        unique_models = results_df['model'].nunique()\n",
    "        unique_docs = results_df['document'].nunique()\n",
    "        unique_ann_types = results_df['annotation_type'].nunique()\n",
    "        \n",
    "        report.append(f\"Total evaluations: {total_evaluations}\")\n",
    "        report.append(f\"Unique models: {unique_models}\")\n",
    "        report.append(f\"Unique documents: {unique_docs}\")\n",
    "        report.append(f\"Annotation types evaluated: {unique_ann_types}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Average F1 by model\n",
    "        report.append(\"Average F1 Scores by Model:\")\n",
    "        model_f1 = results_df.groupby('model')['f1'].mean().sort_values(ascending=False)\n",
    "        for model, f1 in model_f1.items():\n",
    "            report.append(f\"  {model}: {f1:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Average F1 by annotation type\n",
    "        report.append(\"Average F1 Scores by Annotation Type:\")\n",
    "        ann_type_f1 = results_df.groupby('annotation_type')['f1'].mean().sort_values(ascending=False)\n",
    "        for ann_type, f1 in ann_type_f1.items():\n",
    "            report.append(f\"  {ann_type}: {f1:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Best performing model-annotation combinations\n",
    "        report.append(\"Best Model-Annotation Combinations (Top 10 F1):\")\n",
    "        best_combinations = results_df.groupby(['model', 'annotation_type'])['f1'].mean().sort_values(ascending=False).head(10)\n",
    "        for (model, ann_type), f1 in best_combinations.items():\n",
    "            report.append(f\"  {model} - {ann_type}: {f1:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Detailed precision/recall breakdown\n",
    "        report.append(\"Detailed Metrics by Annotation Type:\")\n",
    "        for ann_type in results_df['annotation_type'].unique():\n",
    "            ann_data = results_df[results_df['annotation_type'] == ann_type]\n",
    "            avg_precision = ann_data['precision'].mean()\n",
    "            avg_recall = ann_data['recall'].mean()\n",
    "            avg_f1 = ann_data['f1'].mean()\n",
    "            total_gold = ann_data['gold_count'].sum()\n",
    "            total_pred = ann_data['pred_count'].sum()\n",
    "            \n",
    "            report.append(f\"  {ann_type}:\")\n",
    "            report.append(f\"    Precision: {avg_precision:.3f}\")\n",
    "            report.append(f\"    Recall: {avg_recall:.3f}\")\n",
    "            report.append(f\"    F1: {avg_f1:.3f}\")\n",
    "            report.append(f\"    Total gold annotations: {total_gold}\")\n",
    "            report.append(f\"    Total predicted annotations: {total_pred}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "    \n",
    "    def generate_runtime_report(self, runtime_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate runtime analysis report.\"\"\"\n",
    "        if runtime_df.empty:\n",
    "            return \"No runtime data available\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"LLM Runtime Analysis\")\n",
    "        report.append(\"=\" * 40)\n",
    "        \n",
    "        # Overall runtime statistics\n",
    "        total_runtime = runtime_df['runtime_seconds'].sum()\n",
    "        avg_runtime = runtime_df['runtime_seconds'].mean()\n",
    "        \n",
    "        report.append(f\"Total processing time: {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
    "        report.append(f\"Average processing time per document: {avg_runtime:.2f} seconds\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Runtime by model\n",
    "        report.append(\"Average Runtime by Model:\")\n",
    "        model_runtime = runtime_df.groupby('model')['runtime_seconds'].agg(['mean', 'std', 'min', 'max']).round(2)\n",
    "        for model, stats in model_runtime.iterrows():\n",
    "            report.append(f\"  {model}:\")\n",
    "            report.append(f\"    Mean: {stats['mean']:.2f}s\")\n",
    "            report.append(f\"    Std:  {stats['std']:.2f}s\")\n",
    "            report.append(f\"    Min:  {stats['min']:.2f}s\")\n",
    "            report.append(f\"    Max:  {stats['max']:.2f}s\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Runtime by document\n",
    "        report.append(\"Runtime by Document:\")\n",
    "        doc_runtime = runtime_df.groupby('document')['runtime_seconds'].sum().sort_values(ascending=False)\n",
    "        for doc, runtime in doc_runtime.items():\n",
    "            report.append(f\"  {doc}: {runtime:.2f}s ({runtime/60:.2f}m)\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "    \n",
    "    def generate_annotation_counts_report(self, counts_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate annotation counts comparison report.\"\"\"\n",
    "        if counts_df.empty:\n",
    "            return \"No annotation count data available\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"Annotation Counts Comparison\")\n",
    "        report.append(\"=\" * 50)\n",
    "        \n",
    "        # Get unique documents\n",
    "        documents = counts_df['document'].unique()\n",
    "        \n",
    "        for doc in sorted(documents):\n",
    "            report.append(f\"\\nDocument: {doc}\")\n",
    "            report.append(\"-\" * (len(doc) + 10))\n",
    "            \n",
    "            doc_data = counts_df[counts_df['document'] == doc]\n",
    "            \n",
    "            # Create a comparison table\n",
    "            comparison_data = []\n",
    "            for ann_type in self.event_annotation_types:\n",
    "                row = {'annotation_type': ann_type}\n",
    "                for _, row_data in doc_data.iterrows():\n",
    "                    model = row_data['model']\n",
    "                    count = row_data[ann_type]\n",
    "                    row[model] = count\n",
    "                comparison_data.append(row)\n",
    "            \n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            if not comparison_df.empty:\n",
    "                comparison_df = comparison_df.set_index('annotation_type')\n",
    "                report.append(comparison_df.to_string())\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Overall summary\n",
    "        report.append(\"Overall Annotation Count Summary:\")\n",
    "        report.append(\"-\" * 40)\n",
    "        \n",
    "        for ann_type in self.event_annotation_types:\n",
    "            report.append(f\"\\n{ann_type}:\")\n",
    "            ann_summary = counts_df.groupby('model')[ann_type].agg(['sum', 'mean', 'std']).round(2)\n",
    "            for model, stats in ann_summary.iterrows():\n",
    "                report.append(f\"  {model}: Total={int(stats['sum'])}, Avg={stats['mean']:.1f}, Std={stats['std']:.1f}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"Starting LLM Event Annotation Results Evaluation...\")\n",
    "    \n",
    "    # Use the globally defined pipeline results folder\n",
    "    evaluator = LLMEventResultsEvaluator(output_dir=PIPELINE_RESULTS_FOLDER)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.run_evaluation()\n",
    "    \n",
    "    results_df = results['evaluation']\n",
    "    runtime_df = results['runtime']\n",
    "    counts_df = results['annotation_counts']\n",
    "    \n",
    "    runtime_dir = Path(f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis\")\n",
    "    runtime_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not results_df.empty:\n",
    "        # Save evaluation results\n",
    "        output_path = f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_event_evaluation_results.csv\"\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nEvaluation results saved to: {output_path}\")\n",
    "        \n",
    "        # Save runtime results\n",
    "        if not runtime_df.empty:\n",
    "            runtime_path = f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_runtime_analysis.csv\"\n",
    "            runtime_df.to_csv(runtime_path, index=False)\n",
    "            print(f\"Runtime analysis saved to: {runtime_path}\")\n",
    "        \n",
    "        # Save annotation counts\n",
    "        if not counts_df.empty:\n",
    "            counts_path = f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_annotation_counts.csv\"\n",
    "            counts_df.to_csv(counts_path, index=False)\n",
    "            print(f\"Annotation counts saved to: {counts_path}\")\n",
    "        \n",
    "        # Generate and save summary report\n",
    "        summary = evaluator.generate_summary_report(results_df)\n",
    "        summary_path = f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_event_evaluation_summary.txt\"\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(summary)\n",
    "        print(f\"Summary report saved to: {summary_path}\")\n",
    "        \n",
    "        # Generate and save runtime report\n",
    "        if not runtime_df.empty:\n",
    "            runtime_report = evaluator.generate_runtime_report(runtime_df)\n",
    "            runtime_report_path = f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_runtime_report.txt\"\n",
    "            with open(runtime_report_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(runtime_report)\n",
    "            print(f\"Runtime report saved to: {runtime_report_path}\")\n",
    "        \n",
    "        # Generate and save annotation counts report\n",
    "        if not counts_df.empty:\n",
    "            counts_report = evaluator.generate_annotation_counts_report(counts_df)\n",
    "            counts_report_path = f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_annotation_counts_report.txt\"\n",
    "            with open(counts_report_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(counts_report)\n",
    "            print(f\"Annotation counts report saved to: {counts_report_path}\")\n",
    "        \n",
    "        # Print summary to console\n",
    "        print(\"\\n\" + summary)\n",
    "        \n",
    "        # Print runtime summary if available\n",
    "        if not runtime_df.empty:\n",
    "            print(\"\\n\" + runtime_report)\n",
    "        \n",
    "        # Print annotation counts summary if available\n",
    "        if not counts_df.empty:\n",
    "            print(\"\\n\" + counts_report)\n",
    "        \n",
    "        # Display some basic statistics\n",
    "        print(\"\\nDetailed Results Preview:\")\n",
    "        print(results_df.head(15).to_string(index=False))\n",
    "        \n",
    "    else:\n",
    "        print(\"No results generated. Check if result folders and JSON files exist.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9afde6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus_with_annotations(self):\n",
    "    \"\"\"Save the corpus with all the new model annotations.\"\"\"\n",
    "    try:\n",
    "        from gatenlp.corpora import DirFilesDestination\n",
    "        \n",
    "        # Create output directory for annotated corpus\n",
    "        output_corpus_dir = Path(f\"{PIPELINE_RESULTS_FOLDER}/annotated_corpus_with_predictions\")\n",
    "        output_corpus_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n💾 Saving annotated corpus to: {output_corpus_dir}\")\n",
    "        \n",
    "        # Use GateNLP's DirFilesDestination to properly save the corpus\n",
    "        with DirFilesDestination(str(output_corpus_dir), ext=\"xml\", fmt=\"gatexml\") as dest:\n",
    "            for doc in self.corpus:\n",
    "                doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "                \n",
    "                # Set a filename for the document based on the original name\n",
    "                # This will override the default path generation\n",
    "                doc.features[\"_relpath\"] = f\"{doc_name}.xml\"\n",
    "                \n",
    "                dest.append(doc)\n",
    "                \n",
    "                # Print summary of annotation sets for this document\n",
    "                annset_summary = []\n",
    "                for annset_name in doc.annset_names():\n",
    "                    if annset_name and not annset_name.endswith(\"_predictions\"):  # Skip temporary sets\n",
    "                        ann_count = len(doc.annset(annset_name))\n",
    "                        if ann_count > 0:\n",
    "                            annset_summary.append(f\"{annset_name}({ann_count})\")\n",
    "                \n",
    "                if annset_summary:\n",
    "                    print(f\"  {doc_name}.xml: {', '.join(annset_summary)}\")\n",
    "        \n",
    "        # Count saved files\n",
    "        saved_files = list(output_corpus_dir.glob(\"*.xml\"))\n",
    "        print(f\"✅ Saved {len(saved_files)} annotated documents\")\n",
    "        \n",
    "        # Create a summary file\n",
    "        summary_file = output_corpus_dir / \"annotation_summary.txt\"\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"Annotated Corpus Summary\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Generated on: {pd.Timestamp.now()}\\n\")\n",
    "            f.write(f\"Total documents: {len(saved_files)}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Annotation Sets Added:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            \n",
    "            # Get all unique annotation set names across all documents\n",
    "            all_annsets = set()\n",
    "            for doc in self.corpus:\n",
    "                for annset_name in doc.annset_names():\n",
    "                    if annset_name and not annset_name.endswith(\"_predictions\"):\n",
    "                        all_annsets.add(annset_name)\n",
    "            \n",
    "            for annset_name in sorted(all_annsets):\n",
    "                if annset_name not in [\"consensus\", \"\"]:  # Skip gold standard and default\n",
    "                    f.write(f\"- {annset_name} (LLM predictions)\\n\")\n",
    "            \n",
    "            f.write(f\"\\nFiles saved to: {output_corpus_dir}\\n\")\n",
    "            f.write(\"\\nTo view in Gate:\\n\")\n",
    "            f.write(\"1. Open Gate Developer\\n\")\n",
    "            f.write(\"2. Load documents from this directory\\n\")\n",
    "            f.write(\"3. View different annotation sets in the annotation sets panel\\n\")\n",
    "        \n",
    "        return str(output_corpus_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving corpus: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Analysis and Better Results Overview\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def create_visual_analysis(results_df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive visual analysis of the evaluation results.\"\"\"\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create a large figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Heatmap: F1 scores by Model and Annotation Type\n",
    "    plt.subplot(3, 3, 1)\n",
    "    pivot_f1 = results_df.pivot_table(values='f1', index='model', columns='annotation_type', aggfunc='mean')\n",
    "    sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='RdYlGn', center=0.5, \n",
    "                cbar_kws={'label': 'F1 Score'})\n",
    "    plt.title('F1 Scores by Model and Annotation Type')\n",
    "    plt.xlabel('Annotation Type')\n",
    "    plt.ylabel('Model')\n",
    "    \n",
    "    # 2. Bar plot: Average F1 by Model\n",
    "    plt.subplot(3, 3, 2)\n",
    "    model_f1 = results_df.groupby('model')['f1'].mean().sort_values(ascending=True)\n",
    "    model_f1.plot(kind='barh', color='skyblue')\n",
    "    plt.title('Average F1 Score by Model')\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. Bar plot: Average F1 by Annotation Type\n",
    "    plt.subplot(3, 3, 3)\n",
    "    ann_f1 = results_df.groupby('annotation_type')['f1'].mean().sort_values(ascending=True)\n",
    "    ann_f1.plot(kind='barh', color='lightcoral')\n",
    "    plt.title('Average F1 Score by Annotation Type')\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 4. Precision vs Recall scatter plot\n",
    "    plt.subplot(3, 3, 4)\n",
    "    for model in results_df['model'].unique():\n",
    "        model_data = results_df[results_df['model'] == model]\n",
    "        plt.scatter(model_data['recall'], model_data['precision'], \n",
    "                   label=model, alpha=0.7, s=60)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision vs Recall by Model')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 5. Distribution of F1 scores\n",
    "    plt.subplot(3, 3, 5)\n",
    "    results_df.boxplot(column='f1', by='model', ax=plt.gca())\n",
    "    plt.title('F1 Score Distribution by Model')\n",
    "    plt.suptitle('')  # Remove the default title\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('F1 Score')\n",
    "    \n",
    "    # 6. Document-level performance heatmap\n",
    "    plt.subplot(3, 3, 6)\n",
    "    doc_model_f1 = results_df.groupby(['document', 'model'])['f1'].mean().reset_index()\n",
    "    doc_model_pivot = doc_model_f1.pivot(index='document', columns='model', values='f1')\n",
    "    sns.heatmap(doc_model_pivot, annot=True, fmt='.2f', cmap='RdYlGn', center=0.3,\n",
    "                cbar_kws={'label': 'F1 Score'})\n",
    "    plt.title('F1 Scores by Document and Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Document')\n",
    "    \n",
    "    # 7. Gold vs Predicted annotations count\n",
    "    plt.subplot(3, 3, 7)\n",
    "    total_gold = results_df.groupby('model')['gold_count'].sum()\n",
    "    total_pred = results_df.groupby('model')['pred_count'].sum()\n",
    "    \n",
    "    x = np.arange(len(total_gold))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, total_gold.values, width, label='Gold Standard', color='gold', alpha=0.8)\n",
    "    plt.bar(x + width/2, total_pred.values, width, label='Predicted', color='steelblue', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Total Annotations')\n",
    "    plt.title('Gold vs Predicted Annotation Counts')\n",
    "    plt.xticks(x, total_gold.index, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 8. Annotation type performance comparison\n",
    "    plt.subplot(3, 3, 8)\n",
    "    ann_metrics = results_df.groupby('annotation_type')[['precision', 'recall', 'f1']].mean()\n",
    "    ann_metrics.plot(kind='bar', ax=plt.gca(), color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "    plt.title('Average Metrics by Annotation Type')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 9. Model performance ranking\n",
    "    plt.subplot(3, 3, 9)\n",
    "    model_ranking = results_df.groupby('model')[['precision', 'recall', 'f1']].mean().sort_values('f1', ascending=False)\n",
    "    model_ranking.plot(kind='bar', ax=plt.gca(), color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "    plt.title('Model Performance Ranking')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_detailed_results_table(results_df: pd.DataFrame):\n",
    "    \"\"\"Create a detailed results table with better formatting.\"\"\"\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"No results to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"DETAILED EVALUATION RESULTS\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    # Summary by model\n",
    "    print(\"\\n📊 SUMMARY BY MODEL:\")\n",
    "    print(\"-\" * 80)\n",
    "    model_summary = results_df.groupby('model').agg({\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'gold_count': 'sum',\n",
    "        'pred_count': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    # Flatten column names\n",
    "    model_summary.columns = ['_'.join(col).strip() for col in model_summary.columns.values]\n",
    "    print(model_summary.to_string())\n",
    "    \n",
    "    # Summary by annotation type\n",
    "    print(\"\\n\\n📋 SUMMARY BY ANNOTATION TYPE:\")\n",
    "    print(\"-\" * 80)\n",
    "    ann_summary = results_df.groupby('annotation_type').agg({\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'], \n",
    "        'f1': ['mean', 'std'],\n",
    "        'gold_count': 'sum',\n",
    "        'pred_count': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    ann_summary.columns = ['_'.join(col).strip() for col in ann_summary.columns.values]\n",
    "    print(ann_summary.to_string())\n",
    "    \n",
    "    # Best and worst performers\n",
    "    print(\"\\n\\n🏆 TOP PERFORMERS:\")\n",
    "    print(\"-\" * 50)\n",
    "    top_performers = results_df.nlargest(10, 'f1')[['model', 'annotation_type', 'document', 'f1', 'precision', 'recall']]\n",
    "    print(top_performers.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\\n⚠️  LOWEST PERFORMERS:\")\n",
    "    print(\"-\" * 50)\n",
    "    low_performers = results_df.nsmallest(10, 'f1')[['model', 'annotation_type', 'document', 'f1', 'precision', 'recall']]\n",
    "    print(low_performers.to_string(index=False))\n",
    "    \n",
    "    # Document-level analysis\n",
    "    print(\"\\n\\n📄 DOCUMENT-LEVEL ANALYSIS:\")\n",
    "    print(\"-\" * 80)\n",
    "    doc_analysis = results_df.groupby('document').agg({\n",
    "        'f1': ['mean', 'std', 'min', 'max'],\n",
    "        'model': 'count'\n",
    "    }).round(3)\n",
    "    \n",
    "    doc_analysis.columns = ['_'.join(col).strip() for col in doc_analysis.columns.values]\n",
    "    doc_analysis = doc_analysis.rename(columns={'model_count': 'num_evaluations'})\n",
    "    print(doc_analysis.to_string())\n",
    "\n",
    "def enhanced_main():\n",
    "    \"\"\"Enhanced main function with better visualization.\"\"\"\n",
    "    print(\"Starting LLM Event Annotation Results Evaluation...\")\n",
    "    \n",
    "    # Use the globally defined pipeline results folder\n",
    "    evaluator = LLMEventResultsEvaluator(output_dir=PIPELINE_RESULTS_FOLDER)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.run_evaluation()\n",
    "    \n",
    "    results_df = results['evaluation']\n",
    "    runtime_df = results['runtime']\n",
    "    counts_df = results['annotation_counts']\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        # Save results\n",
    "        output_path = f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_event_evaluation_results.csv\"\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nEvaluation results saved to: {output_path}\")\n",
    "        \n",
    "        # Save additional data\n",
    "        if not runtime_df.empty:\n",
    "            runtime_df.to_csv(f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_runtime_analysis.csv\", index=False)\n",
    "            print(f\"Runtime analysis saved to: {PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_runtime_analysis.csv\")\n",
    "        \n",
    "        if not counts_df.empty:\n",
    "            counts_df.to_csv(f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_annotation_counts.csv\", index=False)\n",
    "            print(f\"Annotation counts saved to: {PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_annotation_counts.csv\")\n",
    "        \n",
    "        # Generate and save summary report\n",
    "        summary = evaluator.generate_summary_report(results_df)\n",
    "        summary_path = f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_event_evaluation_summary.txt\"\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(summary)\n",
    "        print(f\"Summary report saved to: {summary_path}\")\n",
    "        \n",
    "        # Create detailed results table\n",
    "        create_detailed_results_table(results_df)\n",
    "        \n",
    "        # Create visual analysis\n",
    "        print(\"\\n\\n📈 GENERATING VISUAL ANALYSIS...\")\n",
    "        create_visual_analysis(results_df)\n",
    "        \n",
    "        # Print basic summary\n",
    "        print(f\"\\n\\n📊 QUICK STATS:\")\n",
    "        print(f\"Total evaluations: {len(results_df)}\")\n",
    "        print(f\"Models evaluated: {results_df['model'].nunique()}\")\n",
    "        print(f\"Documents processed: {results_df['document'].nunique()}\")\n",
    "        print(f\"Annotation types: {', '.join(results_df['annotation_type'].unique())}\")\n",
    "        print(f\"Average F1 score: {results_df['f1'].mean():.3f}\")\n",
    "        print(f\"Best performing model: {results_df.groupby('model')['f1'].mean().idxmax()}\")\n",
    "        print(f\"Best annotation type: {results_df.groupby('annotation_type')['f1'].mean().idxmax()}\")\n",
    "        \n",
    "        # Runtime summary\n",
    "        if not runtime_df.empty:\n",
    "            print(f\"\\n⏱️  RUNTIME SUMMARY:\")\n",
    "            total_time = runtime_df['runtime_seconds'].sum()\n",
    "            avg_time_per_doc = runtime_df.groupby('document')['runtime_seconds'].sum().mean()\n",
    "            fastest_model = runtime_df.groupby('model')['runtime_seconds'].mean().idxmin()\n",
    "            print(f\"Total processing time: {total_time:.1f}s ({total_time/60:.1f}m)\")\n",
    "            print(f\"Average time per document: {avg_time_per_doc:.1f}s\")\n",
    "            print(f\"Fastest model on average: {fastest_model}\")\n",
    "        \n",
    "        # Annotation counts summary\n",
    "        if not counts_df.empty:\n",
    "            print(f\"\\n📊 ANNOTATION COUNTS SUMMARY:\")\n",
    "            consensus_total = counts_df[counts_df['model'] == 'consensus'][evaluator.event_annotation_types].sum().sum()\n",
    "            print(f\"Total consensus annotations: {consensus_total}\")\n",
    "            \n",
    "            for model in counts_df[counts_df['model'] != 'consensus']['model'].unique():\n",
    "                model_total = counts_df[counts_df['model'] == model][evaluator.event_annotation_types].sum().sum()\n",
    "                print(f\"Total {model} annotations: {model_total}\")\n",
    "        \n",
    "        print(f\"\\n🎯 OUTPUTS GENERATED:\")\n",
    "        print(f\"📊 Evaluation results: {PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_event_evaluation_results.csv\")\n",
    "        print(f\"📋 Summary report: {PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_event_evaluation_summary.txt\")\n",
    "        if not runtime_df.empty:\n",
    "            print(f\"⏱️  Runtime analysis: {PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_runtime_analysis.csv\")\n",
    "        if not counts_df.empty:\n",
    "            print(f\"📊 Annotation counts: {PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_annotation_counts.csv\")\n",
    "        print(f\"📁 Annotated corpus: {PIPELINE_RESULTS_FOLDER}/runtime_analysis/annotated_corpus_with_predictions/\")\n",
    "        print(f\"   → Open these XML files in Gate to view model predictions!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No results generated. Check if result folders and JSON files exist.\")\n",
    "\n",
    "# Run the enhanced analysis\n",
    "if __name__ == \"__main__\":\n",
    "    enhanced_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Analysis with Runtime and Annotation Count Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def create_runtime_visualizations(runtime_df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations for runtime analysis.\"\"\"\n",
    "    if runtime_df.empty:\n",
    "        print(\"No runtime data to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Runtime Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Runtime by model (box plot)\n",
    "    ax1 = axes[0, 0]\n",
    "    runtime_df.boxplot(column='runtime_seconds', by='model', ax=ax1)\n",
    "    ax1.set_title('Runtime Distribution by Model')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Runtime (seconds)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Runtime by document (bar plot)\n",
    "    ax2 = axes[0, 1]\n",
    "    doc_runtime = runtime_df.groupby('document')['runtime_seconds'].sum().sort_values(ascending=True)\n",
    "    doc_runtime.plot(kind='barh', ax=ax2, color='lightblue')\n",
    "    ax2.set_title('Total Runtime by Document')\n",
    "    ax2.set_xlabel('Runtime (seconds)')\n",
    "    \n",
    "    # 3. Average runtime by model\n",
    "    ax3 = axes[1, 0]\n",
    "    model_avg_runtime = runtime_df.groupby('model')['runtime_seconds'].mean().sort_values(ascending=True)\n",
    "    model_avg_runtime.plot(kind='barh', ax=ax3, color='lightgreen')\n",
    "    ax3.set_title('Average Runtime by Model')\n",
    "    ax3.set_xlabel('Average Runtime (seconds)')\n",
    "    \n",
    "    # 4. Runtime heatmap (model vs document)\n",
    "    ax4 = axes[1, 1]\n",
    "    runtime_pivot = runtime_df.pivot_table(values='runtime_seconds', index='model', columns='document', aggfunc='mean')\n",
    "    sns.heatmap(runtime_pivot, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax4, cbar_kws={'label': 'Runtime (seconds)'})\n",
    "    ax4.set_title('Runtime Heatmap (Model vs Document)')\n",
    "    ax4.set_xlabel('Document')\n",
    "    ax4.set_ylabel('Model')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_annotation_count_visualizations(counts_df: pd.DataFrame, event_types: List[str]):\n",
    "    \"\"\"Create visualizations for annotation count analysis.\"\"\"\n",
    "    if counts_df.empty:\n",
    "        print(\"No annotation count data to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Annotation Count Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Total annotations by model and type (stacked bar)\n",
    "    ax1 = axes[0, 0]\n",
    "    model_type_counts = counts_df.groupby('model')[event_types].sum()\n",
    "    model_type_counts.plot(kind='bar', stacked=True, ax=ax1, colormap='Set3')\n",
    "    ax1.set_title('Total Annotations by Model and Type')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Annotation Count')\n",
    "    ax1.legend(title='Annotation Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Annotation counts comparison (consensus vs models)\n",
    "    ax2 = axes[0, 1]\n",
    "    consensus_counts = counts_df[counts_df['model'] == 'consensus'][event_types].sum()\n",
    "    model_counts = counts_df[counts_df['model'] != 'consensus'].groupby('model')[event_types].sum().mean()\n",
    "    \n",
    "    x = np.arange(len(event_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2.bar(x - width/2, consensus_counts, width, label='Consensus', color='gold', alpha=0.8)\n",
    "    ax2.bar(x + width/2, model_counts, width, label='Models (avg)', color='steelblue', alpha=0.8)\n",
    "    \n",
    "    ax2.set_title('Consensus vs Model Predictions (Average)')\n",
    "    ax2.set_xlabel('Annotation Type')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(event_types, rotation=45)\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Document-wise annotation distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    doc_totals = counts_df.groupby(['document', 'model'])[event_types].sum().sum(axis=1).reset_index()\n",
    "    doc_totals_pivot = doc_totals.pivot(index='document', columns='model', values=0)\n",
    "    doc_totals_pivot.plot(kind='bar', ax=ax3, colormap='viridis')\n",
    "    ax3.set_title('Total Annotations by Document and Model')\n",
    "    ax3.set_xlabel('Document')\n",
    "    ax3.set_ylabel('Total Annotations')\n",
    "    ax3.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Annotation type performance (model deviation from consensus)\n",
    "    ax4 = axes[1, 1]\n",
    "    consensus_data = counts_df[counts_df['model'] == 'consensus'].groupby('document')[event_types].sum()\n",
    "    model_data = counts_df[counts_df['model'] != 'consensus'].groupby(['document', 'model'])[event_types].sum().groupby('document').mean()\n",
    "    \n",
    "    differences = []\n",
    "    for event_type in event_types:\n",
    "        diff = model_data[event_type] - consensus_data[event_type]\n",
    "        differences.append(diff.mean())\n",
    "    \n",
    "    colors = ['red' if d < 0 else 'green' for d in differences]\n",
    "    ax4.bar(event_types, differences, color=colors, alpha=0.7)\n",
    "    ax4.set_title('Average Model Deviation from Consensus')\n",
    "    ax4.set_xlabel('Annotation Type')\n",
    "    ax4.set_ylabel('Average Difference (Model - Consensus)')\n",
    "    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_comprehensive_analysis(results_df: pd.DataFrame, runtime_df: pd.DataFrame, counts_df: pd.DataFrame, event_types: List[str]):\n",
    "    \"\"\"Create comprehensive analysis combining all data.\"\"\"\n",
    "    print(\"📊 COMPREHENSIVE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Performance vs Runtime correlation\n",
    "    if not results_df.empty and not runtime_df.empty:\n",
    "        print(\"\\n🔗 PERFORMANCE vs RUNTIME CORRELATION:\")\n",
    "        \n",
    "        # Merge performance and runtime data\n",
    "        perf_runtime = results_df.merge(runtime_df, on=['document', 'model'], how='inner')\n",
    "        \n",
    "        if not perf_runtime.empty:\n",
    "            correlation = perf_runtime['f1'].corr(perf_runtime['runtime_seconds'])\n",
    "            print(f\"F1 Score vs Runtime correlation: {correlation:.3f}\")\n",
    "            \n",
    "            # Create scatter plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for model in perf_runtime['model'].unique():\n",
    "                model_data = perf_runtime[perf_runtime['model'] == model]\n",
    "                plt.scatter(model_data['runtime_seconds'], model_data['f1'], \n",
    "                          label=model, alpha=0.7, s=60)\n",
    "            \n",
    "            plt.xlabel('Runtime (seconds)')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.title('Performance vs Runtime by Model')\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.show()\n",
    "    \n",
    "    # Annotation count vs Performance analysis\n",
    "    if not results_df.empty and not counts_df.empty:\n",
    "        print(\"\\n📈 ANNOTATION COUNT vs PERFORMANCE ANALYSIS:\")\n",
    "        \n",
    "        # Calculate total predictions per model per document\n",
    "        model_totals = counts_df[counts_df['model'] != 'consensus'].groupby(['document', 'model'])[event_types].sum().sum(axis=1).reset_index()\n",
    "        model_totals.columns = ['document', 'model', 'total_predictions']\n",
    "        \n",
    "        # Get consensus totals\n",
    "        consensus_totals = counts_df[counts_df['model'] == 'consensus'].groupby('document')[event_types].sum().sum(axis=1).reset_index()\n",
    "        consensus_totals.columns = ['document', 'consensus_total']\n",
    "        \n",
    "        # Merge with performance data\n",
    "        perf_counts = results_df.merge(model_totals, on=['document', 'model'], how='inner')\n",
    "        perf_counts = perf_counts.merge(consensus_totals, on='document', how='inner')\n",
    "        \n",
    "        if not perf_counts.empty:\n",
    "            # Calculate prediction ratio (model predictions / consensus annotations)\n",
    "            perf_counts['prediction_ratio'] = perf_counts['total_predictions'] / perf_counts['consensus_total']\n",
    "            \n",
    "            avg_f1_by_ratio = perf_counts.groupby('model').agg({\n",
    "                'f1': 'mean',\n",
    "                'prediction_ratio': 'mean'\n",
    "            }).round(3)\n",
    "            \n",
    "            print(\"Model Performance vs Prediction Ratio:\")\n",
    "            print(avg_f1_by_ratio.to_string())\n",
    "    \n",
    "    # Create visualizations\n",
    "    if not runtime_df.empty:\n",
    "        create_runtime_visualizations(runtime_df)\n",
    "    \n",
    "    if not counts_df.empty:\n",
    "        create_annotation_count_visualizations(counts_df, event_types)\n",
    "\n",
    "def run_comprehensive_analysis():\n",
    "    \"\"\"Run the comprehensive analysis with all new features.\"\"\"\n",
    "    print(\"🚀 Starting Comprehensive LLM Analysis...\")\n",
    "    \n",
    "    # Use the globally defined pipeline results folder\n",
    "    evaluator = LLMEventResultsEvaluator(output_dir=PIPELINE_RESULTS_FOLDER)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.run_evaluation()\n",
    "    \n",
    "    results_df = results['evaluation']\n",
    "    runtime_df = results['runtime']\n",
    "    counts_df = results['annotation_counts']\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(f\"\\n✅ Analysis completed successfully!\")\n",
    "        print(f\"📊 Evaluation data: {len(results_df)} rows\")\n",
    "        print(f\"⏱️  Runtime data: {len(runtime_df)} rows\")\n",
    "        print(f\"📋 Annotation count data: {len(counts_df)} rows\")\n",
    "        \n",
    "        # Create comprehensive analysis\n",
    "        create_comprehensive_analysis(results_df, runtime_df, counts_df, evaluator.event_annotation_types)\n",
    "        \n",
    "        # Save all data\n",
    "        results_df.to_csv(f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_event_evaluation_results.csv\", index=False)\n",
    "        if not runtime_df.empty:\n",
    "            runtime_df.to_csv(f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_runtime_analysis.csv\", index=False)\n",
    "        if not counts_df.empty:\n",
    "            counts_df.to_csv(f\"{PIPELINE_RESULTS_FOLDER}/runtime_analysis/llm_annotation_counts.csv\", index=False)\n",
    "        \n",
    "        print(f\"\\n💾 All data saved to output/ directory\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No results generated. Check if result folders and JSON files exist.\")\n",
    "\n",
    "# Run comprehensive analysis\n",
    "if __name__ == \"__main__\":\n",
    "    run_comprehensive_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all documents with predictions using JSON format\n",
    "def save_all_documents_with_predictions():\n",
    "    \"\"\"Save all documents that have predictions using JSON format.\"\"\"\n",
    "    \n",
    "    # Load corpus and process predictions using the global folder\n",
    "    evaluator = LLMEventResultsEvaluator(PIPELINE_RESULTS_FOLDER)\n",
    "    result_folders = evaluator.find_result_folders()\n",
    "    \n",
    "    if result_folders:\n",
    "        for folder in result_folders:\n",
    "            result_jsons = evaluator.find_result_jsons(folder)\n",
    "            for json_path in result_jsons:\n",
    "                evaluator.process_result_json(json_path, folder.name)\n",
    "    \n",
    "    output_dir = Path(f\"{PIPELINE_RESULTS_FOLDER}/annotated_corpus_with_predictions\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    saved_count = 0\n",
    "    \n",
    "    # Process all documents\n",
    "    for doc in evaluator.corpus:\n",
    "        doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "        pred_annsets = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "        \n",
    "        if pred_annsets:\n",
    "            print(f\"📋 Processing document: {doc_name}\")\n",
    "            \n",
    "            # Create permanent annotation sets\n",
    "            for annset_name in pred_annsets:\n",
    "                model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "                pred_annset = doc.annset(annset_name)\n",
    "                permanent_annset = doc.annset(model_name)\n",
    "                permanent_annset.clear()\n",
    "                \n",
    "                for ann in pred_annset:\n",
    "                    features = dict(ann.features)\n",
    "                    features.pop(\"source\", None)\n",
    "                    features.pop(\"model\", None)\n",
    "                    permanent_annset.add(ann.start, ann.end, ann.type, features)\n",
    "                \n",
    "                print(f\"  ✅ Created {model_name} annotation set: {len(pred_annset)} annotations\")\n",
    "            \n",
    "            # Save document as JSON\n",
    "            try:\n",
    "                output_file = output_dir / f\"{doc_name}.bdocjs\"\n",
    "                doc.save(str(output_file), fmt=\"bdocjs\")\n",
    "                saved_count += 1\n",
    "                print(f\"  💾 Saved: {output_file.name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Failed to save {doc_name}: {e}\")\n",
    "    \n",
    "    # List final results\n",
    "    files = list(output_dir.glob(\"*.json\"))\n",
    "    print(f\"\\n✅ Successfully saved {len(files)} documents with predictions!\")\n",
    "    print(f\"\\n📁 Files in {output_dir}:\")\n",
    "    for file in files:\n",
    "        print(f\"  📄 {file.name}\")\n",
    "    \n",
    "    # Update summary file\n",
    "    summary_file = output_dir / \"annotation_summary.txt\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Annotated Corpus Summary\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Generated on: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"Total documents: {len(files)}\\n\")\n",
    "        f.write(f\"Format: JSON (GateNLP BDOC format)\\n\\n\")\n",
    "        f.write(\"Model annotation sets included:\\n\")\n",
    "        f.write(\"- gemma3:1b\\n\")\n",
    "        f.write(\"- gemma3:12b\\n\") \n",
    "        f.write(\"- mistral:latest\\n\\n\")\n",
    "        f.write(\"To view in Gate:\\n\")\n",
    "        f.write(\"1. Open Gate Developer\\n\")\n",
    "        f.write(\"2. Load documents from this directory\\n\")\n",
    "        f.write(\"3. Select JSON/BDOC format when loading\\n\")\n",
    "        f.write(\"4. View different annotation sets in the annotation sets panel\\n\")\n",
    "    \n",
    "    return str(output_dir)\n",
    "\n",
    "# Run the complete saving\n",
    "result = save_all_documents_with_predictions()\n",
    "print(f\"\\n🎯 FINAL SUCCESS! Annotated corpus saved to: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bd03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus for viewing with GateNLP CorpusViewer\n",
    "from GatenlpUtils import loadCorpus\n",
    "from gatenlp.visualization import CorpusViewer\n",
    "\n",
    "# Load corpus WITH model predictions for comparison\n",
    "def load_corpus_with_predictions():\n",
    "    \"\"\"Load corpus and add the model predictions for viewing.\"\"\"\n",
    "    \n",
    "    # Start fresh\n",
    "    corpus_with_preds = loadCorpus()\n",
    "    \n",
    "    # Add predictions using the global folder\n",
    "    evaluator = LLMEventResultsEvaluator(PIPELINE_RESULTS_FOLDER)\n",
    "    result_folders = evaluator.find_result_folders()\n",
    "    \n",
    "    if result_folders:\n",
    "        for folder in result_folders:\n",
    "            result_jsons = evaluator.find_result_jsons(folder)\n",
    "            for json_path in result_jsons:\n",
    "                evaluator.process_result_json(json_path, folder.name)\n",
    "    \n",
    "    # Copy predictions to our viewing corpus\n",
    "    docs_with_preds = 0\n",
    "    for i, doc in enumerate(corpus_with_preds):\n",
    "        if i < len(evaluator.corpus):\n",
    "            evaluator_doc = evaluator.corpus[i]\n",
    "            pred_annsets = [name for name in evaluator_doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "            \n",
    "            if pred_annsets:\n",
    "                docs_with_preds += 1\n",
    "                for annset_name in pred_annsets:\n",
    "                    model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "                    pred_annset = evaluator_doc.annset(annset_name)\n",
    "                    \n",
    "                    # Create permanent annotation set\n",
    "                    permanent_annset = doc.annset(model_name)\n",
    "                    permanent_annset.clear()\n",
    "                    \n",
    "                    for ann in pred_annset:\n",
    "                        features = dict(ann.features)\n",
    "                        features.pop(\"source\", None)\n",
    "                        features.pop(\"model\", None)\n",
    "                        permanent_annset.add(ann.start, ann.end, ann.type, features)\n",
    "    \n",
    "    print(f\"Added predictions to {docs_with_preds} documents\")\n",
    "    return corpus_with_preds\n",
    "\n",
    "corpus_with_predictions = load_corpus_with_predictions()\n",
    "viewer_with_preds = CorpusViewer(corpus_with_predictions)\n",
    "viewer_with_preds.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f320fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save documents with predictions in BDOC format for GateNLP\n",
    "output_dir = Path(f\"{PIPELINE_RESULTS_FOLDER}/annotated_corpus_with_predictions\")\n",
    "for doc_pred in corpus_with_predictions:\n",
    "    # Save each document with predictions in BDOC format\n",
    "    doc_name = Path(doc_pred.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "    output_file = output_dir / f\"{doc_name}.bdocjs\"\n",
    "    doc_pred.save(str(output_file), fmt=\"bdocjs\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_level_performance_tables():\n",
    "    \"\"\"Create comprehensive performance tables for each document showing precision, recall, and F1-score.\"\"\"\n",
    "    \n",
    "    print(\"📊 CREATING DOCUMENT-LEVEL PERFORMANCE TABLES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load the evaluator and run the analysis\n",
    "    evaluator = LLMEventResultsEvaluator(PIPELINE_RESULTS_FOLDER)\n",
    "    results = evaluator.run_evaluation()\n",
    "    results_df = results['evaluation']\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"❌ No evaluation results found!\")\n",
    "        return\n",
    "    \n",
    "    # Create a performance table output directory\n",
    "    performance_table_output_dir = Path(f\"{PIPELINE_RESULTS_FOLDER}/performance_tables\")\n",
    "    performance_table_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get unique documents\n",
    "    documents = sorted(results_df['document'].unique())\n",
    "    \n",
    "    # Annotation types to analyze (including event_type which is part of Event annotations)\n",
    "    annotation_types = [\"Event\", \"Event_who\", \"Event_what\", \"Event_when\"]\n",
    "    \n",
    "    for doc_name in documents:\n",
    "        print(f\"\\n📄 DOCUMENT: {doc_name}\")\n",
    "        print(\"=\" * (len(doc_name) + 20))\n",
    "        \n",
    "        # Filter data for this document\n",
    "        doc_data = results_df[results_df['document'] == doc_name]\n",
    "        \n",
    "        if doc_data.empty:\n",
    "            print(f\"No data found for document: {doc_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Get all models for this document\n",
    "        models = sorted(doc_data['model'].unique())\n",
    "        \n",
    "        # Create a comprehensive table for this document\n",
    "        print(f\"\\n🎯 Performance Metrics for {doc_name}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        # Create table data\n",
    "        table_data = []\n",
    "        \n",
    "        for ann_type in annotation_types:\n",
    "            # Get data for this annotation type\n",
    "            type_data = doc_data[doc_data['annotation_type'] == ann_type]\n",
    "            \n",
    "            if type_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Create row for each annotation type\n",
    "            for _, row in type_data.iterrows():\n",
    "                model = row['model']\n",
    "                precision = row['precision']\n",
    "                recall = row['recall']\n",
    "                f1 = row['f1']\n",
    "                gold_count = row['gold_count']\n",
    "                pred_count = row['pred_count']\n",
    "                matches = row.get('matches', 0)\n",
    "                \n",
    "                # Handle NaN values properly\n",
    "                import math\n",
    "                \n",
    "                table_data.append({\n",
    "                    'Annotation_Type': ann_type,\n",
    "                    'Model': model,\n",
    "                    'Precision': f\"{precision:.3f}\" if not math.isnan(precision) else \"0.000\",\n",
    "                    'Recall': f\"{recall:.3f}\" if not math.isnan(recall) else \"0.000\",\n",
    "                    'F1_Score': f\"{f1:.3f}\" if not math.isnan(f1) else \"0.000\",\n",
    "                    'Gold_Count': int(gold_count) if not math.isnan(gold_count) else 0,\n",
    "                    'Pred_Count': int(pred_count) if not math.isnan(pred_count) else 0,\n",
    "                    'Matches': int(matches) if not math.isnan(matches) else 0\n",
    "                })\n",
    "        \n",
    "        if table_data:\n",
    "            # Convert to DataFrame for better display\n",
    "            doc_df = pd.DataFrame(table_data)\n",
    "            \n",
    "            # Display the table\n",
    "            print(doc_df.to_string(index=False))\n",
    "            \n",
    "            # Calculate and display summary statistics for this document\n",
    "            print(f\"\\n📈 Summary Statistics for {doc_name}:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Overall performance by model\n",
    "            model_summary = doc_data.groupby('model').agg({\n",
    "                'precision': 'mean',\n",
    "                'recall': 'mean', \n",
    "                'f1': 'mean',\n",
    "                'gold_count': 'sum',\n",
    "                'pred_count': 'sum'\n",
    "            }).round(3)\n",
    "            \n",
    "            print(\"\\nOverall Performance by Model:\")\n",
    "            print(model_summary.to_string())\n",
    "            \n",
    "            # Performance by annotation type\n",
    "            type_summary = doc_data.groupby('annotation_type').agg({\n",
    "                'precision': 'mean',\n",
    "                'recall': 'mean',\n",
    "                'f1': 'mean',\n",
    "                'gold_count': 'sum', \n",
    "                'pred_count': 'sum'\n",
    "            }).round(3)\n",
    "            \n",
    "            print(f\"\\nPerformance by Annotation Type:\")\n",
    "            print(type_summary.to_string())\n",
    "            \n",
    "            # Best performing combinations for this document\n",
    "            print(f\"\\n🏆 Best Performing Model-Type Combinations for {doc_name}:\")\n",
    "            best_combos = doc_data.nlargest(5, 'f1')[['model', 'annotation_type', 'f1', 'precision', 'recall']]\n",
    "            print(best_combos.to_string(index=False))\n",
    "            \n",
    "            # Save document-specific results\n",
    "            output_file = f\"{PIPELINE_RESULTS_FOLDER}/performance_tables/{doc_name}_performance_table.csv\"\n",
    "            doc_df.to_csv(output_file, index=False)\n",
    "            print(f\"\\n💾 Document table saved to: {output_file}\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "    \n",
    "    # Create a consolidated summary table across all documents\n",
    "    print(f\"\\n📊 CONSOLIDATED SUMMARY ACROSS ALL DOCUMENTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Overall summary by model across all documents\n",
    "    overall_model_summary = results_df.groupby('model').agg({\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'gold_count': 'sum',\n",
    "        'pred_count': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"\\n🔍 Overall Performance by Model (across all documents):\")\n",
    "    print(overall_model_summary.to_string())\n",
    "    \n",
    "    # Overall summary by annotation type across all documents\n",
    "    overall_type_summary = results_df.groupby('annotation_type').agg({\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'gold_count': 'sum',\n",
    "        'pred_count': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(f\"\\n🎯 Overall Performance by Annotation Type (across all documents):\")\n",
    "    print(overall_type_summary.to_string())\n",
    "    \n",
    "    # Event type analysis (if available in features)\n",
    "    print(f\"\\n🏷️  EVENT TYPE ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # For Event annotations, also analyze performance by event_type if available\n",
    "    # We need to look at the actual documents to get event type information\n",
    "    event_type_analysis = []\n",
    "    \n",
    "    # Load corpus to check event types\n",
    "    try:\n",
    "        for doc in evaluator.corpus:\n",
    "            doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "            if doc_name in documents:\n",
    "                consensus_annset = doc.annset(\"consensus\")\n",
    "                event_anns = list(consensus_annset.with_type(\"Event\"))\n",
    "                \n",
    "                # Count event types in consensus\n",
    "                event_types = {}\n",
    "                for ann in event_anns:\n",
    "                    event_type = ann.features.get(\"type\", \"unspecified\")\n",
    "                    event_types[event_type] = event_types.get(event_type, 0) + 1\n",
    "                \n",
    "                if event_types:\n",
    "                    print(f\"\\n{doc_name} - Consensus Event Types:\")\n",
    "                    for etype, count in sorted(event_types.items()):\n",
    "                        print(f\"  {etype}: {count}\")\n",
    "                \n",
    "                # Check model predictions for event types\n",
    "                pred_annsets = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "                for annset_name in pred_annsets:\n",
    "                    model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "                    pred_annset = doc.annset(annset_name)\n",
    "                    pred_events = list(pred_annset.with_type(\"Event\"))\n",
    "                    \n",
    "                    pred_event_types = {}\n",
    "                    for ann in pred_events:\n",
    "                        event_type = ann.features.get(\"type\", \"unspecified\")\n",
    "                        pred_event_types[event_type] = pred_event_types.get(event_type, 0) + 1\n",
    "                    \n",
    "                    if pred_event_types:\n",
    "                        print(f\"{doc_name} - {model_name} Event Types:\")\n",
    "                        for etype, count in sorted(pred_event_types.items()):\n",
    "                            print(f\"  {etype}: {count}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Could not analyze event types: {e}\")\n",
    "    \n",
    "    # Save consolidated results\n",
    "    consolidated_output = f\"{PIPELINE_RESULTS_FOLDER}/consolidated_document_performance_summary.csv\"\n",
    "    results_df.to_csv(consolidated_output, index=False)\n",
    "    print(f\"\\n💾 Consolidated results saved to: {consolidated_output}\")\n",
    "    \n",
    "    print(f\"\\n✅ Document-level performance analysis complete!\")\n",
    "    print(f\"📁 Individual document tables saved to: {PIPELINE_RESULTS_FOLDER}/\")\n",
    "    print(f\"🔍 Check the generated CSV files for detailed per-document analysis\")\n",
    "\n",
    "# Run the document-level analysis\n",
    "create_document_level_performance_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concise_document_performance_summary():\n",
    "    \"\"\"Create concise, easy-to-read performance summary tables for each document.\"\"\"\n",
    "    \n",
    "    print(\"📊 CONCISE DOCUMENT PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load the evaluator and run the analysis\n",
    "    evaluator = LLMEventResultsEvaluator(PIPELINE_RESULTS_FOLDER)\n",
    "    results = evaluator.run_evaluation()\n",
    "    results_df = results['evaluation']\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"❌ No evaluation results found!\")\n",
    "        return\n",
    "    \n",
    "    # Get unique documents\n",
    "    documents = sorted(results_df['document'].unique())\n",
    "    annotation_types = [\"Event\", \"Event_who\", \"Event_what\", \"Event_when\"]\n",
    "    \n",
    "    # Create a master summary table\n",
    "    print(\"\\n📋 MASTER SUMMARY TABLE - All Documents\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    master_data = []\n",
    "    \n",
    "    for doc_name in documents:\n",
    "        doc_data = results_df[results_df['document'] == doc_name]\n",
    "        \n",
    "        if doc_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Calculate average metrics across all annotation types for each model\n",
    "        for model in sorted(doc_data['model'].unique()):\n",
    "            model_data = doc_data[doc_data['model'] == model]\n",
    "            \n",
    "            avg_precision = model_data['precision'].mean()\n",
    "            avg_recall = model_data['recall'].mean()\n",
    "            avg_f1 = model_data['f1'].mean()\n",
    "            total_gold = model_data['gold_count'].sum()\n",
    "            total_pred = model_data['pred_count'].sum()\n",
    "            \n",
    "            master_data.append({\n",
    "                'Document': doc_name,\n",
    "                'Model': model,\n",
    "                'Avg_Precision': f\"{avg_precision:.3f}\",\n",
    "                'Avg_Recall': f\"{avg_recall:.3f}\",\n",
    "                'Avg_F1': f\"{avg_f1:.3f}\",\n",
    "                'Total_Gold': int(total_gold),\n",
    "                'Total_Pred': int(total_pred)\n",
    "            })\n",
    "    \n",
    "    master_df = pd.DataFrame(master_data)\n",
    "    print(master_df.to_string(index=False))\n",
    "    \n",
    "    # Save master summary\n",
    "    master_output = f\"{PIPELINE_RESULTS_FOLDER}/performance_tables/master_document_performance_summary.csv\"\n",
    "    master_df.to_csv(master_output, index=False)\n",
    "    print(f\"\\n💾 Master summary saved to: {master_output}\")\n",
    "    \n",
    "    # Create individual document tables in a more readable format\n",
    "    for doc_name in documents:\n",
    "        print(f\"\\n\\n📄 {doc_name}\")\n",
    "        print(\"=\" * (len(doc_name) + 4))\n",
    "        \n",
    "        doc_data = results_df[results_df['document'] == doc_name]\n",
    "        \n",
    "        if doc_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Create pivot table for better readability\n",
    "        models = sorted(doc_data['model'].unique())\n",
    "        \n",
    "        print(f\"\\n🎯 F1 Scores by Annotation Type:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # F1 Score table\n",
    "        f1_data = []\n",
    "        for ann_type in annotation_types:\n",
    "            row = {'Annotation_Type': ann_type}\n",
    "            type_data = doc_data[doc_data['annotation_type'] == ann_type]\n",
    "            \n",
    "            for model in models:\n",
    "                model_type_data = type_data[type_data['model'] == model]\n",
    "                if not model_type_data.empty:\n",
    "                    f1_score = model_type_data['f1'].iloc[0]\n",
    "                    row[model] = f\"{f1_score:.3f}\" if not pd.isna(f1_score) else \"0.000\"\n",
    "                else:\n",
    "                    row[model] = \"N/A\"\n",
    "            \n",
    "            f1_data.append(row)\n",
    "        \n",
    "        f1_df = pd.DataFrame(f1_data)\n",
    "        print(f1_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n🎯 Precision by Annotation Type:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Precision table\n",
    "        prec_data = []\n",
    "        for ann_type in annotation_types:\n",
    "            row = {'Annotation_Type': ann_type}\n",
    "            type_data = doc_data[doc_data['annotation_type'] == ann_type]\n",
    "            \n",
    "            for model in models:\n",
    "                model_type_data = type_data[type_data['model'] == model]\n",
    "                if not model_type_data.empty:\n",
    "                    precision = model_type_data['precision'].iloc[0]\n",
    "                    row[model] = f\"{precision:.3f}\" if not pd.isna(precision) else \"0.000\"\n",
    "                else:\n",
    "                    row[model] = \"N/A\"\n",
    "            \n",
    "            prec_data.append(row)\n",
    "        \n",
    "        prec_df = pd.DataFrame(prec_data)\n",
    "        print(prec_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n🎯 Recall by Annotation Type:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Recall table\n",
    "        recall_data = []\n",
    "        for ann_type in annotation_types:\n",
    "            row = {'Annotation_Type': ann_type}\n",
    "            type_data = doc_data[doc_data['annotation_type'] == ann_type]\n",
    "            \n",
    "            for model in models:\n",
    "                model_type_data = type_data[type_data['model'] == model]\n",
    "                if not model_type_data.empty:\n",
    "                    recall = model_type_data['recall'].iloc[0]\n",
    "                    row[model] = f\"{recall:.3f}\" if not pd.isna(recall) else \"0.000\"\n",
    "                else:\n",
    "                    row[model] = \"N/A\"\n",
    "            \n",
    "            recall_data.append(row)\n",
    "        \n",
    "        recall_df = pd.DataFrame(recall_data)\n",
    "        print(recall_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n📊 Annotation Counts:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Annotation counts table\n",
    "        count_data = []\n",
    "        for ann_type in annotation_types:\n",
    "            row = {'Type': ann_type}\n",
    "            type_data = doc_data[doc_data['annotation_type'] == ann_type]\n",
    "            \n",
    "            if not type_data.empty:\n",
    "                # Gold count should be the same across all models for the same annotation type\n",
    "                gold_count = type_data['gold_count'].iloc[0]\n",
    "                row['Gold'] = int(gold_count) if not pd.isna(gold_count) else 0\n",
    "                \n",
    "                for model in models:\n",
    "                    model_type_data = type_data[type_data['model'] == model]\n",
    "                    if not model_type_data.empty:\n",
    "                        pred_count = model_type_data['pred_count'].iloc[0]\n",
    "                        row[f'{model}_Pred'] = int(pred_count) if not pd.isna(pred_count) else 0\n",
    "                    else:\n",
    "                        row[f'{model}_Pred'] = 0\n",
    "            \n",
    "            count_data.append(row)\n",
    "        \n",
    "        count_df = pd.DataFrame(count_data)\n",
    "        print(count_df.to_string(index=False))\n",
    "        \n",
    "        # Save individual document summary\n",
    "        doc_output_dir = Path(f\"{PIPELINE_RESULTS_FOLDER}/document_summaries\")\n",
    "        doc_output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save all tables for this document\n",
    "        f1_df.to_csv(doc_output_dir / f\"{doc_name}_f1_scores.csv\", index=False)\n",
    "        prec_df.to_csv(doc_output_dir / f\"{doc_name}_precision.csv\", index=False)\n",
    "        recall_df.to_csv(doc_output_dir / f\"{doc_name}_recall.csv\", index=False)\n",
    "        count_df.to_csv(doc_output_dir / f\"{doc_name}_counts.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n\\n✅ CONCISE SUMMARY COMPLETE!\")\n",
    "    print(f\"📁 Master summary: {master_output}\")\n",
    "    print(f\"📁 Individual document summaries: {PIPELINE_RESULTS_FOLDER}/document_summaries/\")\n",
    "    print(f\"🔍 Each document has separate CSV files for F1, Precision, Recall, and Counts\")\n",
    "\n",
    "# Run the concise summary\n",
    "create_concise_document_performance_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_documents_without_predictions():\n",
    "    \"\"\"Save documents with only permanent model annotation sets, removing temporary _predictions sets.\"\"\"\n",
    "    \n",
    "    print(\"🧹 SAVING CLEAN DOCUMENTS (WITHOUT _predictions ANNOTATION SETS)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load corpus and process predictions using the global folder\n",
    "    evaluator = LLMEventResultsEvaluator(PIPELINE_RESULTS_FOLDER)\n",
    "    result_folders = evaluator.find_result_folders()\n",
    "    \n",
    "    if result_folders:\n",
    "        for folder in result_folders:\n",
    "            result_jsons = evaluator.find_result_jsons(folder)\n",
    "            for json_path in result_jsons:\n",
    "                evaluator.process_result_json(json_path, folder.name)\n",
    "    \n",
    "    # Create a clean output directory\n",
    "    clean_output_dir = Path(f\"{PIPELINE_RESULTS_FOLDER}/clean_annotated_corpus\")\n",
    "    clean_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    saved_count = 0\n",
    "    model_sets_created = set()\n",
    "    \n",
    "    # Process all documents\n",
    "    for doc in evaluator.corpus:\n",
    "        doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "        pred_annsets = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "        \n",
    "        if pred_annsets:\n",
    "            print(f\"📋 Cleaning document: {doc_name}\")\n",
    "            \n",
    "            # First, ensure we have clean permanent annotation sets\n",
    "            for annset_name in pred_annsets:\n",
    "                model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "                pred_annset = doc.annset(annset_name)\n",
    "                \n",
    "                # Create/update permanent annotation set\n",
    "                permanent_annset = doc.annset(model_name)\n",
    "                permanent_annset.clear()\n",
    "                \n",
    "                for ann in pred_annset:\n",
    "                    features = dict(ann.features)\n",
    "                    # Remove evaluation-specific metadata\n",
    "                    features.pop(\"source\", None)\n",
    "                    features.pop(\"model\", None)\n",
    "                    permanent_annset.add(ann.start, ann.end, ann.type, features)\n",
    "                \n",
    "                model_sets_created.add(model_name)\n",
    "                print(f\"  ✅ Clean {model_name} annotation set: {len(pred_annset)} annotations\")\n",
    "            \n",
    "            # Now remove ALL temporary _predictions annotation sets\n",
    "            annsets_to_remove = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "            for temp_annset_name in annsets_to_remove:\n",
    "                try:\n",
    "                    # Remove the temporary annotation set\n",
    "                    doc.remove_annset(temp_annset_name)\n",
    "                    print(f\"  🗑️  Removed temporary set: {temp_annset_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠️  Could not remove {temp_annset_name}: {e}\")\n",
    "            \n",
    "            # Verify what annotation sets remain\n",
    "            remaining_sets = [name for name in doc.annset_names() if name]\n",
    "            print(f\"  📊 Remaining annotation sets: {remaining_sets}\")\n",
    "            \n",
    "            # Save clean document\n",
    "            try:\n",
    "                clean_output_file = clean_output_dir / f\"{doc_name}.bdocjs\"\n",
    "                doc.save(str(clean_output_file), fmt=\"bdocjs\")\n",
    "                saved_count += 1\n",
    "                print(f\"  💾 Saved clean document: {clean_output_file.name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Failed to save clean {doc_name}: {e}\")\n",
    "    \n",
    "    # List final results\n",
    "    clean_files = list(clean_output_dir.glob(\"*.bdocjs\"))\n",
    "    print(f\"\\n✅ Successfully saved {len(clean_files)} CLEAN documents!\")\n",
    "    print(f\"🗑️  Removed all temporary '*_predictions' annotation sets\")\n",
    "    print(f\"📊 Created clean annotation sets for models: {sorted(model_sets_created)}\")\n",
    "    \n",
    "    print(f\"\\n📁 Clean files saved to: {clean_output_dir}\")\n",
    "    for file in clean_files:\n",
    "        print(f\"  📄 {file.name}\")\n",
    "    \n",
    "    # Create updated summary file\n",
    "    clean_summary_file = clean_output_dir / \"clean_annotation_summary.txt\"\n",
    "    with open(clean_summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Clean Annotated Corpus Summary\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Generated on: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"Total documents: {len(clean_files)}\\n\")\n",
    "        f.write(f\"Format: JSON (GateNLP BDOC format)\\n\\n\")\n",
    "        f.write(\"✅ CLEAN VERSION - No temporary annotation sets included\\n\")\n",
    "        f.write(\"Model annotation sets included:\\n\")\n",
    "        for model in sorted(model_sets_created):\n",
    "            f.write(f\"- {model}\\n\")\n",
    "        f.write(\"\\nRemoved annotation sets:\\n\")\n",
    "        f.write(\"- All *_predictions sets (temporary evaluation sets)\\n\\n\")\n",
    "        f.write(\"To view in Gate:\\n\")\n",
    "        f.write(\"1. Open Gate Developer\\n\")\n",
    "        f.write(\"2. Load documents from this directory\\n\")\n",
    "        f.write(\"3. Select JSON/BDOC format when loading\\n\")\n",
    "        f.write(\"4. View different annotation sets in the annotation sets panel\\n\")\n",
    "        f.write(\"5. Only consensus and model prediction sets should be visible\\n\")\n",
    "    \n",
    "    # Verify one of the clean files doesn't contain _predictions\n",
    "    if clean_files:\n",
    "        test_file = clean_files[0]\n",
    "        print(f\"\\n🔍 VERIFICATION: Checking {test_file.name} for cleanliness...\")\n",
    "        \n",
    "        # Check if the file contains _predictions\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            if '_predictions' in content:\n",
    "                print(f\"  ⚠️  WARNING: {test_file.name} still contains '_predictions' strings!\")\n",
    "            else:\n",
    "                print(f\"  ✅ VERIFIED: {test_file.name} is clean - no '_predictions' found!\")\n",
    "    \n",
    "    return str(clean_output_dir)\n",
    "\n",
    "# Run the clean saving\n",
    "clean_result = save_clean_documents_without_predictions()\n",
    "print(f\"\\n🎯 CLEAN CORPUS SUCCESS! Clean documents saved to: {clean_result}\")\n",
    "print(f\"\\n📝 The clean corpus should now only contain:\")\n",
    "print(f\"   - 'consensus' annotation sets (gold standard)\")\n",
    "print(f\"   - Model name annotation sets (e.g., 'gemma3:1b', 'mistral:latest')\")\n",
    "print(f\"   - NO '*_predictions' annotation sets in corpus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
