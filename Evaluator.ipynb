{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Event Results Evaluator - Complete Working Version\n",
    "# Evaluates LLM event annotation results and integrates with GateNLP\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from gatenlp import Document\n",
    "from gatenlp.corpora.memory import ListCorpus\n",
    "from GatenlpUtils import loadCorpus\n",
    "\n",
    "class LLMEventResultsEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates LLM event annotation results against gold standard annotations.\n",
    "    Provides comprehensive analysis and saves annotated corpus with predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str):\n",
    "        \"\"\"Initialize evaluator with output directory containing pipeline results.\"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.corpus = loadCorpus()  # Load the corpus with gold standard annotations\n",
    "        self.results_df = pd.DataFrame()\n",
    "        \n",
    "    def find_result_folders(self) -> List[Path]:\n",
    "        \"\"\"Find all result folders (model output directories).\"\"\"\n",
    "        folders = [f for f in self.output_dir.iterdir() \n",
    "                  if f.is_dir() and not f.name.startswith('.') and f.name != \"annotated_corpus_with_predictions\"]\n",
    "        print(f\"Found {len(folders)} result folders: {[f.name for f in folders]}\")\n",
    "        return folders\n",
    "    \n",
    "    def find_result_jsons(self, folder: Path) -> List[Path]:\n",
    "        \"\"\"Find all JSON result files in a folder.\"\"\"\n",
    "        json_files = list(folder.glob(\"*.json\"))\n",
    "        print(f\"  Found {len(json_files)} JSON files in {folder.name}\")\n",
    "        return json_files\n",
    "    \n",
    "    def parse_llm_event_predictions(self, json_path: Path) -> List[Dict]:\n",
    "        \"\"\"Parse LLM predictions from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            predictions = []\n",
    "            \n",
    "            # Handle both single response and list of responses\n",
    "            responses = data if isinstance(data, list) else [data]\n",
    "            \n",
    "            for response in responses:\n",
    "                # Extract events from response\n",
    "                if 'events' in response:\n",
    "                    events = response['events']\n",
    "                elif 'predicted_events' in response:\n",
    "                    events = response['predicted_events']\n",
    "                elif isinstance(response, dict) and 'response' in response:\n",
    "                    # Handle nested response structure\n",
    "                    inner_response = response['response']\n",
    "                    if isinstance(inner_response, str):\n",
    "                        try:\n",
    "                            inner_response = json.loads(inner_response)\n",
    "                        except:\n",
    "                            continue\n",
    "                    events = inner_response.get('events', [])\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                for event in events:\n",
    "                    if isinstance(event, dict):\n",
    "                        predictions.append({\n",
    "                            'type': event.get('type', 'Event'),\n",
    "                            'start': event.get('start', 0),\n",
    "                            'end': event.get('end', 0),\n",
    "                            'text': event.get('text', ''),\n",
    "                            'features': {k: v for k, v in event.items() \n",
    "                                       if k not in ['type', 'start', 'end', 'text']}\n",
    "                        })\n",
    "            \n",
    "            return predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {json_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_result_json(self, json_path: Path, model_name: str):\n",
    "        \"\"\"Process a result JSON file and add predictions to corresponding document.\"\"\"\n",
    "        # Extract document name from JSON filename\n",
    "        doc_name = json_path.stem\n",
    "        \n",
    "        # Find corresponding document in corpus\n",
    "        target_doc = None\n",
    "        for doc in self.corpus:\n",
    "            corpus_doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "            if corpus_doc_name == doc_name:\n",
    "                target_doc = doc\n",
    "                break\n",
    "        \n",
    "        if not target_doc:\n",
    "            print(f\"Warning: No corpus document found for {doc_name}\")\n",
    "            return\n",
    "        \n",
    "        # Parse predictions\n",
    "        predictions = self.parse_llm_event_predictions(json_path)\n",
    "        \n",
    "        if predictions:\n",
    "            # Create annotation set for this model's predictions\n",
    "            pred_annset_name = f\"{model_name}_predictions\"\n",
    "            pred_annset = target_doc.annset(pred_annset_name)\n",
    "            pred_annset.clear()  # Clear any existing predictions\n",
    "            \n",
    "            # Add predictions as annotations\n",
    "            for pred in predictions:\n",
    "                features = dict(pred['features'])\n",
    "                features['source'] = 'llm_prediction'\n",
    "                features['model'] = model_name\n",
    "                \n",
    "                pred_annset.add(\n",
    "                    start=pred['start'],\n",
    "                    end=pred['end'],\n",
    "                    type=pred['type'],\n",
    "                    features=features\n",
    "                )\n",
    "            \n",
    "            print(f\"  Added {len(predictions)} predictions for {doc_name} ({model_name})\")\n",
    "    \n",
    "    def calculate_annotation_overlap_metrics(self, gold_annset, pred_annset, annotation_type: str = None) -> Dict[str, float]:\n",
    "        \"\"\"Calculate precision, recall, F1 for annotation overlap.\"\"\"\n",
    "        \n",
    "        # Filter by annotation type if specified\n",
    "        if annotation_type:\n",
    "            gold_anns = [ann for ann in gold_annset if ann.type == annotation_type]\n",
    "            pred_anns = [ann for ann in pred_annset if ann.type == annotation_type]\n",
    "        else:\n",
    "            gold_anns = list(gold_annset)\n",
    "            pred_anns = list(pred_annset)\n",
    "        \n",
    "        if not gold_anns and not pred_anns:\n",
    "            return {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'gold_count': 0, 'pred_count': 0}\n",
    "        \n",
    "        if not pred_anns:\n",
    "            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'gold_count': len(gold_anns), 'pred_count': 0}\n",
    "        \n",
    "        if not gold_anns:\n",
    "            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'gold_count': 0, 'pred_count': len(pred_anns)}\n",
    "        \n",
    "        # Calculate overlaps\n",
    "        matches = 0\n",
    "        for pred_ann in pred_anns:\n",
    "            for gold_ann in gold_anns:\n",
    "                # Check for overlap (any overlap counts as a match)\n",
    "                if (pred_ann.start < gold_ann.end and pred_ann.end > gold_ann.start):\n",
    "                    matches += 1\n",
    "                    break\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = matches / len(pred_anns) if pred_anns else 0.0\n",
    "        recall = matches / len(gold_anns) if gold_anns else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'gold_count': len(gold_anns),\n",
    "            'pred_count': len(pred_anns)\n",
    "        }\n",
    "    \n",
    "    def evaluate_all_predictions(self) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate all model predictions against gold standard.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for doc in self.corpus:\n",
    "            doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "            gold_annset = doc.annset(\"consensus\")  # Gold standard annotation set\n",
    "            \n",
    "            # Find all prediction annotation sets\n",
    "            pred_annsets = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "            \n",
    "            for pred_annset_name in pred_annsets:\n",
    "                model_name = pred_annset_name.replace(\"_predictions\", \"\")\n",
    "                pred_annset = doc.annset(pred_annset_name)\n",
    "                \n",
    "                # Get all unique annotation types\n",
    "                all_types = set()\n",
    "                for ann in gold_annset:\n",
    "                    all_types.add(ann.type)\n",
    "                for ann in pred_annset:\n",
    "                    all_types.add(ann.type)\n",
    "                \n",
    "                # Evaluate each annotation type\n",
    "                for ann_type in all_types:\n",
    "                    metrics = self.calculate_annotation_overlap_metrics(\n",
    "                        gold_annset, pred_annset, ann_type\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        'document': doc_name,\n",
    "                        'model': model_name,\n",
    "                        'annotation_type': ann_type,\n",
    "                        'precision': metrics['precision'],\n",
    "                        'recall': metrics['recall'],\n",
    "                        'f1': metrics['f1'],\n",
    "                        'gold_count': metrics['gold_count'],\n",
    "                        'pred_count': metrics['pred_count']\n",
    "                    })\n",
    "                \n",
    "                # Overall evaluation (all types combined)\n",
    "                overall_metrics = self.calculate_annotation_overlap_metrics(gold_annset, pred_annset)\n",
    "                results.append({\n",
    "                    'document': doc_name,\n",
    "                    'model': model_name,\n",
    "                    'annotation_type': 'OVERALL',\n",
    "                    'precision': overall_metrics['precision'],\n",
    "                    'recall': overall_metrics['recall'],\n",
    "                    'f1': overall_metrics['f1'],\n",
    "                    'gold_count': overall_metrics['gold_count'],\n",
    "                    'pred_count': overall_metrics['pred_count']\n",
    "                })\n",
    "        \n",
    "        self.results_df = pd.DataFrame(results)\n",
    "        return self.results_df\n",
    "    \n",
    "    def save_corpus_with_annotations(self):\n",
    "        \"\"\"\n",
    "        Save the corpus with all model predictions as separate annotation sets.\n",
    "        Files will be saved in JSON format within the pipeline results directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create output directory WITHIN the pipeline results folder\n",
    "            output_corpus_dir = self.output_dir / \"annotated_corpus_with_predictions\"\n",
    "            output_corpus_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            print(f\"üìÅ Saving annotated corpus to: {output_corpus_dir}\")\n",
    "            \n",
    "            saved_files = []\n",
    "            \n",
    "            for doc in self.corpus:\n",
    "                doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "                \n",
    "                # Check if document has any predictions\n",
    "                pred_annsets = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "                \n",
    "                if pred_annsets:\n",
    "                    try:\n",
    "                        # Create permanent annotation sets from predictions\n",
    "                        for annset_name in pred_annsets:\n",
    "                            model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "                            pred_annset = doc.annset(annset_name)\n",
    "                            permanent_annset = doc.annset(model_name)\n",
    "                            permanent_annset.clear()\n",
    "                            \n",
    "                            # Copy annotations\n",
    "                            for ann in pred_annset:\n",
    "                                features = dict(ann.features)\n",
    "                                features.pop(\"source\", None)  # Remove metadata\n",
    "                                features.pop(\"model\", None)\n",
    "                                permanent_annset.add(ann.start, ann.end, ann.type, features)\n",
    "                        \n",
    "                        # Save document in JSON format\n",
    "                        output_file = output_corpus_dir / f\"{doc_name}.json\"\n",
    "                        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(doc.to_dict(), f, ensure_ascii=False, indent=2)\n",
    "                        \n",
    "                        saved_files.append(doc_name)\n",
    "                        \n",
    "                        # Print summary of what was saved\n",
    "                        annset_summary = []\n",
    "                        for annset_name in doc.annset_names():\n",
    "                            if annset_name and not annset_name.endswith(\"_predictions\"):  # Skip temporary sets\n",
    "                                ann_count = len(doc.annset(annset_name))\n",
    "                                if ann_count > 0:\n",
    "                                    annset_summary.append(f\"{annset_name}({ann_count})\")\n",
    "                        \n",
    "                        if annset_summary:\n",
    "                            print(f\"  {doc_name}.json: {', '.join(annset_summary)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå Failed to save {doc_name}: {e}\")\n",
    "            \n",
    "            print(f\"‚úÖ Saved {len(saved_files)} annotated documents\")\n",
    "            \n",
    "            # Create a summary file\n",
    "            summary_file = output_corpus_dir / \"annotation_summary.txt\"\n",
    "            with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"Annotated Corpus Summary\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "                f.write(f\"Generated on: {pd.Timestamp.now()}\\n\")\n",
    "                f.write(f\"Pipeline results folder: {self.output_dir}\\n\")\n",
    "                f.write(f\"Total documents: {len(saved_files)}\\n\")\n",
    "                f.write(f\"Format: JSON (GateNLP BDOC format)\\n\\n\")\n",
    "                \n",
    "                f.write(\"Annotation Sets Added:\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                \n",
    "                # Get all unique annotation set names across all documents\n",
    "                all_annsets = set()\n",
    "                for doc in self.corpus:\n",
    "                    for annset_name in doc.annset_names():\n",
    "                        if annset_name and not annset_name.endswith(\"_predictions\"):\n",
    "                            all_annsets.add(annset_name)\n",
    "                \n",
    "                for annset_name in sorted(all_annsets):\n",
    "                    if annset_name not in [\"consensus\", \"\"]:  # Skip gold standard and default\n",
    "                        f.write(f\"- {annset_name} (LLM predictions)\\n\")\n",
    "                \n",
    "                f.write(f\"\\nFiles saved to: {output_corpus_dir}\\n\")\n",
    "                f.write(\"\\nTo view in Gate:\\n\")\n",
    "                f.write(\"1. Open Gate Developer\\n\")\n",
    "                f.write(\"2. Load documents from this directory\\n\")\n",
    "                f.write(\"3. Select JSON/BDOC format when loading\\n\")\n",
    "                f.write(\"4. View different annotation sets in the annotation sets panel\\n\")\n",
    "            \n",
    "            return str(output_corpus_dir)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving corpus: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "print(\"‚úÖ LLMEventResultsEvaluator class loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4a8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Visual analysis and results display functions loaded successfully!\n",
      "These functions will be called automatically by the main execution pipeline.\n"
     ]
    }
   ],
   "source": [
    "# Visual Analysis and Enhanced Results Display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_visual_analysis(results_df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive visual analysis of the evaluation results.\"\"\"\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create a large figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Heatmap: F1 scores by Model and Annotation Type\n",
    "    plt.subplot(3, 3, 1)\n",
    "    pivot_f1 = results_df.pivot_table(values='f1', index='model', columns='annotation_type', aggfunc='mean')\n",
    "    sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='RdYlGn', center=0.5, \n",
    "                cbar_kws={'label': 'F1 Score'})\n",
    "    plt.title('F1 Scores by Model and Annotation Type')\n",
    "    plt.xlabel('Annotation Type')\n",
    "    plt.ylabel('Model')\n",
    "    \n",
    "    # 2. Bar plot: Average F1 by Model\n",
    "    plt.subplot(3, 3, 2)\n",
    "    model_f1 = results_df.groupby('model')['f1'].mean().sort_values(ascending=True)\n",
    "    model_f1.plot(kind='barh', color='skyblue')\n",
    "    plt.title('Average F1 Score by Model')\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. Bar plot: Average F1 by Annotation Type\n",
    "    plt.subplot(3, 3, 3)\n",
    "    ann_f1 = results_df.groupby('annotation_type')['f1'].mean().sort_values(ascending=True)\n",
    "    ann_f1.plot(kind='barh', color='lightcoral')\n",
    "    plt.title('Average F1 Score by Annotation Type')\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 4. Precision vs Recall scatter plot\n",
    "    plt.subplot(3, 3, 4)\n",
    "    for model in results_df['model'].unique():\n",
    "        model_data = results_df[results_df['model'] == model]\n",
    "        plt.scatter(model_data['recall'], model_data['precision'], \n",
    "                   label=model, alpha=0.7, s=60)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision vs Recall by Model')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 5. Distribution of F1 scores\n",
    "    plt.subplot(3, 3, 5)\n",
    "    results_df.boxplot(column='f1', by='model', ax=plt.gca())\n",
    "    plt.title('F1 Score Distribution by Model')\n",
    "    plt.suptitle('')  # Remove the default title\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('F1 Score')\n",
    "    \n",
    "    # 6. Document-level performance heatmap\n",
    "    plt.subplot(3, 3, 6)\n",
    "    doc_model_f1 = results_df.groupby(['document', 'model'])['f1'].mean().reset_index()\n",
    "    if not doc_model_f1.empty:\n",
    "        doc_model_pivot = doc_model_f1.pivot(index='document', columns='model', values='f1')\n",
    "        sns.heatmap(doc_model_pivot, annot=True, fmt='.2f', cmap='RdYlGn', center=0.3,\n",
    "                    cbar_kws={'label': 'F1 Score'})\n",
    "        plt.title('F1 Scores by Document and Model')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Document')\n",
    "    \n",
    "    # 7. Gold vs Predicted annotations count\n",
    "    plt.subplot(3, 3, 7)\n",
    "    total_gold = results_df.groupby('model')['gold_count'].sum()\n",
    "    total_pred = results_df.groupby('model')['pred_count'].sum()\n",
    "    \n",
    "    x = np.arange(len(total_gold))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, total_gold.values, width, label='Gold Standard', color='gold', alpha=0.8)\n",
    "    plt.bar(x + width/2, total_pred.values, width, label='Predicted', color='steelblue', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Total Annotations')\n",
    "    plt.title('Gold vs Predicted Annotation Counts')\n",
    "    plt.xticks(x, total_gold.index, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 8. Annotation type performance comparison\n",
    "    plt.subplot(3, 3, 8)\n",
    "    ann_metrics = results_df.groupby('annotation_type')[['precision', 'recall', 'f1']].mean()\n",
    "    ann_metrics.plot(kind='bar', ax=plt.gca(), color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "    plt.title('Average Metrics by Annotation Type')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 9. Model performance ranking\n",
    "    plt.subplot(3, 3, 9)\n",
    "    model_ranking = results_df.groupby('model')[['precision', 'recall', 'f1']].mean().sort_values('f1', ascending=False)\n",
    "    model_ranking.plot(kind='bar', ax=plt.gca(), color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "    plt.title('Model Performance Ranking')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_detailed_results_table(results_df: pd.DataFrame):\n",
    "    \"\"\"Create a detailed results table with better formatting.\"\"\"\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"No results to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"DETAILED EVALUATION RESULTS\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    # Summary by model\n",
    "    print(\"\\nüìä SUMMARY BY MODEL:\")\n",
    "    print(\"-\" * 80)\n",
    "    model_summary = results_df.groupby('model').agg({\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'gold_count': 'sum',\n",
    "        'pred_count': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    # Flatten column names\n",
    "    model_summary.columns = ['_'.join(col).strip() for col in model_summary.columns.values]\n",
    "    print(model_summary.to_string())\n",
    "    \n",
    "    # Summary by annotation type\n",
    "    print(\"\\n\\nüìã SUMMARY BY ANNOTATION TYPE:\")\n",
    "    print(\"-\" * 80)\n",
    "    ann_summary = results_df.groupby('annotation_type').agg({\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'], \n",
    "        'f1': ['mean', 'std'],\n",
    "        'gold_count': 'sum',\n",
    "        'pred_count': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    ann_summary.columns = ['_'.join(col).strip() for col in ann_summary.columns.values]\n",
    "    print(ann_summary.to_string())\n",
    "    \n",
    "    # Best and worst performers\n",
    "    print(\"\\n\\nüèÜ TOP PERFORMERS:\")\n",
    "    print(\"-\" * 50)\n",
    "    top_performers = results_df.nlargest(10, 'f1')[['model', 'annotation_type', 'document', 'f1', 'precision', 'recall']]\n",
    "    print(top_performers.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\\n‚ö†Ô∏è  LOWEST PERFORMERS:\")\n",
    "    print(\"-\" * 50)\n",
    "    low_performers = results_df.nsmallest(10, 'f1')[['model', 'annotation_type', 'document', 'f1', 'precision', 'recall']]\n",
    "    print(low_performers.to_string(index=False))\n",
    "    \n",
    "    # Document-level analysis\n",
    "    print(\"\\n\\nüìÑ DOCUMENT-LEVEL ANALYSIS:\")\n",
    "    print(\"-\" * 50)\n",
    "    doc_analysis = results_df.groupby('document').agg({\n",
    "        'f1': ['mean', 'std', 'min', 'max'],\n",
    "        'model': 'count'\n",
    "    }).round(3)\n",
    "    doc_analysis.columns = ['_'.join(col).strip() for col in doc_analysis.columns.values]\n",
    "    print(doc_analysis.to_string())\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\\nüéØ OVERALL SUMMARY:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total evaluations performed: {len(results_df)}\")\n",
    "    print(f\"Documents analyzed: {results_df['document'].nunique()}\")\n",
    "    print(f\"Models compared: {results_df['model'].nunique()}\")\n",
    "    print(f\"Annotation types evaluated: {results_df['annotation_type'].nunique()}\")\n",
    "    print(f\"Average F1 score: {results_df['f1'].mean():.3f}\")\n",
    "    print(f\"Best performing model: {results_df.groupby('model')['f1'].mean().idxmax()}\")\n",
    "    print(f\"Best annotation type: {results_df.groupby('annotation_type')['f1'].mean().idxmax()}\")\n",
    "\n",
    "print(\"‚úÖ Visual analysis and results display functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0148eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Utility functions loaded:\n",
      "   - manual_corpus_save(): Force save corpus with predictions\n",
      "   - check_corpus_status(): Check annotation status\n",
      "   - list_pipeline_results(): List available pipeline directories\n",
      "\n",
      "Use these for debugging or manual operations if needed.\n"
     ]
    }
   ],
   "source": [
    "# Utility Functions for Manual Operations and Testing\n",
    "from pathlib import Path\n",
    "\n",
    "def run_evaluation_on_folder(output_dir: str = \"output/pipeline_results_20250718_085458\"):\n",
    "    \"\"\"Run evaluation on a specific pipeline results folder.\"\"\"\n",
    "    \n",
    "    print(f\"\ude80 Running evaluation on: {output_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Initialize evaluator\n",
    "        evaluator = LLMEventResultsEvaluator(output_dir)\n",
    "        \n",
    "        # Find and process all result folders\n",
    "        result_folders = evaluator.find_result_folders()\n",
    "        \n",
    "        if not result_folders:\n",
    "            print(\"‚ùå No result folders found!\")\n",
    "            # Try fallback to parent directory\n",
    "            parent_dir = Path(output_dir).parent\n",
    "            print(f\"üîç Checking parent directory: {parent_dir}\")\n",
    "            \n",
    "            model_folders = []\n",
    "            for folder in parent_dir.iterdir():\n",
    "                if folder.is_dir() and any(folder.glob(\"*.json\")) and folder.name != \"annotated_corpus_with_predictions\":\n",
    "                    model_folders.append(folder)\n",
    "            \n",
    "            if model_folders:\n",
    "                print(f\"Found {len(model_folders)} model folders in parent directory\")\n",
    "                evaluator.output_dir = parent_dir\n",
    "                result_folders = model_folders\n",
    "            else:\n",
    "                print(\"‚ùå No model folders found!\")\n",
    "                return None\n",
    "        \n",
    "        # Process all result JSON files\n",
    "        print(f\"\\nüìä Processing prediction results from {len(result_folders)} folders...\")\n",
    "        for folder in result_folders:\n",
    "            result_jsons = evaluator.find_result_jsons(folder)\n",
    "            for json_path in result_jsons:\n",
    "                evaluator.process_result_json(json_path, folder.name)\n",
    "        \n",
    "        # Check if we have predictions\n",
    "        docs_with_predictions = 0\n",
    "        for doc in evaluator.corpus:\n",
    "            pred_annsets = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "            if pred_annsets:\n",
    "                docs_with_predictions += 1\n",
    "        \n",
    "        print(f\"Documents with predictions: {docs_with_predictions}\")\n",
    "        \n",
    "        if docs_with_predictions == 0:\n",
    "            print(\"‚ùå No predictions found in corpus!\")\n",
    "            return None\n",
    "        \n",
    "        # Run evaluation\n",
    "        print(\"\\nüîç Evaluating predictions against gold standard...\")\n",
    "        results_df = evaluator.evaluate_all_predictions()\n",
    "        \n",
    "        if results_df.empty:\n",
    "            print(\"‚ùå No evaluation results generated!\")\n",
    "            return None\n",
    "        \n",
    "        # Save results\n",
    "        results_file = Path(output_dir) / \"evaluation_results.csv\"\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"üíæ Results saved to: {results_file}\")\n",
    "        \n",
    "        # Save annotated corpus\n",
    "        print(\"\\nüìÅ Saving annotated corpus...\")\n",
    "        corpus_path = evaluator.save_corpus_with_annotations()\n",
    "        \n",
    "        if corpus_path:\n",
    "            print(f\"‚úÖ Annotated corpus saved to: {corpus_path}\")\n",
    "        \n",
    "        # Generate visualizations\n",
    "        print(\"\\nüìà Generating visual analysis...\")\n",
    "        create_visual_analysis(results_df)\n",
    "        \n",
    "        print(\"\\nüìã Generating detailed results...\")\n",
    "        create_detailed_results_table(results_df)\n",
    "        \n",
    "        return evaluator, results_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def check_corpus_status(output_dir: str = \"output/pipeline_results_20250718_085458\"):\n",
    "    \"\"\"Check the status of annotations in the corpus.\"\"\"\n",
    "    \n",
    "    print(\"üîç Checking corpus annotation status\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    evaluator = LLMEventResultsEvaluator(output_dir)\n",
    "    \n",
    "    docs_with_preds = 0\n",
    "    total_docs = len(evaluator.corpus)\n",
    "    \n",
    "    for doc in evaluator.corpus:\n",
    "        doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "        pred_annsets = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "        \n",
    "        if pred_annsets:\n",
    "            docs_with_preds += 1\n",
    "            print(f\"üìã {doc_name}: {len(pred_annsets)} prediction sets\")\n",
    "            for annset_name in pred_annsets:\n",
    "                ann_count = len(doc.annset(annset_name))\n",
    "                print(f\"    - {annset_name}: {ann_count} annotations\")\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Total documents: {total_docs}\")\n",
    "    print(f\"   Documents with predictions: {docs_with_preds}\")\n",
    "    print(f\"   Coverage: {docs_with_preds/total_docs*100:.1f}%\")\n",
    "    \n",
    "    return docs_with_preds, total_docs\n",
    "\n",
    "def list_available_results(base_dir: str = \"output\"):\n",
    "    \"\"\"List all available pipeline result directories.\"\"\"\n",
    "    \n",
    "    print(\"üìÅ Available pipeline result directories:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    base_path = Path(base_dir)\n",
    "    pipeline_dirs = [d for d in base_path.iterdir() \n",
    "                    if d.is_dir() and d.name.startswith(\"pipeline_results_\")]\n",
    "    \n",
    "    for i, directory in enumerate(sorted(pipeline_dirs), 1):\n",
    "        print(f\"{i}. {directory.name}\")\n",
    "        \n",
    "        # Check if it has result folders\n",
    "        result_folders = [f for f in directory.iterdir() \n",
    "                         if f.is_dir() and not f.name.startswith('.') and f.name != \"annotated_corpus_with_predictions\"]\n",
    "        if result_folders:\n",
    "            print(f\"   Models: {[f.name for f in result_folders[:3]]}{'...' if len(result_folders) > 3 else ''}\")\n",
    "        \n",
    "        # Check if annotated corpus exists\n",
    "        corpus_dir = directory / \"annotated_corpus_with_predictions\"\n",
    "        if corpus_dir.exists():\n",
    "            corpus_files = list(corpus_dir.glob(\"*.json\"))\n",
    "            print(f\"   Annotated corpus: ‚úÖ ({len(corpus_files)} documents)\")\n",
    "        else:\n",
    "            print(f\"   Annotated corpus: ‚ùå (not created)\")\n",
    "    \n",
    "    return pipeline_dirs\n",
    "\n",
    "print(\"üîß Utility functions loaded:\")\n",
    "print(\"   - run_evaluation_on_folder(): Run complete evaluation\")\n",
    "print(\"   - check_corpus_status(): Check annotation status\")\n",
    "print(\"   - list_available_results(): List available directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d1c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting evaluation...\n",
      "üß™ Testing with existing corpus predictions\n",
      "--------------------------------------------------\n",
      "Loaded input/updated/annotated\\dev\\CASE OF ALTAY v. TURKEY (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BELYAYEV AND OTHERS v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BIGUN v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CABUCAK v. GERMANY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CAN v. TURKEY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CRISTIAN CATALIN UNGUREANU v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF DOKTOROV v. BULGARIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF EGILL EINARSSON v. ICELAND (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF HOINESS v. NORWAY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF KOSAITE - CYPIENE AND OTHERS v. LITHUANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF LOZOVYYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF M.T. v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MOSKALEV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MURUZHEVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF NODI v. HUNGARY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF O.C.I. AND OTHERS v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF OTGON v. THE REPUBLIC OF MOLDOVA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PAKHTUSOV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PANYUSHKINY v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF RESIN v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.N. v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.V. v. ITALY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SHVIDKIYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SIDOROVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SOLCAN v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF STANA v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF VISY v. SLOVAKIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YAKUSHEV v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YERMAKOVICH v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YEVGENIY ZAKHAROV v. RUSSIA.xml into corpus\n",
      "All documents loaded into the corpus.\n",
      "Checking for existing predictions in corpus...\n",
      "\n",
      "Found 0 prediction sets in 0 documents\n",
      "‚ùå No prediction sets found in corpus\n",
      "\n",
      "‚ö†Ô∏è No existing predictions found, running full pipeline...\n",
      "üöÄ Starting Complete LLM Event Evaluation Pipeline\n",
      "============================================================\n",
      "Loaded input/updated/annotated\\dev\\CASE OF ALTAY v. TURKEY (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BELYAYEV AND OTHERS v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BIGUN v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CABUCAK v. GERMANY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CAN v. TURKEY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CRISTIAN CATALIN UNGUREANU v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF DOKTOROV v. BULGARIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF EGILL EINARSSON v. ICELAND (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF HOINESS v. NORWAY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MURUZHEVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF NODI v. HUNGARY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF O.C.I. AND OTHERS v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF OTGON v. THE REPUBLIC OF MOLDOVA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PAKHTUSOV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PANYUSHKINY v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF RESIN v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.N. v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.V. v. ITALY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SHVIDKIYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SIDOROVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SOLCAN v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF STANA v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF VISY v. SLOVAKIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YAKUSHEV v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YERMAKOVICH v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YEVGENIY ZAKHAROV v. RUSSIA.xml into corpus\n",
      "All documents loaded into the corpus.\n",
      "Checking for existing predictions in corpus...\n",
      "\n",
      "Found 0 prediction sets in 0 documents\n",
      "‚ùå No prediction sets found in corpus\n",
      "\n",
      "‚ö†Ô∏è No existing predictions found, running full pipeline...\n",
      "üöÄ Starting Complete LLM Event Evaluation Pipeline\n",
      "============================================================\n",
      "Loaded input/updated/annotated\\dev\\CASE OF ALTAY v. TURKEY (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BELYAYEV AND OTHERS v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BIGUN v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CABUCAK v. GERMANY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CAN v. TURKEY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CRISTIAN CATALIN UNGUREANU v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF DOKTOROV v. BULGARIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF EGILL EINARSSON v. ICELAND (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF HOINESS v. NORWAY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF KOSAITE - CYPIENE AND OTHERS v. LITHUANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF LOZOVYYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF M.T. v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MOSKALEV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MURUZHEVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF NODI v. HUNGARY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF O.C.I. AND OTHERS v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF OTGON v. THE REPUBLIC OF MOLDOVA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PAKHTUSOV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PANYUSHKINY v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF KOSAITE - CYPIENE AND OTHERS v. LITHUANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF LOZOVYYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF M.T. v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MOSKALEV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MURUZHEVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF NODI v. HUNGARY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF O.C.I. AND OTHERS v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF OTGON v. THE REPUBLIC OF MOLDOVA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PAKHTUSOV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PANYUSHKINY v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF RESIN v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.N. v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.V. v. ITALY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SHVIDKIYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SIDOROVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SOLCAN v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF STANA v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF VISY v. SLOVAKIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YAKUSHEV v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YERMAKOVICH v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YEVGENIY ZAKHAROV v. RUSSIA.xml into corpus\n",
      "All documents loaded into the corpus.\n",
      "Found 1 result folders: ['annotated_corpus_with_predictions']\n",
      "‚ùå No model result folders found!\n",
      "Available folders: ['annotated_corpus_with_predictions']\n",
      "\n",
      "üîç Checking parent directory: output\n",
      "Found 3 model folders in parent directory\n",
      "\n",
      "üìä Processing LLM prediction results from 3 folders...\n",
      "  Found 1 JSON files in 270525\n",
      "Warning: No corpus document found for chat_responses_with_instructions\n",
      "  Found 3 JSON files in pipeline_results_20250718_085458\n",
      "Warning: No corpus document found for ALTAY v. TURKEY (No. 2)\n",
      "Warning: No corpus document found for BELYAYEV AND OTHERS v. UKRAINE\n",
      "Warning: No corpus document found for pipeline_results_20250718_085458\n",
      "  Found 3 JSON files in pipeline_results_20250718_124701\n",
      "Warning: No corpus document found for ALTAY v. TURKEY (No. 2)\n",
      "Warning: No corpus document found for BELYAYEV AND OTHERS v. UKRAINE\n",
      "Warning: No corpus document found for pipeline_results_20250718_124701\n",
      "Processed 7 prediction files\n",
      "Documents with predictions: 0\n",
      "‚ùå No predictions found in corpus!\n",
      "Available annotation sets in first document:\n",
      "  - \n",
      "  - consensus\n",
      "  - Original markups\n",
      "  - Cristiana\n",
      "  - Cristiana_round_1\n",
      "  - Section\n",
      "  - Key\n",
      "  - Erwin\n",
      "‚ùå Full pipeline also failed\n",
      "Loaded input/updated/annotated\\train\\CASE OF RESIN v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.N. v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.V. v. ITALY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SHVIDKIYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SIDOROVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SOLCAN v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF STANA v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF VISY v. SLOVAKIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YAKUSHEV v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YERMAKOVICH v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YEVGENIY ZAKHAROV v. RUSSIA.xml into corpus\n",
      "All documents loaded into the corpus.\n",
      "Found 1 result folders: ['annotated_corpus_with_predictions']\n",
      "‚ùå No model result folders found!\n",
      "Available folders: ['annotated_corpus_with_predictions']\n",
      "\n",
      "üîç Checking parent directory: output\n",
      "Found 3 model folders in parent directory\n",
      "\n",
      "üìä Processing LLM prediction results from 3 folders...\n",
      "  Found 1 JSON files in 270525\n",
      "Warning: No corpus document found for chat_responses_with_instructions\n",
      "  Found 3 JSON files in pipeline_results_20250718_085458\n",
      "Warning: No corpus document found for ALTAY v. TURKEY (No. 2)\n",
      "Warning: No corpus document found for BELYAYEV AND OTHERS v. UKRAINE\n",
      "Warning: No corpus document found for pipeline_results_20250718_085458\n",
      "  Found 3 JSON files in pipeline_results_20250718_124701\n",
      "Warning: No corpus document found for ALTAY v. TURKEY (No. 2)\n",
      "Warning: No corpus document found for BELYAYEV AND OTHERS v. UKRAINE\n",
      "Warning: No corpus document found for pipeline_results_20250718_124701\n",
      "Processed 7 prediction files\n",
      "Documents with predictions: 0\n",
      "‚ùå No predictions found in corpus!\n",
      "Available annotation sets in first document:\n",
      "  - \n",
      "  - consensus\n",
      "  - Original markups\n",
      "  - Cristiana\n",
      "  - Cristiana_round_1\n",
      "  - Section\n",
      "  - Key\n",
      "  - Erwin\n",
      "‚ùå Full pipeline also failed\n"
     ]
    }
   ],
   "source": [
    "# Main Execution with GateNLP Integration and Corpus Viewer\n",
    "from gatenlp.visualization.corpusviewer import CorpusViewer\n",
    "\n",
    "def enhanced_main(output_dir: str = \"output/pipeline_results_20250718_085458\"):\n",
    "    \"\"\"Enhanced main function with comprehensive analysis and GateNLP integration.\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Enhanced LLM Event Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run the evaluation\n",
    "    result = run_evaluation_on_folder(output_dir)\n",
    "    \n",
    "    if result:\n",
    "        evaluator, results_df = result\n",
    "        \n",
    "        print(\"\\n\udc41Ô∏è Opening GateNLP Corpus Viewer...\")\n",
    "        try:\n",
    "            # Create and display corpus viewer\n",
    "            viewer = CorpusViewer(evaluator.corpus)\n",
    "            viewer.show()\n",
    "            print(\"‚úÖ Corpus viewer opened successfully!\")\n",
    "            print(\"   - Use the viewer to explore documents and annotations\")\n",
    "            print(\"   - Switch between annotation sets to see different model predictions\")\n",
    "            print(\"   - Compare predictions with gold standard (consensus)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Corpus viewer failed to open: {e}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n\udcc8 Final Summary:\")\n",
    "        print(f\"   Total evaluations: {len(results_df)}\")\n",
    "        print(f\"   Documents processed: {results_df['document'].nunique()}\")\n",
    "        print(f\"   Models evaluated: {results_df['model'].nunique()}\")\n",
    "        print(f\"   Average F1 score: {results_df['f1'].mean():.3f}\")\n",
    "        print(f\"   Best performing model: {results_df.groupby('model')['f1'].mean().idxmax()}\")\n",
    "        \n",
    "        print(f\"\\nüéØ OUTPUTS GENERATED:\")\n",
    "        print(f\"üìä Evaluation results: {evaluator.output_dir}/evaluation_results.csv\")\n",
    "        print(f\"üìÅ Annotated corpus: {evaluator.output_dir}/annotated_corpus_with_predictions/\")\n",
    "        print(f\"   ‚Üí Open these JSON files in Gate to view model predictions!\")\n",
    "        \n",
    "        return evaluator, results_df\n",
    "    else:\n",
    "        print(\"‚ùå Evaluation failed. Check if result folders and JSON files exist.\")\n",
    "        return None\n",
    "\n",
    "# Test with existing predictions in corpus\n",
    "def test_existing_predictions():\n",
    "    \"\"\"Test evaluation using any existing predictions in the corpus.\"\"\"\n",
    "    \n",
    "    print(\"üß™ Testing with existing corpus predictions\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        evaluator = LLMEventResultsEvaluator(\"output/pipeline_results_20250718_085458\")\n",
    "        \n",
    "        # Check for existing predictions\n",
    "        docs_with_preds = 0\n",
    "        total_pred_sets = 0\n",
    "        \n",
    "        for doc in evaluator.corpus:\n",
    "            doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "            pred_annsets = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "            \n",
    "            if pred_annsets:\n",
    "                docs_with_preds += 1\n",
    "                total_pred_sets += len(pred_annsets)\n",
    "                print(f\"üìã {doc_name}: {pred_annsets}\")\n",
    "        \n",
    "        print(f\"\\nFound {total_pred_sets} prediction sets in {docs_with_preds} documents\")\n",
    "        \n",
    "        if total_pred_sets > 0:\n",
    "            # Run evaluation on existing predictions\n",
    "            results_df = evaluator.evaluate_all_predictions()\n",
    "            \n",
    "            if not results_df.empty:\n",
    "                print(f\"‚úÖ Generated {len(results_df)} evaluation results\")\n",
    "                \n",
    "                # Quick summary\n",
    "                print(\"\\nüìä Quick Summary:\")\n",
    "                model_summary = results_df.groupby('model')['f1'].mean().sort_values(ascending=False)\n",
    "                for model, f1 in model_summary.items():\n",
    "                    print(f\"  {model}: F1 = {f1:.3f}\")\n",
    "                \n",
    "                # Generate visualizations\n",
    "                create_visual_analysis(results_df)\n",
    "                create_detailed_results_table(results_df)\n",
    "                \n",
    "                # Show corpus viewer\n",
    "                print(\"\\nüëÅÔ∏è Opening corpus viewer...\")\n",
    "                viewer = CorpusViewer(evaluator.corpus)\n",
    "                viewer.show()\n",
    "                \n",
    "                return evaluator, results_df\n",
    "            else:\n",
    "                print(\"‚ùå No evaluation results generated\")\n",
    "        else:\n",
    "            print(\"‚ùå No prediction sets found in corpus\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Auto-run: Try existing predictions first, then full pipeline\n",
    "print(\"üöÄ Auto-starting evaluation...\")\n",
    "\n",
    "# First, check what we have\n",
    "list_available_results()\n",
    "\n",
    "# Try existing predictions\n",
    "result = test_existing_predictions()\n",
    "\n",
    "if result:\n",
    "    evaluator, results_df = result\n",
    "    print(f\"\\n‚úÖ Evaluation completed using existing predictions!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No existing predictions found, running full pipeline...\")\n",
    "    result = enhanced_main()\n",
    "    \n",
    "    if result:\n",
    "        evaluator, results_df = result\n",
    "        print(\"\\n‚úÖ Full evaluation pipeline completed!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Both existing predictions test and full pipeline failed.\")\n",
    "        print(\"Please check your data and try manual evaluation with:\")\n",
    "        print(\"   run_evaluation_on_folder('your_output_directory')\")\n",
    "\n",
    "print(\"\\nüéØ Evaluation complete! Use the corpus viewer to explore results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
