{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Set your pipeline results folder here\n",
    "PIPELINE_RESULTS_FOLDER = \"output/pipeline_results_20250718_085458\"\n",
    "\n",
    "print(f\"📁 Using pipeline results folder: {PIPELINE_RESULTS_FOLDER}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import pandas as pd\n",
    "from gatenlp import Document\n",
    "from gatenlp.corpora import ListCorpus\n",
    "from GatenlpUtils import loadCorpus\n",
    "\n",
    "class LLMEventResultsEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates LLM event annotation results by comparing predictions against \n",
    "    gold standard consensus annotations in GateNLP documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"output\", corpus=None):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.corpus = corpus if corpus is not None else loadCorpus()\n",
    "        self.evaluation_results = []\n",
    "        \n",
    "        # Event-related annotation types to evaluate based on consensus annotation set\n",
    "        self.event_annotation_types = [\n",
    "            \"Event\",      # General event annotation\n",
    "            \"Event_who\",  # Who annotation \n",
    "            \"Event_what\", # What annotation\n",
    "            \"Event_when\"  # When annotation\n",
    "        ]\n",
    "        \n",
    "    def find_result_folders(self) -> List[Path]:\n",
    "        \"\"\"Find all timestamped result folders in output directory.\"\"\"\n",
    "        folders = []\n",
    "        if self.output_dir.exists():\n",
    "            # Check if output_dir itself is a timestamped folder\n",
    "            if re.match(r'.*\\d{8}_\\d{6}', self.output_dir.name):\n",
    "                # We're pointing directly to a timestamped folder\n",
    "                folders.append(self.output_dir)\n",
    "            else:\n",
    "                # Look for timestamped folders within the output directory\n",
    "                for item in self.output_dir.iterdir():\n",
    "                    if item.is_dir() and re.match(r'\\d{8}_\\d{6}', item.name):\n",
    "                        folders.append(item)\n",
    "        return sorted(folders)\n",
    "    \n",
    "    def find_result_jsons(self, folder: Path) -> List[Path]:\n",
    "        \"\"\"Find all result JSON files (excluding pipeline_results_*.json).\"\"\"\n",
    "        jsons = []\n",
    "        for json_file in folder.glob(\"*.json\"):\n",
    "            if not json_file.name.startswith(\"pipeline_results_\"):\n",
    "                jsons.append(json_file)\n",
    "        return jsons\n",
    "    \n",
    "    def extract_doc_name_from_path(self, file_path: str) -> str:\n",
    "        \"\"\"Extract document name from file path for matching with corpus.\"\"\"\n",
    "        return Path(file_path).stem\n",
    "    \n",
    "    def find_corpus_document(self, doc_identifier: str) -> Document:\n",
    "        \"\"\"Find corresponding document in corpus.\"\"\"\n",
    "        for doc in self.corpus:\n",
    "            doc_name = doc.features.get(\"gate.SourceURL\", \"\")\n",
    "            if (doc_identifier in doc_name or \n",
    "                doc_name.endswith(f\"{doc_identifier}.xml\") or\n",
    "                Path(doc_name).stem == doc_identifier):\n",
    "                return doc\n",
    "        return None\n",
    "    \n",
    "    def parse_llm_event_predictions(self, result_data: Dict[str, Any]) -> Dict[str, Dict[str, List[Dict]]]:\n",
    "        \"\"\"\n",
    "        Parse LLM event predictions from result JSON.\n",
    "        Maps JSON events to consensus annotation types:\n",
    "        - \"event\" in JSON -> \"Event\" in consensus\n",
    "        - \"event_who\" in JSON -> \"Event_who\" in consensus\n",
    "        - \"event_what\" in JSON -> \"Event_what\" in consensus  \n",
    "        - \"event_when\" in JSON -> \"Event_when\" in consensus\n",
    "        - \"event_type\" in JSON -> type metadata of \"Event\" annotation\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        # Check if annotations array exists\n",
    "        if \"annotations\" not in result_data:\n",
    "            print(\"No 'annotations' key found in result data\")\n",
    "            return predictions\n",
    "        \n",
    "        # Process each model's annotations\n",
    "        for model_annotation in result_data[\"annotations\"]:\n",
    "            if not isinstance(model_annotation, dict):\n",
    "                continue\n",
    "                \n",
    "            model_name = model_annotation.get(\"model_name\", \"unknown_model\")\n",
    "            predictions[model_name] = {}\n",
    "            \n",
    "            # Initialize all event annotation types\n",
    "            for ann_type in self.event_annotation_types:\n",
    "                predictions[model_name][ann_type] = []\n",
    "            \n",
    "            # Extract events from this model's results\n",
    "            if \"events\" in model_annotation:\n",
    "                events = model_annotation[\"events\"]\n",
    "                for event in events:\n",
    "                    if isinstance(event, dict):\n",
    "                        source_text = event.get(\"source_text\", \"\")\n",
    "                        \n",
    "                        # Map \"event\" field to \"Event\" annotation\n",
    "                        if \"event\" in event:\n",
    "                            event_type = event.get(\"event_type\", \"\")\n",
    "                            # Remove \"event_\" prefix from event_type to match consensus type metadata\n",
    "                            if event_type.startswith(\"event_\"):\n",
    "                                event_type = event_type[6:]  # Remove \"event_\" prefix\n",
    "                            elif event_type.startswith(\"Event_\"):\n",
    "                                event_type = event_type[6:]  # Remove \"Event_\" prefix\n",
    "                            \n",
    "                            ann_dict = {\n",
    "                                \"start\": 0,  # Will be calculated from source_text\n",
    "                                \"end\": 0,    # Will be calculated from source_text\n",
    "                                \"features\": {\n",
    "                                    \"source_text\": source_text,\n",
    "                                    \"type\": event_type,  # This will be compared to consensus Event type metadata\n",
    "                                    \"event_value\": event.get(\"event\", \"\")\n",
    "                                }\n",
    "                            }\n",
    "                            predictions[model_name][\"Event\"].append(ann_dict)\n",
    "                        \n",
    "                        # Map \"event_who\" field to \"Event_who\" annotation\n",
    "                        if \"event_who\" in event:\n",
    "                            ann_dict = {\n",
    "                                \"start\": 0,\n",
    "                                \"end\": 0,\n",
    "                                \"features\": {\n",
    "                                    \"source_text\": source_text,\n",
    "                                    \"event_who_value\": event.get(\"event_who\", \"\")\n",
    "                                }\n",
    "                            }\n",
    "                            predictions[model_name][\"Event_who\"].append(ann_dict)\n",
    "                        \n",
    "                        # Map \"event_what\" field to \"Event_what\" annotation\n",
    "                        if \"event_what\" in event:\n",
    "                            ann_dict = {\n",
    "                                \"start\": 0,\n",
    "                                \"end\": 0,\n",
    "                                \"features\": {\n",
    "                                    \"source_text\": source_text,\n",
    "                                    \"event_what_value\": event.get(\"event_what\", \"\")\n",
    "                                }\n",
    "                            }\n",
    "                            predictions[model_name][\"Event_what\"].append(ann_dict)\n",
    "                        \n",
    "                        # Map \"event_when\" field to \"Event_when\" annotation\n",
    "                        if \"event_when\" in event:\n",
    "                            ann_dict = {\n",
    "                                \"start\": 0,\n",
    "                                \"end\": 0,\n",
    "                                \"features\": {\n",
    "                                    \"source_text\": source_text,\n",
    "                                    \"event_when_value\": event.get(\"event_when\", \"\")\n",
    "                                }\n",
    "                            }\n",
    "                            predictions[model_name][\"Event_when\"].append(ann_dict)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_text_positions(self, source_text: str, document_text: str) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Calculate start and end positions of source_text within document_text.\n",
    "        Returns: (start, end) positions or (0, 0) if not found.\n",
    "        \"\"\"\n",
    "        if not source_text or not document_text:\n",
    "            return (0, 0)\n",
    "        \n",
    "        # Clean up the source text for better matching\n",
    "        cleaned_source = source_text.strip()\n",
    "        \n",
    "        # Try to find the text in the document\n",
    "        start_pos = document_text.find(cleaned_source)\n",
    "        if start_pos != -1:\n",
    "            end_pos = start_pos + len(cleaned_source)\n",
    "            return (start_pos, end_pos)\n",
    "        \n",
    "        # If exact match fails, try with normalized whitespace\n",
    "        import re\n",
    "        normalized_source = re.sub(r'\\s+', ' ', cleaned_source)\n",
    "        normalized_doc = re.sub(r'\\s+', ' ', document_text)\n",
    "        \n",
    "        start_pos = normalized_doc.find(normalized_source)\n",
    "        if start_pos != -1:\n",
    "            # Find the actual positions in the original text\n",
    "            # This is a simplified approach - might need refinement\n",
    "            end_pos = start_pos + len(normalized_source)\n",
    "            return (start_pos, end_pos)\n",
    "        \n",
    "        return (0, 0)\n",
    "    \n",
    "    def extract_annotations_from_response(self, response_text: str) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Extract event annotations from LLM response text.\n",
    "        Adjust this method based on your actual LLM response format.\n",
    "        \"\"\"\n",
    "        annotations = {}\n",
    "        for ann_type in self.event_annotation_types:\n",
    "            annotations[ann_type] = []\n",
    "        \n",
    "        # Example parsing - adjust based on your actual response format\n",
    "        # This assumes annotations are in JSON format within the response\n",
    "        try:\n",
    "            # Try to find JSON blocks in the response\n",
    "            json_matches = re.findall(r'\\{.*?\\}', response_text, re.DOTALL)\n",
    "            for json_str in json_matches:\n",
    "                try:\n",
    "                    parsed_json = json.loads(json_str)\n",
    "                    if isinstance(parsed_json, dict):\n",
    "                        for ann_type in self.event_annotation_types:\n",
    "                            if ann_type in parsed_json:\n",
    "                                annotations[ann_type].extend(parsed_json[ann_type])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing annotations from response: {e}\")\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    def add_prediction_annotations(self, doc: Document, model_name: str, \n",
    "                                 predictions: Dict[str, List[Dict]]):\n",
    "        \"\"\"Add prediction annotations to document as new annotation set.\"\"\"\n",
    "        annset_name = f\"{model_name}_predictions\"\n",
    "        annset = doc.annset(annset_name)\n",
    "        \n",
    "        # Clear any existing annotations in this set\n",
    "        annset.clear()\n",
    "        \n",
    "        # Add predicted event annotations\n",
    "        for ann_type, ann_list in predictions.items():\n",
    "            for ann_dict in ann_list:\n",
    "                try:\n",
    "                    # Extract start, end, and features from annotation dict\n",
    "                    start = ann_dict.get(\"start\", 0)\n",
    "                    end = ann_dict.get(\"end\", 0)\n",
    "                    features = ann_dict.get(\"features\", {})\n",
    "                    \n",
    "                    # If start/end positions are not provided or are 0, try to calculate from source text\n",
    "                    if start == 0 and end == 0 and \"source_text\" in features:\n",
    "                        source_text = features[\"source_text\"]\n",
    "                        if source_text:\n",
    "                            start, end = self.calculate_text_positions(source_text, doc.text)\n",
    "                    \n",
    "                    # Ensure boundaries are within document bounds\n",
    "                    start = max(0, min(start, len(doc.text)))\n",
    "                    end = max(start, min(end, len(doc.text)))\n",
    "                    \n",
    "                    # Add features indicating source\n",
    "                    features.update({\n",
    "                        \"source\": \"llm_prediction\",\n",
    "                        \"model\": model_name\n",
    "                    })\n",
    "                    \n",
    "                    annset.add(start, end, ann_type, features)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error adding annotation {ann_dict}: {e}\")\n",
    "    \n",
    "    def calculate_annotation_overlap_metrics(self, gold_annset, pred_annset, ann_type: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate precision, recall, F1 for a specific annotation type.\"\"\"\n",
    "        gold_anns = list(gold_annset.with_type(ann_type))\n",
    "        pred_anns = list(pred_annset.with_type(ann_type))\n",
    "        \n",
    "        if not gold_anns and not pred_anns:\n",
    "            return {\"precision\": 1.0, \"recall\": 1.0, \"f1\": 1.0, \"gold_count\": 0, \"pred_count\": 0}\n",
    "        \n",
    "        if not pred_anns:\n",
    "            return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"gold_count\": len(gold_anns), \"pred_count\": 0}\n",
    "        \n",
    "        if not gold_anns:\n",
    "            return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"gold_count\": 0, \"pred_count\": len(pred_anns)}\n",
    "        \n",
    "        # Calculate token-level overlap\n",
    "        gold_tokens = set()\n",
    "        pred_tokens = set()\n",
    "        \n",
    "        # For Event annotations, also consider type metadata matching\n",
    "        if ann_type == \"Event\":\n",
    "            # Create sets of (start, end, type) tuples for exact matching\n",
    "            gold_spans_with_type = set()\n",
    "            pred_spans_with_type = set()\n",
    "            \n",
    "            for ann in gold_anns:\n",
    "                ann_type_meta = ann.features.get(\"type\", \"\")\n",
    "                for pos in range(ann.start, ann.end):\n",
    "                    gold_spans_with_type.add((pos, ann_type_meta))\n",
    "                    \n",
    "            for ann in pred_anns:\n",
    "                ann_type_meta = ann.features.get(\"type\", \"\")\n",
    "                for pos in range(ann.start, ann.end):\n",
    "                    pred_spans_with_type.add((pos, ann_type_meta))\n",
    "            \n",
    "            # Calculate overlap considering both position and type\n",
    "            true_positives = len(gold_spans_with_type.intersection(pred_spans_with_type))\n",
    "            \n",
    "            precision = true_positives / len(pred_spans_with_type) if pred_spans_with_type else 0\n",
    "            recall = true_positives / len(gold_spans_with_type) if gold_spans_with_type else 0\n",
    "            \n",
    "        else:\n",
    "            # For other annotation types, use token-level overlap only\n",
    "            for ann in gold_anns:\n",
    "                gold_tokens.update(range(ann.start, ann.end))\n",
    "            \n",
    "            for ann in pred_anns:\n",
    "                pred_tokens.update(range(ann.start, ann.end))\n",
    "            \n",
    "            true_positives = len(gold_tokens.intersection(pred_tokens))\n",
    "            precision = true_positives / len(pred_tokens) if pred_tokens else 0\n",
    "            recall = true_positives / len(gold_tokens) if gold_tokens else 0\n",
    "        \n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"gold_count\": len(gold_anns),\n",
    "            \"pred_count\": len(pred_anns),\n",
    "            \"true_positives\": true_positives if ann_type != \"Event\" else len(gold_spans_with_type.intersection(pred_spans_with_type)),\n",
    "            \"gold_tokens\": len(gold_tokens) if ann_type != \"Event\" else len(gold_spans_with_type),\n",
    "            \"pred_tokens\": len(pred_tokens) if ann_type != \"Event\" else len(pred_spans_with_type)\n",
    "        }\n",
    "    \n",
    "    def evaluate_document(self, doc: Document, doc_name: str, result_folder: str) -> List[Dict]:\n",
    "        \"\"\"Evaluate all model predictions for a single document against consensus annotations.\"\"\"\n",
    "        gold_annset = doc.annset(\"consensus\")  # Use consensus annotation set as gold standard\n",
    "        \n",
    "        if not gold_annset:\n",
    "            print(f\"Warning: No 'consensus' annotation set found in document {doc_name}\")\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Get all prediction annotation sets\n",
    "        pred_annset_names = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "        \n",
    "        for annset_name in pred_annset_names:\n",
    "            model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "            pred_annset = doc.annset(annset_name)\n",
    "            \n",
    "            # Evaluate each event annotation type\n",
    "            for ann_type in self.event_annotation_types:\n",
    "                metrics = self.calculate_annotation_overlap_metrics(gold_annset, pred_annset, ann_type)\n",
    "                \n",
    "                result = {\n",
    "                    \"result_folder\": result_folder,\n",
    "                    \"document\": doc_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"annotation_type\": ann_type,\n",
    "                    **metrics\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_result_json(self, json_path: Path, result_folder: str):\n",
    "        \"\"\"Process a single result JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                result_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {json_path}: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Extract document identifier\n",
    "        doc_path = result_data.get(\"Document\", \"\")\n",
    "        doc_name = self.extract_doc_name_from_path(doc_path)\n",
    "        \n",
    "        # Find corresponding corpus document\n",
    "        corpus_doc = self.find_corpus_document(doc_name)\n",
    "        if corpus_doc is None:\n",
    "            print(f\"Warning: Could not find corpus document for {doc_name}\")\n",
    "            return\n",
    "        \n",
    "        # Parse LLM event predictions\n",
    "        predictions = self.parse_llm_event_predictions(result_data)\n",
    "        \n",
    "        if not predictions:\n",
    "            print(f\"Warning: No predictions found in {json_path.name}\")\n",
    "            return\n",
    "        \n",
    "        # Debug: Print what we found\n",
    "        print(f\"Found predictions for models: {list(predictions.keys())}\")\n",
    "        for model_name, model_preds in predictions.items():\n",
    "            total_events = sum(len(events) for events in model_preds.values())\n",
    "            print(f\"  {model_name}: {total_events} total events\")\n",
    "            for ann_type, events in model_preds.items():\n",
    "                if events:\n",
    "                    print(f\"    {ann_type}: {len(events)} events\")\n",
    "        \n",
    "        # Add prediction annotations to document\n",
    "        for model_name, model_predictions in predictions.items():\n",
    "            self.add_prediction_annotations(corpus_doc, model_name, model_predictions)\n",
    "        \n",
    "        # Evaluate predictions against consensus annotations\n",
    "        doc_results = self.evaluate_document(corpus_doc, doc_name, result_folder)\n",
    "        self.evaluation_results.extend(doc_results)\n",
    "        \n",
    "        print(f\"Processed {json_path.name}: {len(predictions)} models, {len(doc_results)} evaluations\")\n",
    "    \n",
    "    def run_evaluation(self) -> pd.DataFrame:\n",
    "        \"\"\"Run complete evaluation on all result folders.\"\"\"\n",
    "        result_folders = self.find_result_folders()\n",
    "        \n",
    "        if not result_folders:\n",
    "            print(\"No result folders found in output directory\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Found {len(result_folders)} result folders\")\n",
    "        \n",
    "        for folder in result_folders:\n",
    "            print(f\"\\nProcessing folder: {folder.name}\")\n",
    "            result_jsons = self.find_result_jsons(folder)\n",
    "            \n",
    "            for json_path in result_jsons:\n",
    "                self.process_result_json(json_path, folder.name)\n",
    "        \n",
    "        # Convert results to DataFrame\n",
    "        if self.evaluation_results:\n",
    "            df = pd.DataFrame(self.evaluation_results)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No evaluation results generated\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def generate_summary_report(self, results_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate summary report of evaluation results.\"\"\"\n",
    "        if results_df.empty:\n",
    "            return \"No results to summarize\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"LLM Event Annotation Evaluation Summary\")\n",
    "        report.append(\"=\" * 60)\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_evaluations = len(results_df)\n",
    "        unique_models = results_df['model'].nunique()\n",
    "        unique_docs = results_df['document'].nunique()\n",
    "        unique_ann_types = results_df['annotation_type'].nunique()\n",
    "        \n",
    "        report.append(f\"Total evaluations: {total_evaluations}\")\n",
    "        report.append(f\"Unique models: {unique_models}\")\n",
    "        report.append(f\"Unique documents: {unique_docs}\")\n",
    "        report.append(f\"Annotation types evaluated: {unique_ann_types}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Average F1 by model\n",
    "        report.append(\"Average F1 Scores by Model:\")\n",
    "        model_f1 = results_df.groupby('model')['f1'].mean().sort_values(ascending=False)\n",
    "        for model, f1 in model_f1.items():\n",
    "            report.append(f\"  {model}: {f1:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Average F1 by annotation type\n",
    "        report.append(\"Average F1 Scores by Annotation Type:\")\n",
    "        ann_type_f1 = results_df.groupby('annotation_type')['f1'].mean().sort_values(ascending=False)\n",
    "        for ann_type, f1 in ann_type_f1.items():\n",
    "            report.append(f\"  {ann_type}: {f1:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Best performing model-annotation combinations\n",
    "        report.append(\"Best Model-Annotation Combinations (Top 10 F1):\")\n",
    "        best_combinations = results_df.groupby(['model', 'annotation_type'])['f1'].mean().sort_values(ascending=False).head(10)\n",
    "        for (model, ann_type), f1 in best_combinations.items():\n",
    "            report.append(f\"  {model} - {ann_type}: {f1:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Detailed precision/recall breakdown\n",
    "        report.append(\"Detailed Metrics by Annotation Type:\")\n",
    "        for ann_type in results_df['annotation_type'].unique():\n",
    "            ann_data = results_df[results_df['annotation_type'] == ann_type]\n",
    "            avg_precision = ann_data['precision'].mean()\n",
    "            avg_recall = ann_data['recall'].mean()\n",
    "            avg_f1 = ann_data['f1'].mean()\n",
    "            total_gold = ann_data['gold_count'].sum()\n",
    "            total_pred = ann_data['pred_count'].sum()\n",
    "            \n",
    "            report.append(f\"  {ann_type}:\")\n",
    "            report.append(f\"    Precision: {avg_precision:.3f}\")\n",
    "            report.append(f\"    Recall: {avg_recall:.3f}\")\n",
    "            report.append(f\"    F1: {avg_f1:.3f}\")\n",
    "            report.append(f\"    Total gold annotations: {total_gold}\")\n",
    "            report.append(f\"    Total predicted annotations: {total_pred}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"Starting LLM Event Annotation Results Evaluation...\")\n",
    "    \n",
    "    # Use the globally defined pipeline results folder\n",
    "    evaluator = LLMEventResultsEvaluator(output_dir=PIPELINE_RESULTS_FOLDER)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results_df = evaluator.run_evaluation()\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        # Save results\n",
    "        output_path = \"output/llm_event_evaluation_results.csv\"\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nEvaluation results saved to: {output_path}\")\n",
    "        \n",
    "        # Generate and save summary report\n",
    "        summary = evaluator.generate_summary_report(results_df)\n",
    "        summary_path = \"output/llm_event_evaluation_summary.txt\"\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(summary)\n",
    "        print(f\"Summary report saved to: {summary_path}\")\n",
    "        \n",
    "        # Print summary to console\n",
    "        print(\"\\n\" + summary)\n",
    "        \n",
    "        # Display some basic statistics\n",
    "        print(\"\\nDetailed Results Preview:\")\n",
    "        print(results_df.head(15).to_string(index=False))\n",
    "        \n",
    "    else:\n",
    "        print(\"No results generated. Check if result folders and JSON files exist.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9afde6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus_with_annotations(self):\n",
    "    \"\"\"Save the corpus with all the new model annotations.\"\"\"\n",
    "    try:\n",
    "        from gatenlp.corpora import DirFilesDestination\n",
    "        \n",
    "        # Create output directory for annotated corpus\n",
    "        output_corpus_dir = Path(\"output/annotated_corpus_with_predictions\")\n",
    "        output_corpus_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n💾 Saving annotated corpus to: {output_corpus_dir}\")\n",
    "        \n",
    "        # Use GateNLP's DirFilesDestination to properly save the corpus\n",
    "        with DirFilesDestination(str(output_corpus_dir), ext=\"xml\", fmt=\"gatexml\") as dest:\n",
    "            for doc in self.corpus:\n",
    "                doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "                \n",
    "                # Set a filename for the document based on the original name\n",
    "                # This will override the default path generation\n",
    "                doc.features[\"_relpath\"] = f\"{doc_name}.xml\"\n",
    "                \n",
    "                dest.append(doc)\n",
    "                \n",
    "                # Print summary of annotation sets for this document\n",
    "                annset_summary = []\n",
    "                for annset_name in doc.annset_names():\n",
    "                    if annset_name and not annset_name.endswith(\"_predictions\"):  # Skip temporary sets\n",
    "                        ann_count = len(doc.annset(annset_name))\n",
    "                        if ann_count > 0:\n",
    "                            annset_summary.append(f\"{annset_name}({ann_count})\")\n",
    "                \n",
    "                if annset_summary:\n",
    "                    print(f\"  {doc_name}.xml: {', '.join(annset_summary)}\")\n",
    "        \n",
    "        # Count saved files\n",
    "        saved_files = list(output_corpus_dir.glob(\"*.xml\"))\n",
    "        print(f\"✅ Saved {len(saved_files)} annotated documents\")\n",
    "        \n",
    "        # Create a summary file\n",
    "        summary_file = output_corpus_dir / \"annotation_summary.txt\"\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"Annotated Corpus Summary\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Generated on: {pd.Timestamp.now()}\\n\")\n",
    "            f.write(f\"Total documents: {len(saved_files)}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Annotation Sets Added:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            \n",
    "            # Get all unique annotation set names across all documents\n",
    "            all_annsets = set()\n",
    "            for doc in self.corpus:\n",
    "                for annset_name in doc.annset_names():\n",
    "                    if annset_name and not annset_name.endswith(\"_predictions\"):\n",
    "                        all_annsets.add(annset_name)\n",
    "            \n",
    "            for annset_name in sorted(all_annsets):\n",
    "                if annset_name not in [\"consensus\", \"\"]:  # Skip gold standard and default\n",
    "                    f.write(f\"- {annset_name} (LLM predictions)\\n\")\n",
    "            \n",
    "            f.write(f\"\\nFiles saved to: {output_corpus_dir}\\n\")\n",
    "            f.write(\"\\nTo view in Gate:\\n\")\n",
    "            f.write(\"1. Open Gate Developer\\n\")\n",
    "            f.write(\"2. Load documents from this directory\\n\")\n",
    "            f.write(\"3. View different annotation sets in the annotation sets panel\\n\")\n",
    "        \n",
    "        return str(output_corpus_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving corpus: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Analysis and Better Results Overview\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def create_visual_analysis(results_df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive visual analysis of the evaluation results.\"\"\"\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create a large figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Heatmap: F1 scores by Model and Annotation Type\n",
    "    plt.subplot(3, 3, 1)\n",
    "    pivot_f1 = results_df.pivot_table(values='f1', index='model', columns='annotation_type', aggfunc='mean')\n",
    "    sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='RdYlGn', center=0.5, \n",
    "                cbar_kws={'label': 'F1 Score'})\n",
    "    plt.title('F1 Scores by Model and Annotation Type')\n",
    "    plt.xlabel('Annotation Type')\n",
    "    plt.ylabel('Model')\n",
    "    \n",
    "    # 2. Bar plot: Average F1 by Model\n",
    "    plt.subplot(3, 3, 2)\n",
    "    model_f1 = results_df.groupby('model')['f1'].mean().sort_values(ascending=True)\n",
    "    model_f1.plot(kind='barh', color='skyblue')\n",
    "    plt.title('Average F1 Score by Model')\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. Bar plot: Average F1 by Annotation Type\n",
    "    plt.subplot(3, 3, 3)\n",
    "    ann_f1 = results_df.groupby('annotation_type')['f1'].mean().sort_values(ascending=True)\n",
    "    ann_f1.plot(kind='barh', color='lightcoral')\n",
    "    plt.title('Average F1 Score by Annotation Type')\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 4. Precision vs Recall scatter plot\n",
    "    plt.subplot(3, 3, 4)\n",
    "    for model in results_df['model'].unique():\n",
    "        model_data = results_df[results_df['model'] == model]\n",
    "        plt.scatter(model_data['recall'], model_data['precision'], \n",
    "                   label=model, alpha=0.7, s=60)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision vs Recall by Model')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 5. Distribution of F1 scores\n",
    "    plt.subplot(3, 3, 5)\n",
    "    results_df.boxplot(column='f1', by='model', ax=plt.gca())\n",
    "    plt.title('F1 Score Distribution by Model')\n",
    "    plt.suptitle('')  # Remove the default title\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('F1 Score')\n",
    "    \n",
    "    # 6. Document-level performance heatmap\n",
    "    plt.subplot(3, 3, 6)\n",
    "    doc_model_f1 = results_df.groupby(['document', 'model'])['f1'].mean().reset_index()\n",
    "    doc_model_pivot = doc_model_f1.pivot(index='document', columns='model', values='f1')\n",
    "    sns.heatmap(doc_model_pivot, annot=True, fmt='.2f', cmap='RdYlGn', center=0.3,\n",
    "                cbar_kws={'label': 'F1 Score'})\n",
    "    plt.title('F1 Scores by Document and Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Document')\n",
    "    \n",
    "    # 7. Gold vs Predicted annotations count\n",
    "    plt.subplot(3, 3, 7)\n",
    "    total_gold = results_df.groupby('model')['gold_count'].sum()\n",
    "    total_pred = results_df.groupby('model')['pred_count'].sum()\n",
    "    \n",
    "    x = np.arange(len(total_gold))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, total_gold.values, width, label='Gold Standard', color='gold', alpha=0.8)\n",
    "    plt.bar(x + width/2, total_pred.values, width, label='Predicted', color='steelblue', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Total Annotations')\n",
    "    plt.title('Gold vs Predicted Annotation Counts')\n",
    "    plt.xticks(x, total_gold.index, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 8. Annotation type performance comparison\n",
    "    plt.subplot(3, 3, 8)\n",
    "    ann_metrics = results_df.groupby('annotation_type')[['precision', 'recall', 'f1']].mean()\n",
    "    ann_metrics.plot(kind='bar', ax=plt.gca(), color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "    plt.title('Average Metrics by Annotation Type')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 9. Model performance ranking\n",
    "    plt.subplot(3, 3, 9)\n",
    "    model_ranking = results_df.groupby('model')[['precision', 'recall', 'f1']].mean().sort_values('f1', ascending=False)\n",
    "    model_ranking.plot(kind='bar', ax=plt.gca(), color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "    plt.title('Model Performance Ranking')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_detailed_results_table(results_df: pd.DataFrame):\n",
    "    \"\"\"Create a detailed results table with better formatting.\"\"\"\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"No results to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"DETAILED EVALUATION RESULTS\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    # Summary by model\n",
    "    print(\"\\n📊 SUMMARY BY MODEL:\")\n",
    "    print(\"-\" * 80)\n",
    "    model_summary = results_df.groupby('model').agg({\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'gold_count': 'sum',\n",
    "        'pred_count': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    # Flatten column names\n",
    "    model_summary.columns = ['_'.join(col).strip() for col in model_summary.columns.values]\n",
    "    print(model_summary.to_string())\n",
    "    \n",
    "    # Summary by annotation type\n",
    "    print(\"\\n\\n📋 SUMMARY BY ANNOTATION TYPE:\")\n",
    "    print(\"-\" * 80)\n",
    "    ann_summary = results_df.groupby('annotation_type').agg({\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'], \n",
    "        'f1': ['mean', 'std'],\n",
    "        'gold_count': 'sum',\n",
    "        'pred_count': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    ann_summary.columns = ['_'.join(col).strip() for col in ann_summary.columns.values]\n",
    "    print(ann_summary.to_string())\n",
    "    \n",
    "    # Best and worst performers\n",
    "    print(\"\\n\\n🏆 TOP PERFORMERS:\")\n",
    "    print(\"-\" * 50)\n",
    "    top_performers = results_df.nlargest(10, 'f1')[['model', 'annotation_type', 'document', 'f1', 'precision', 'recall']]\n",
    "    print(top_performers.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\\n⚠️  LOWEST PERFORMERS:\")\n",
    "    print(\"-\" * 50)\n",
    "    low_performers = results_df.nsmallest(10, 'f1')[['model', 'annotation_type', 'document', 'f1', 'precision', 'recall']]\n",
    "    print(low_performers.to_string(index=False))\n",
    "    \n",
    "    # Document-level analysis\n",
    "    print(\"\\n\\n📄 DOCUMENT-LEVEL ANALYSIS:\")\n",
    "    print(\"-\" * 80)\n",
    "    doc_analysis = results_df.groupby('document').agg({\n",
    "        'f1': ['mean', 'std', 'min', 'max'],\n",
    "        'model': 'count'\n",
    "    }).round(3)\n",
    "    \n",
    "    doc_analysis.columns = ['_'.join(col).strip() for col in doc_analysis.columns.values]\n",
    "    doc_analysis = doc_analysis.rename(columns={'model_count': 'num_evaluations'})\n",
    "    print(doc_analysis.to_string())\n",
    "\n",
    "def enhanced_main():\n",
    "    \"\"\"Enhanced main function with better visualization.\"\"\"\n",
    "    print(\"Starting LLM Event Annotation Results Evaluation...\")\n",
    "    \n",
    "    # Use the globally defined pipeline results folder\n",
    "    evaluator = LLMEventResultsEvaluator(output_dir=PIPELINE_RESULTS_FOLDER)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results_df = evaluator.run_evaluation()\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        # Save results\n",
    "        output_path = \"output/llm_event_evaluation_results.csv\"\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nEvaluation results saved to: {output_path}\")\n",
    "        \n",
    "        # Generate and save summary report\n",
    "        summary = evaluator.generate_summary_report(results_df)\n",
    "        summary_path = \"output/llm_event_evaluation_summary.txt\"\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(summary)\n",
    "        print(f\"Summary report saved to: {summary_path}\")\n",
    "        \n",
    "        # Create detailed results table\n",
    "        create_detailed_results_table(results_df)\n",
    "        \n",
    "        # Create visual analysis\n",
    "        print(\"\\n\\n📈 GENERATING VISUAL ANALYSIS...\")\n",
    "        create_visual_analysis(results_df)\n",
    "        \n",
    "        # Print basic summary\n",
    "        print(f\"\\n\\n📊 QUICK STATS:\")\n",
    "        print(f\"Total evaluations: {len(results_df)}\")\n",
    "        print(f\"Models evaluated: {results_df['model'].nunique()}\")\n",
    "        print(f\"Documents processed: {results_df['document'].nunique()}\")\n",
    "        print(f\"Annotation types: {', '.join(results_df['annotation_type'].unique())}\")\n",
    "        print(f\"Average F1 score: {results_df['f1'].mean():.3f}\")\n",
    "        print(f\"Best performing model: {results_df.groupby('model')['f1'].mean().idxmax()}\")\n",
    "        print(f\"Best annotation type: {results_df.groupby('annotation_type')['f1'].mean().idxmax()}\")\n",
    "        \n",
    "        print(f\"\\n🎯 OUTPUTS GENERATED:\")\n",
    "        print(f\"📊 Evaluation results: output/llm_event_evaluation_results.csv\")\n",
    "        print(f\"📋 Summary report: output/llm_event_evaluation_summary.txt\")\n",
    "        print(f\"📁 Annotated corpus: output/annotated_corpus_with_predictions/\")\n",
    "        print(f\"   → Open these XML files in Gate to view model predictions!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No results generated. Check if result folders and JSON files exist.\")\n",
    "\n",
    "# Run the enhanced analysis\n",
    "if __name__ == \"__main__\":\n",
    "    enhanced_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all documents with predictions using JSON format\n",
    "def save_all_documents_with_predictions():\n",
    "    \"\"\"Save all documents that have predictions using JSON format.\"\"\"\n",
    "    \n",
    "    # Load corpus and process predictions using the global folder\n",
    "    evaluator = LLMEventResultsEvaluator(PIPELINE_RESULTS_FOLDER)\n",
    "    result_folders = evaluator.find_result_folders()\n",
    "    \n",
    "    if result_folders:\n",
    "        for folder in result_folders:\n",
    "            result_jsons = evaluator.find_result_jsons(folder)\n",
    "            for json_path in result_jsons:\n",
    "                evaluator.process_result_json(json_path, folder.name)\n",
    "    \n",
    "    output_dir = Path(\"output/annotated_corpus_with_predictions\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    saved_count = 0\n",
    "    \n",
    "    # Process all documents\n",
    "    for doc in evaluator.corpus:\n",
    "        doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "        pred_annsets = [name for name in doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "        \n",
    "        if pred_annsets:\n",
    "            print(f\"📋 Processing document: {doc_name}\")\n",
    "            \n",
    "            # Create permanent annotation sets\n",
    "            for annset_name in pred_annsets:\n",
    "                model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "                pred_annset = doc.annset(annset_name)\n",
    "                permanent_annset = doc.annset(model_name)\n",
    "                permanent_annset.clear()\n",
    "                \n",
    "                for ann in pred_annset:\n",
    "                    features = dict(ann.features)\n",
    "                    features.pop(\"source\", None)\n",
    "                    features.pop(\"model\", None)\n",
    "                    permanent_annset.add(ann.start, ann.end, ann.type, features)\n",
    "                \n",
    "                print(f\"  ✅ Created {model_name} annotation set: {len(pred_annset)} annotations\")\n",
    "            \n",
    "            # Save document as JSON\n",
    "            try:\n",
    "                output_file = output_dir / f\"{doc_name}.json\"\n",
    "                doc.save(str(output_file), fmt=\"json\")\n",
    "                saved_count += 1\n",
    "                print(f\"  💾 Saved: {output_file.name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Failed to save {doc_name}: {e}\")\n",
    "    \n",
    "    # List final results\n",
    "    files = list(output_dir.glob(\"*.json\"))\n",
    "    print(f\"\\n✅ Successfully saved {len(files)} documents with predictions!\")\n",
    "    print(f\"\\n📁 Files in {output_dir}:\")\n",
    "    for file in files:\n",
    "        print(f\"  📄 {file.name}\")\n",
    "    \n",
    "    # Update summary file\n",
    "    summary_file = output_dir / \"annotation_summary.txt\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Annotated Corpus Summary\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Generated on: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"Total documents: {len(files)}\\n\")\n",
    "        f.write(f\"Format: JSON (GateNLP BDOC format)\\n\\n\")\n",
    "        f.write(\"Model annotation sets included:\\n\")\n",
    "        f.write(\"- gemma3:1b\\n\")\n",
    "        f.write(\"- gemma3:12b\\n\") \n",
    "        f.write(\"- mistral:latest\\n\\n\")\n",
    "        f.write(\"To view in Gate:\\n\")\n",
    "        f.write(\"1. Open Gate Developer\\n\")\n",
    "        f.write(\"2. Load documents from this directory\\n\")\n",
    "        f.write(\"3. Select JSON/BDOC format when loading\\n\")\n",
    "        f.write(\"4. View different annotation sets in the annotation sets panel\\n\")\n",
    "    \n",
    "    return str(output_dir)\n",
    "\n",
    "# Run the complete saving\n",
    "result = save_all_documents_with_predictions()\n",
    "print(f\"\\n🎯 FINAL SUCCESS! Annotated corpus saved to: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus for viewing with GateNLP CorpusViewer\n",
    "from GatenlpUtils import loadCorpus\n",
    "from gatenlp.visualization import CorpusViewer\n",
    "\n",
    "# Load the corpus\n",
    "corpus = loadCorpus()\n",
    "print(f\"Loaded corpus with {len(corpus)} documents\")\n",
    "\n",
    "# Show what annotation sets are available\n",
    "print(\"\\nAvailable annotation sets:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_name = Path(doc.features.get(\"gate.SourceURL\", \"\")).stem\n",
    "    annsets = [name for name in doc.annset_names() if name]\n",
    "    if annsets:\n",
    "        print(f\"  Doc {i+1} ({doc_name}): {annsets}\")\n",
    "\n",
    "# Start the corpus viewer\n",
    "print(\"\\nStarting CorpusViewer...\")\n",
    "viewer = CorpusViewer(corpus)\n",
    "viewer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bd03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus WITH model predictions for comparison\n",
    "def load_corpus_with_predictions():\n",
    "    \"\"\"Load corpus and add the model predictions for viewing.\"\"\"\n",
    "    \n",
    "    # Start fresh\n",
    "    corpus_with_preds = loadCorpus()\n",
    "    \n",
    "    # Add predictions using the global folder\n",
    "    evaluator = LLMEventResultsEvaluator(PIPELINE_RESULTS_FOLDER)\n",
    "    result_folders = evaluator.find_result_folders()\n",
    "    \n",
    "    if result_folders:\n",
    "        for folder in result_folders:\n",
    "            result_jsons = evaluator.find_result_jsons(folder)\n",
    "            for json_path in result_jsons:\n",
    "                evaluator.process_result_json(json_path, folder.name)\n",
    "    \n",
    "    # Copy predictions to our viewing corpus\n",
    "    docs_with_preds = 0\n",
    "    for i, doc in enumerate(corpus_with_preds):\n",
    "        if i < len(evaluator.corpus):\n",
    "            evaluator_doc = evaluator.corpus[i]\n",
    "            pred_annsets = [name for name in evaluator_doc.annset_names() if name.endswith(\"_predictions\")]\n",
    "            \n",
    "            if pred_annsets:\n",
    "                docs_with_preds += 1\n",
    "                for annset_name in pred_annsets:\n",
    "                    model_name = annset_name.replace(\"_predictions\", \"\")\n",
    "                    pred_annset = evaluator_doc.annset(annset_name)\n",
    "                    \n",
    "                    # Create permanent annotation set\n",
    "                    permanent_annset = doc.annset(model_name)\n",
    "                    permanent_annset.clear()\n",
    "                    \n",
    "                    for ann in pred_annset:\n",
    "                        features = dict(ann.features)\n",
    "                        features.pop(\"source\", None)\n",
    "                        features.pop(\"model\", None)\n",
    "                        permanent_annset.add(ann.start, ann.end, ann.type, features)\n",
    "    \n",
    "    print(f\"Added predictions to {docs_with_preds} documents\")\n",
    "    return corpus_with_preds\n",
    "\n",
    "corpus_with_predictions = load_corpus_with_predictions()\n",
    "viewer_with_preds = CorpusViewer(corpus_with_predictions)\n",
    "viewer_with_preds.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
