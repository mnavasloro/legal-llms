{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gatenlp import Document\n",
    "from gatenlp.corpora import ListCorpus\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from GatenlpUtils import loadCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    load_dotenv()\n",
    "\n",
    "    user_email = os.getenv(\"USEREMAIL\")  # Enter your email here\n",
    "    password = os.getenv(\"PASSWORD\")  # Enter your password here\n",
    "\n",
    "    # Fetch Access Token\n",
    "\n",
    "    # Define the URL for the authentication endpoint\n",
    "    auth_url = \"http://localhost:8080/api/v1/auths/signin\"\n",
    "\n",
    "    # Define the payload with user credentials\n",
    "    auth_payload = json.dumps({\"email\": user_email, \"password\": \"admin\"})\n",
    "\n",
    "    # Define the headers for the authentication request\n",
    "    auth_headers = {\"accept\": \"application/json\", \"content-type\": \"application/json\"}\n",
    "\n",
    "    # Make the POST request to fetch the access token\n",
    "    auth_response = requests.post(auth_url, data=auth_payload, headers=auth_headers)\n",
    "\n",
    "    # Extract the access token from the response\n",
    "    access_token = auth_response.json().get(\"token\")\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event(BaseModel):\n",
    "  source_text: str\n",
    "  event: str\n",
    "  event_who: str\n",
    "  event_when: str\n",
    "  event_what: str\n",
    "  event_type: str\n",
    "\n",
    "class EventList(BaseModel):\n",
    "  events: list[Event]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askChatbot(model, role, instruction, content):\n",
    "    chat_url = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "    # Define the headers for the chat completion request, including the access token\n",
    "    chat_headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "    }\n",
    "\n",
    "    # Define the payload for the chat completion request\n",
    "    chat_payload = json.dumps(\n",
    "        {\n",
    "            \"stream\": False,\n",
    "            \"model\": model,\n",
    "            \"temperature\": 0.0,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": role},  # System role with additional context\n",
    "                {\"role\": \"user\", \"content\": f\"{instruction}\\n\\n{content}\"},  # User message with instruction and text\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Make the POST request to the chat completion endpoint\n",
    "    chat_response = requests.post(chat_url, data=chat_payload, headers=chat_headers)\n",
    "    #print(chat_response.json()[\"message\"][\"content\"])\n",
    "    structured_response = EventList.model_validate_json(chat_response.json())\n",
    "    return chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askChatbotLocal(model, role, instruction, content):\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model = model,\n",
    "            options = {\n",
    "                'temperature': 0\n",
    "            }, \n",
    "            format = EventList.model_json_schema(),  # Use Pydantic to generate the schema or format=schema\n",
    "            messages=\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": role},  # System role with additional context\n",
    "                {\"role\": \"user\", \"content\": f\"{instruction}\\n\\n{content}\"},  # User message with instruction and text\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        chat_response = response['message']['content']\n",
    "        structured_response = EventList.model_validate_json(response.message.content)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with model {model}: {str(e)}\")\n",
    "\n",
    "    return chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input/updated/annotated\\dev\\CASE OF ALTAY v. TURKEY (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BELYAYEV AND OTHERS v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BIGUN v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CABUCAK v. GERMANY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CAN v. TURKEY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CRISTIAN CATALIN UNGUREANU v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF DOKTOROV v. BULGARIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF EGILL EINARSSON v. ICELAND (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF HOINESS v. NORWAY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF KOSAITE - CYPIENE AND OTHERS v. LITHUANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF LOZOVYYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF M.T. v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MOSKALEV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MURUZHEVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF NODI v. HUNGARY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF O.C.I. AND OTHERS v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF OTGON v. THE REPUBLIC OF MOLDOVA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PAKHTUSOV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PANYUSHKINY v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF RESIN v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.N. v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.V. v. ITALY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SHVIDKIYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SIDOROVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SOLCAN v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF STANA v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF VISY v. SLOVAKIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YAKUSHEV v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YERMAKOVICH v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YEVGENIY ZAKHAROV v. RUSSIA.xml into corpus\n",
      "All documents loaded into the corpus.\n",
      "Documents in corpus: 30\n"
     ]
    }
   ],
   "source": [
    "corpus = loadCorpus()\n",
    "print(f\"Documents in corpus: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemma3:12b\",\n",
    "          \"mistral:latest\"\n",
    "]\n",
    "\"\"\"\n",
    "models = [\"gemma3:1b\",\n",
    "          \"gemma3:12b\",\n",
    "          \"chevalblanc/claude-3-haiku:latest\",\n",
    "          \"incept5/llama3.1-claude:latest\",\n",
    "          \"llama3.3:latest\",\n",
    "          \"deepseek-r1:8b\",\n",
    "          \"mistral:latest\"\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "event_definitions = \"\"\"\n",
    "You are an expert in legal text analysis. Here are the definitions of legal events:\n",
    "- Event: Relates to the extent of text containing contextual event-related information. \n",
    "- Event_who: Corresponds to the subject of the event, which can either be a subject, but also an object (i.e., an application). \n",
    "    Examples: applicant, respondent, judge, witness\n",
    "- Event_what: Corresponds to the main verb reflecting the baseline of all the paragraph. Additionally, we include thereto a complementing verb or object whenever the core verb is not self-explicit or requires an extension to attain a sufficient meaning.\n",
    "    Examples: lodged an application, decided, ordered, dismissed\n",
    "- Event_when: Refers to the date of the event, or to any temporal reference thereto.\n",
    "- Event_circumstance: Meaning that the event correspond to the facts under judgment.\n",
    "- Event_procedure: The events belongs to the procedural dimension of the case.\n",
    "\n",
    "Events contain the annotations event_who, event_what and event_when. Events can be of type event_circumstance and event_procedure.\n",
    "\"\"\"\n",
    "\n",
    "instruction = \"Analyze the provided text and extract the legal events. Provide the results in a structured format. Obviously, Event_who, Event_what and Event_when can only appear within an Event. If you find an event, also classify it into an event_circumstance or event_procedure. Do not invent additional information.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document: file:/C:/Users/mnavas/CASE%20OF%20ALTAY%20v.%20TURKEY%20(No.%202).docx\n",
      "Using model: gemma3:12b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:56:54,590|INFO|httpx|HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: mistral:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:57:12,198|INFO|httpx|HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "Processing documents:   3%|▎         | 1/30 [02:53<1:24:03, 173.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document: file:/C:/Users/mnavas/CASE%20OF%20BELYAYEV%20AND%20OTHERS%20v.%20UKRAINE.docx\n",
      "Using model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "viaWeb = False\n",
    "results = []\n",
    "counter = 0\n",
    "\n",
    "# Iterate over documents and models\n",
    "for doc in tqdm(corpus, desc=\"Processing documents\"):\n",
    "    if counter >= 2 :  # Limit to 10 documents for testing\n",
    "        break\n",
    "\n",
    "    doc_dict = {\"Document\": doc.features.get(\"gate.SourceURL\")}\n",
    "    print(f\"Processing document: {doc.features.get(\"gate.SourceURL\")}\")\n",
    "    \n",
    "    # Combine all procedure texts for the document\n",
    "    procedure_texts = []\n",
    "    annotations = doc.annset(\"Section\")\n",
    "    # Procedure\n",
    "    procedure_annotations = annotations.with_type(\"Procedure\")\n",
    "    for ann in procedure_annotations:\n",
    "        procedure_text = doc.text[ann.start:ann.end]\n",
    "        procedure_texts.append(procedure_text)\n",
    "    # Circumstances\n",
    "    circumstances_annotations = annotations.with_type(\"Circumstances\")\n",
    "    for ann in circumstances_annotations:\n",
    "        procedure_text = doc.text[ann.start:ann.end]\n",
    "        procedure_texts.append(procedure_text)\n",
    "    # Decision\n",
    "    decision_annotations = annotations.with_type(\"Decision\")\n",
    "    for ann in decision_annotations:\n",
    "        procedure_text = doc.text[ann.start:ann.end]\n",
    "        procedure_texts.append(procedure_text)\n",
    "\n",
    "    combined_procedure_text = \" \".join(procedure_texts)\n",
    "    #print(f\"Combined procedure text: {combined_procedure_text}\")\n",
    "    \n",
    "    annotations_list = []\n",
    "    # Iterate over models\n",
    "    for model in models:\n",
    "        try:\n",
    "            print(f\"Using model: {model}\")\n",
    "            \n",
    "            # Call the chatbot with role, instruction, and content\n",
    "            if viaWeb == True:\n",
    "                # via WebUI\n",
    "                chat_response = askChatbot(model, event_definitions, instruction, combined_procedure_text)\n",
    "                # Extract and store the response\n",
    "                response_content = chat_response.json().get(\"message\", {}).get(\"content\", \"No response content\")\n",
    "            else:\n",
    "                # without WebUI\n",
    "                chat_response = askChatbotLocal(model, event_definitions, instruction, combined_procedure_text)\n",
    "                response_content = chat_response\n",
    "            \n",
    "            #print(f\"Response from {model}:\\n{response_content}\")\n",
    "            #doc_dict[\"annotations\"] = {\n",
    "            #    \"model_name\": model,\n",
    "            #   \"events\": response_content,\n",
    "                #\"events\": response_content.get(\"events\", []) if isinstance(response_content, dict) else []\n",
    "            #}\n",
    "            annotations_list.append({\"model_name\": model, \"events\": response_content})\n",
    "            \n",
    "        except Exception as e:\n",
    "            with open(\"error.txt\", \"a\") as file:\n",
    "                file.write(f\"Error with model {model}: {str(e)}\")\n",
    "            file.close()\n",
    "        \n",
    "    doc_dict[\"annotations\"] = annotations_list\n",
    "    # Append the document dictionary to the results list\n",
    "    results.append(doc_dict)\n",
    "    \n",
    "    # for testing only\n",
    "    #counter += 1\n",
    "    \n",
    "    \n",
    "\n",
    "# Save results\n",
    "#df = pd.DataFrame(results)\n",
    "# CSV\n",
    "#df.to_csv(\"chat_responses_with_instructions.csv\", index=False)\n",
    "# Excel\n",
    "#df.to_excel(\"chat_responses_with_instructions.xlsx\", index=False)\n",
    "# JSON\n",
    "for doc in results:\n",
    "    for ann in doc[\"annotations\"]:\n",
    "        # If events is a string, parse it to dict, then extract the list under \"events\"\n",
    "        if isinstance(ann[\"events\"], str):\n",
    "            try:\n",
    "                parsed = json.loads(ann[\"events\"])\n",
    "                # If the parsed object has an \"events\" key, use its value (a list)\n",
    "                if isinstance(parsed, dict) and \"events\" in parsed:\n",
    "                    ann[\"events\"] = parsed[\"events\"]\n",
    "                else:\n",
    "                    ann[\"events\"] = parsed\n",
    "            except Exception as e:\n",
    "                ann[\"events\"] = []  # fallback to empty list if parsing fails\n",
    "\n",
    "with open(\"chat_responses_with_instructions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
