{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91b2e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from GatenlpUtils import loadCorpus\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('ie_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for the IE processing pipeline\"\"\"\n",
    "    max_documents: int = 10\n",
    "    max_retries: int = 3\n",
    "    retry_delay: float = 1.0\n",
    "    temperature: float = 0.0\n",
    "    output_dir: str = \"output\"\n",
    "    backup_dir: str = \"backup\"\n",
    "    via_web: bool = False\n",
    "    batch_size: int = 1\n",
    "    reserve_tokens: int = 1000\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Create output directories\n",
    "        Path(self.output_dir).mkdir(exist_ok=True)\n",
    "        Path(self.backup_dir).mkdir(exist_ok=True)\n",
    "\n",
    "class TokenCounter:\n",
    "    \"\"\"Utility class for counting tokens accurately\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        except:\n",
    "            self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        try:\n",
    "            return len(self.encoding.encode(text))\n",
    "        except:\n",
    "            return len(text) // 4  # Fallback estimation\n",
    "    \n",
    "    def check_context_fit(self, text: str, max_context: int, reserve_tokens: int = 1000) -> bool:\n",
    "        \"\"\"Check if text fits within context length\"\"\"\n",
    "        tokens = self.count_tokens(text)\n",
    "        return tokens <= (max_context - reserve_tokens)\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Manages model configurations and context lengths\"\"\"\n",
    "    \n",
    "    MODEL_CONTEXTS = {\n",
    "        \"gemma3:1b\": 8192,\n",
    "        \"gemma3:12b\": 8192,\n",
    "        \"mistral:latest\": 32768,\n",
    "        \"llama3.3:latest\": 128000,\n",
    "        \"deepseek-r1:8b\": 128000,\n",
    "        \"chevalblanc/claude-3-haiku:latest\": 200000,\n",
    "        \"incept5/llama3.1-claude:latest\": 128000,\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.token_counter = TokenCounter()\n",
    "    \n",
    "    def get_context_length(self, model: str) -> int:\n",
    "        \"\"\"Get context length for a model\"\"\"\n",
    "        return self.MODEL_CONTEXTS.get(model, 8192)  # Default to 8192\n",
    "    \n",
    "    def can_process_text(self, model: str, text: str, reserve_tokens: int = 1000) -> bool:\n",
    "        \"\"\"Check if model can process the given text\"\"\"\n",
    "        context_length = self.get_context_length(model)\n",
    "        return self.token_counter.check_context_fit(text, context_length, reserve_tokens)\n",
    "\n",
    "# Initialize components\n",
    "config = ProcessingConfig()\n",
    "model_manager = ModelManager()\n",
    "token_counter = TokenCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "423351da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom JSON encoder to handle Pydantic models and other complex objects\"\"\"\n",
    "    \n",
    "    def default(self, obj):\n",
    "        # Handle Pydantic models\n",
    "        if hasattr(obj, 'model_dump'):\n",
    "            return obj.model_dump()\n",
    "        elif hasattr(obj, 'dict'):\n",
    "            return obj.dict()\n",
    "        # Handle datetime objects\n",
    "        elif isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        # Handle Path objects\n",
    "        elif isinstance(obj, Path):\n",
    "            return str(obj)\n",
    "        # Handle other non-serializable objects\n",
    "        try:\n",
    "            return super().default(obj)\n",
    "        except TypeError:\n",
    "            return str(obj)\n",
    "\n",
    "def safe_json_dump(obj, file_path: Path, **kwargs):\n",
    "    \"\"\"Safely dump JSON with custom encoder\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(obj, f, cls=CustomJSONEncoder, ensure_ascii=False, **kwargs)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write JSON to {file_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef6e7c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event(BaseModel):\n",
    "  source_text: str\n",
    "  event: str\n",
    "  event_who: str\n",
    "  event_when: str\n",
    "  event_what: str\n",
    "  event_type: str\n",
    "\n",
    "class EventList(BaseModel):\n",
    "  events: list[Event]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e11a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemma3:12b\",\n",
    "          \"mistral:latest\"\n",
    "]\n",
    "\"\"\"\n",
    "models = [\"gemma3:1b\",\n",
    "          \"gemma3:12b\",\n",
    "          \"chevalblanc/claude-3-haiku:latest\",\n",
    "          \"incept5/llama3.1-claude:latest\",\n",
    "          \"llama3.3:latest\",\n",
    "          \"deepseek-r1:8b\",\n",
    "          \"mistral:latest\"\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "event_definitions = \"\"\"\n",
    "You are an expert in legal text analysis. Here are the definitions of legal events:\n",
    "- Event: Relates to the extent of text containing contextual event-related information. \n",
    "- Event_who: Corresponds to the subject of the event, which can either be a subject, but also an object (i.e., an application). \n",
    "    Examples: applicant, respondent, judge, witness\n",
    "- Event_what: Corresponds to the main verb reflecting the baseline of all the paragraph. Additionally, we include thereto a complementing verb or object whenever the core verb is not self-explicit or requires an extension to attain a sufficient meaning.\n",
    "    Examples: lodged an application, decided, ordered, dismissed\n",
    "- Event_when: Refers to the date of the event, or to any temporal reference thereto.\n",
    "- Event_circumstance: Meaning that the event correspond to the facts under judgment.\n",
    "- Event_procedure: The events belongs to the procedural dimension of the case.\n",
    "\n",
    "Events contain the annotations event_who, event_what and event_when. Events can be of type event_circumstance and event_procedure.\n",
    "\"\"\"\n",
    "\n",
    "instruction = \"Analyze the provided text and extract the legal events. Provide the results in a structured format. Obviously, Event_who, Event_what and Event_when can only appear within an Event. If you find an event, also classify it into an event_circumstance or event_procedure. Do not invent additional information.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79c2a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    load_dotenv()\n",
    "\n",
    "    user_email = os.getenv(\"USEREMAIL\")  # Enter your email here\n",
    "    password = os.getenv(\"PASSWORD\")  # Enter your password here\n",
    "\n",
    "    # Fetch Access Token\n",
    "\n",
    "    # Define the URL for the authentication endpoint\n",
    "    auth_url = \"http://localhost:8080/api/v1/auths/signin\"\n",
    "\n",
    "    # Define the payload with user credentials\n",
    "    auth_payload = json.dumps({\"email\": user_email, \"password\": \"admin\"})\n",
    "\n",
    "    # Define the headers for the authentication request\n",
    "    auth_headers = {\"accept\": \"application/json\", \"content-type\": \"application/json\"}\n",
    "\n",
    "    # Make the POST request to fetch the access token\n",
    "    auth_response = requests.post(auth_url, data=auth_payload, headers=auth_headers)\n",
    "\n",
    "    # Extract the access token from the response\n",
    "    access_token = auth_response.json().get(\"token\")\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "833aa7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askChatbotImproved(model: str, role: str, instruction: str, content: str, \n",
    "                      max_retries: int = 3, retry_delay: float = 1.0) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Improved chatbot function with better error handling and retries\n",
    "    \"\"\"\n",
    "    chat_url = \"http://localhost:11434/api/chat\"\n",
    "    \n",
    "    # Check if content fits in model context\n",
    "    if not model_manager.can_process_text(model, f\"{role}\\n{instruction}\\n{content}\", config.reserve_tokens):\n",
    "        logger.warning(f\"Text too long for model {model} context. Tokens: {token_counter.count_tokens(content)}\")\n",
    "        return None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_headers = {\n",
    "                \"accept\": \"application/json\",\n",
    "                \"content-type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {access_token}\",\n",
    "            }\n",
    "            \n",
    "            chat_payload = {\n",
    "                \"stream\": False,\n",
    "                \"model\": model,\n",
    "                \"temperature\": config.temperature,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": role},\n",
    "                    {\"role\": \"user\", \"content\": f\"{instruction}\\n\\n{content}\"},\n",
    "                ],\n",
    "            }\n",
    "            \n",
    "            response = requests.post(chat_url, json=chat_payload, headers=chat_headers, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            response_data = response.json()\n",
    "            content = response_data.get(\"message\", {}).get(\"content\", \"\")\n",
    "            \n",
    "            if content:\n",
    "                # Validate JSON structure\n",
    "                try:\n",
    "                    structured_response = EventList.model_validate_json(content)\n",
    "                    logger.info(f\"Successfully processed with {model} on attempt {attempt + 1}\")\n",
    "                    return {\n",
    "                        \"content\": content, \n",
    "                        \"structured\": structured_response.model_dump() if hasattr(structured_response, 'model_dump') else structured_response.dict()\n",
    "                    }\n",
    "                except Exception as validation_error:\n",
    "                    logger.warning(f\"Validation error with {model}: {validation_error}\")\n",
    "                    return {\"content\": content, \"structured\": None}\n",
    "            else:\n",
    "                logger.warning(f\"Empty response from {model}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Request error with {model} (attempt {attempt + 1}): {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error with {model} (attempt {attempt + 1}): {e}\")\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "    \n",
    "    logger.error(f\"Failed to get response from {model} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "def askChatbotLocalImproved(model: str, role: str, instruction: str, content: str, \n",
    "                           max_retries: int = 3, retry_delay: float = 1.0) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Improved local chatbot function with better error handling\n",
    "    \"\"\"\n",
    "    # Check if content fits in model context\n",
    "    if not model_manager.can_process_text(model, f\"{role}\\n{instruction}\\n{content}\", config.reserve_tokens):\n",
    "        logger.warning(f\"Text too long for model {model} context. Tokens: {token_counter.count_tokens(content)}\")\n",
    "        return None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                options={'temperature': config.temperature},\n",
    "                format=EventList.model_json_schema(),\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": role},\n",
    "                    {\"role\": \"user\", \"content\": f\"{instruction}\\n\\n{content}\"},\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            content = response['message']['content']\n",
    "            \n",
    "            if content:\n",
    "                try:\n",
    "                    structured_response = EventList.model_validate_json(content)\n",
    "                    logger.info(f\"Successfully processed with {model} on attempt {attempt + 1}\")\n",
    "                    return {\n",
    "                        \"content\": content, \n",
    "                        \"structured\": structured_response.model_dump() if hasattr(structured_response, 'model_dump') else structured_response.dict()\n",
    "                    }\n",
    "                except Exception as validation_error:\n",
    "                    logger.warning(f\"Validation error with {model}: {validation_error}\")\n",
    "                    return {\"content\": content, \"structured\": None}\n",
    "            else:\n",
    "                logger.warning(f\"Empty response from {model}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error with {model} (attempt {attempt + 1}): {e}\")\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "    \n",
    "    logger.error(f\"Failed to get response from {model} after {max_retries} attempts\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdcdc5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_document_sections(doc) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract sections from a document with proper error handling\n",
    "    \"\"\"\n",
    "    sections = {\n",
    "        \"procedure\": \"\",\n",
    "        \"circumstances\": \"\",\n",
    "        \"decision\": \"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        annotations = doc.annset(\"Section\")\n",
    "        if not annotations:\n",
    "            logger.warning(\"No Section annotations found in document\")\n",
    "            return sections\n",
    "        \n",
    "        # Extract each section type\n",
    "        section_types = [\n",
    "            (\"Procedure\", \"procedure\"),\n",
    "            (\"Circumstances\", \"circumstances\"),\n",
    "            (\"Decision\", \"decision\")\n",
    "        ]\n",
    "        \n",
    "        for gate_type, key in section_types:\n",
    "            section_annotations = annotations.with_type(gate_type)\n",
    "            if section_annotations:\n",
    "                texts = []\n",
    "                for ann in section_annotations:\n",
    "                    text = doc.text[ann.start:ann.end]\n",
    "                    if text.strip():\n",
    "                        texts.append(text.strip())\n",
    "                sections[key] = \" \".join(texts)\n",
    "            else:\n",
    "                logger.warning(f\"No {gate_type} annotations found\")\n",
    "        \n",
    "        return sections\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting sections: {e}\")\n",
    "        return sections\n",
    "\n",
    "def save_results_improved(doc_dict: Dict[str, Any], backup: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Improved save function with better error handling and backup\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Process events in annotations\n",
    "        for ann in doc_dict.get(\"annotations\", []):\n",
    "            # Handle structured_events (Pydantic objects)\n",
    "            if \"structured_events\" in ann and ann[\"structured_events\"] is not None:\n",
    "                try:\n",
    "                    # Convert Pydantic model to dict\n",
    "                    if hasattr(ann[\"structured_events\"], 'model_dump'):\n",
    "                        ann[\"structured_events\"] = ann[\"structured_events\"].model_dump()\n",
    "                    elif hasattr(ann[\"structured_events\"], 'dict'):\n",
    "                        ann[\"structured_events\"] = ann[\"structured_events\"].dict()\n",
    "                    else:\n",
    "                        # If it's already a dict or other serializable type, keep as is\n",
    "                        pass\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to serialize structured_events: {e}\")\n",
    "                    ann[\"structured_events\"] = None\n",
    "            \n",
    "            # Handle events string parsing\n",
    "            if \"events\" in ann and isinstance(ann[\"events\"], str):\n",
    "                try:\n",
    "                    parsed = json.loads(ann[\"events\"])\n",
    "                    if isinstance(parsed, dict) and \"events\" in parsed:\n",
    "                        ann[\"events\"] = parsed[\"events\"]\n",
    "                    else:\n",
    "                        ann[\"events\"] = parsed\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.warning(f\"Failed to parse events JSON: {e}\")\n",
    "                    ann[\"events\"] = []\n",
    "        \n",
    "        # Generate clean document name\n",
    "        doc_name = doc_dict.get(\"Document\", \"unknown\")\n",
    "        if isinstance(doc_name, str):\n",
    "            doc_name = doc_name.replace(\"file:/C:/Users/mnavas/CASE%20OF%20\", \"\")\n",
    "            doc_name = doc_name.replace(\".docx\", \"\").replace(\"%20\", \" \")\n",
    "            doc_name = doc_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "        \n",
    "        # Add metadata\n",
    "        doc_dict[\"metadata\"] = {\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"total_sections\": len([k for k in doc_dict.keys() if k in [\"procedure\", \"circumstances\", \"decision\"]]),\n",
    "            \"total_models\": len(doc_dict.get(\"annotations\", [])),\n",
    "            \"total_tokens\": doc_dict.get(\"total_tokens\", 0)\n",
    "        }\n",
    "        \n",
    "        # Save to main output\n",
    "        output_path = Path(config.output_dir) / f\"{doc_name}.json\"\n",
    "        if not safe_json_dump(doc_dict, output_path, indent=2):\n",
    "            return False\n",
    "        \n",
    "        # Create backup if requested\n",
    "        if backup:\n",
    "            backup_path = Path(config.backup_dir) / f\"{doc_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            safe_json_dump(doc_dict, backup_path, indent=2)\n",
    "        \n",
    "        logger.info(f\"Successfully saved results for {doc_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_document_with_models(doc, models: List[str], event_definitions: str, \n",
    "                                instruction: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single document with multiple models\n",
    "    \"\"\"\n",
    "    doc_name = doc.features.get(\"gate.SourceURL\", \"unknown\")\n",
    "    logger.info(f\"Processing document: {doc_name}\")\n",
    "    \n",
    "    # Extract sections\n",
    "    sections = extract_document_sections(doc)\n",
    "    combined_text = \" \".join([text for text in sections.values() if text])\n",
    "    \n",
    "    if not combined_text.strip():\n",
    "        logger.warning(f\"No text extracted from document: {doc_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Count tokens\n",
    "    total_tokens = token_counter.count_tokens(combined_text)\n",
    "    \n",
    "    doc_dict = {\n",
    "        \"Document\": doc_name,\n",
    "        \"sections\": sections,\n",
    "        \"combined_text_length\": len(combined_text),\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"annotations\": []\n",
    "    }\n",
    "    \n",
    "    # Process with each model\n",
    "    for model in models:\n",
    "        logger.info(f\"Processing with model: {model}\")\n",
    "        \n",
    "        # Choose appropriate function based on configuration\n",
    "        if config.via_web:\n",
    "            response = askChatbotImproved(model, event_definitions, instruction, combined_text)\n",
    "        else:\n",
    "            response = askChatbotLocalImproved(model, event_definitions, instruction, combined_text)\n",
    "        \n",
    "        if response:\n",
    "            annotation = {\n",
    "                \"model_name\": model,\n",
    "                \"events\": response[\"content\"],\n",
    "                \"structured_events\": response.get(\"structured\"),\n",
    "                \"processed_at\": datetime.now().isoformat(),\n",
    "                \"context_length\": model_manager.get_context_length(model),\n",
    "                \"input_tokens\": total_tokens\n",
    "            }\n",
    "            doc_dict[\"annotations\"].append(annotation)\n",
    "        else:\n",
    "            logger.warning(f\"Failed to get response from {model} for document {doc_name}\")\n",
    "    \n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0621e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated model configuration\n",
    "# models = [\n",
    "#     \"gemma3:12b\",\n",
    "#     \"mistral:latest\"\n",
    "# ]\n",
    "\n",
    "# You can add more models as needed\n",
    "models = [\n",
    "    \"gemma3:1b\",\n",
    "    \"gemma3:4b\",\n",
    "    \"gemma3:12b\",\n",
    "    \"llama3.3:latest\",\n",
    "    \"deepseek-r1:8b\",\n",
    "    \"mistral:latest\",\n",
    "    \"incept5/llama3.1-claude:latest\", \n",
    "    \"chevalblanc/claude-3-haiku:latest\",\n",
    "    \"llama4:16x17b\",\n",
    "    \"mixtral:8x7b\"\n",
    "]\n",
    "\n",
    "def run_improved_pipeline(max_documents: int = 10, models: List[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run the improved information extraction pipeline\n",
    "    \"\"\"\n",
    "    # if models is None:\n",
    "    #     models = models\n",
    "    \n",
    "    logger.info(f\"Starting IE pipeline with {len(models)} models and max {max_documents} documents\")\n",
    "    \n",
    "    # Load corpus\n",
    "    try:\n",
    "        corpus = loadCorpus()\n",
    "        logger.info(f\"Loaded corpus with {len(corpus)} documents\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load corpus: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    results = {\n",
    "        \"processed_documents\": 0,\n",
    "        \"failed_documents\": 0,\n",
    "        \"total_annotations\": 0,\n",
    "        \"start_time\": datetime.now().isoformat(),\n",
    "        \"models_used\": models,\n",
    "        \"documents\": []\n",
    "    }\n",
    "    \n",
    "    # Process documents\n",
    "    for doc_idx, doc in enumerate(tqdm(corpus, desc=\"Processing documents\")):\n",
    "        if doc_idx >= max_documents:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            doc_dict = process_document_with_models(doc, models, event_definitions, instruction)\n",
    "            \n",
    "            if doc_dict:\n",
    "                # Save results\n",
    "                if save_results_improved(doc_dict, backup=True):\n",
    "                    results[\"processed_documents\"] += 1\n",
    "                    results[\"total_annotations\"] += len(doc_dict.get(\"annotations\", []))\n",
    "                    results[\"documents\"].append(doc_dict[\"Document\"])\n",
    "                else:\n",
    "                    results[\"failed_documents\"] += 1\n",
    "            else:\n",
    "                results[\"failed_documents\"] += 1\n",
    "                logger.warning(f\"Failed to process document {doc_idx}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing document {doc_idx}: {e}\")\n",
    "            results[\"failed_documents\"] += 1\n",
    "    \n",
    "    # Finalize results\n",
    "    results[\"end_time\"] = datetime.now().isoformat()\n",
    "    results[\"total_processing_time\"] = str(datetime.fromisoformat(results[\"end_time\"]) - datetime.fromisoformat(results[\"start_time\"]))\n",
    "    \n",
    "    # Save pipeline results\n",
    "    pipeline_results_path = Path(config.output_dir) / f\"pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    safe_json_dump(results, pipeline_results_path, indent=2)\n",
    "    \n",
    "    logger.info(f\"Pipeline completed. Processed: {results['processed_documents']}, Failed: {results['failed_documents']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 18:15:36,147|INFO|__main__|Starting IE pipeline with 2 models and max 1 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running improved IE pipeline...\n",
      "==================================================\n",
      "Loaded input/updated/annotated\\dev\\CASE OF ALTAY v. TURKEY (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BELYAYEV AND OTHERS v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\dev\\CASE OF BIGUN v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CABUCAK v. GERMANY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CAN v. TURKEY.xml into corpus\n",
      "Loaded input/updated/annotated\\test\\CASE OF CRISTIAN CATALIN UNGUREANU v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF DOKTOROV v. BULGARIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF EGILL EINARSSON v. ICELAND (No. 2).xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF HOINESS v. NORWAY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF KOSAITE - CYPIENE AND OTHERS v. LITHUANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF LOZOVYYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF M.T. v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MOSKALEV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF MURUZHEVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF NODI v. HUNGARY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF O.C.I. AND OTHERS v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF OTGON v. THE REPUBLIC OF MOLDOVA.xml into corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 18:15:36,458|INFO|__main__|Loaded corpus with 30 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input/updated/annotated\\train\\CASE OF PAKHTUSOV v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF PANYUSHKINY v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF RESIN v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.N. v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF S.V. v. ITALY.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SHVIDKIYE v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SIDOROVA v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF SOLCAN v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF STANA v. ROMANIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF VISY v. SLOVAKIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YAKUSHEV v. UKRAINE.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YERMAKOVICH v. RUSSIA.xml into corpus\n",
      "Loaded input/updated/annotated\\train\\CASE OF YEVGENIY ZAKHAROV v. RUSSIA.xml into corpus\n",
      "All documents loaded into the corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/30 [00:00<?, ?it/s]2025-07-11 18:15:36,460|INFO|__main__|Processing document: file:/C:/Users/mnavas/CASE%20OF%20ALTAY%20v.%20TURKEY%20(No.%202).docx\n",
      "2025-07-11 18:15:36,461|INFO|__main__|Processing with model: gemma3:12b\n",
      "2025-07-11 18:15:36,460|INFO|__main__|Processing document: file:/C:/Users/mnavas/CASE%20OF%20ALTAY%20v.%20TURKEY%20(No.%202).docx\n",
      "2025-07-11 18:15:36,461|INFO|__main__|Processing with model: gemma3:12b\n",
      "2025-07-11 18:18:03,243|INFO|httpx|HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 18:18:03,244|INFO|__main__|Successfully processed with gemma3:12b on attempt 1\n",
      "2025-07-11 18:18:03,244|INFO|__main__|Processing with model: mistral:latest\n",
      "2025-07-11 18:18:03,243|INFO|httpx|HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 18:18:03,244|INFO|__main__|Successfully processed with gemma3:12b on attempt 1\n",
      "2025-07-11 18:18:03,244|INFO|__main__|Processing with model: mistral:latest\n",
      "2025-07-11 18:18:16,734|INFO|httpx|HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 18:18:16,735|INFO|__main__|Successfully processed with mistral:latest on attempt 1\n",
      "2025-07-11 18:18:16,739|INFO|__main__|Successfully saved results for ALTAY v. TURKEY (No. 2)\n",
      "Processing documents:   3%|▎         | 1/30 [02:40<1:17:28, 160.28s/it]\n",
      "2025-07-11 18:18:16,741|INFO|__main__|Pipeline completed. Processed: 1, Failed: 0\n",
      "2025-07-11 18:18:16,734|INFO|httpx|HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 18:18:16,735|INFO|__main__|Successfully processed with mistral:latest on attempt 1\n",
      "2025-07-11 18:18:16,739|INFO|__main__|Successfully saved results for ALTAY v. TURKEY (No. 2)\n",
      "Processing documents:   3%|▎         | 1/30 [02:40<1:17:28, 160.28s/it]\n",
      "2025-07-11 18:18:16,741|INFO|__main__|Pipeline completed. Processed: 1, Failed: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline Results:\n",
      "==================================================\n",
      "Documents processed: 1\n",
      "Documents failed: 0\n",
      "Total annotations: 2\n",
      "Models used: gemma3:12b, mistral:latest\n",
      "Processing time: 0:02:40.281864\n",
      "\n",
      "Processed documents:\n",
      "  - file:/C:/Users/mnavas/CASE%20OF%20ALTAY%20v.%20TURKEY%20(No.%202).docx\n"
     ]
    }
   ],
   "source": [
    "# Execute the improved pipeline\n",
    "print(\"Running improved IE pipeline...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configure processing\n",
    "config.max_documents = 1  # Start with a small number for testing\n",
    "config.via_web = False    # Use local models\n",
    "config.max_retries = 3\n",
    "config.retry_delay = 2.0\n",
    "\n",
    "# Run the pipeline\n",
    "results = run_improved_pipeline(\n",
    "    max_documents=config.max_documents,\n",
    "    models=models\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPipeline Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Documents processed: {results['processed_documents']}\")\n",
    "print(f\"Documents failed: {results['failed_documents']}\")\n",
    "print(f\"Total annotations: {results['total_annotations']}\")\n",
    "print(f\"Models used: {', '.join(results['models_used'])}\")\n",
    "print(f\"Processing time: {results['total_processing_time']}\")\n",
    "\n",
    "if results['documents']:\n",
    "    print(f\"\\nProcessed documents:\")\n",
    "    for doc in results['documents']:\n",
    "        print(f\"  - {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "948d05dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing results...\n",
      "ANALYSIS RESULTS\n",
      "============================================================\n",
      "Total files analyzed: 2\n",
      "\n",
      "Model Performance:\n",
      "----------------------------------------\n",
      "gemma3:12b:\n",
      "  Documents processed: 1\n",
      "  Success rate: 100.0%\n",
      "  Average tokens: 2210\n",
      "\n",
      "mistral:latest:\n",
      "  Documents processed: 1\n",
      "  Success rate: 100.0%\n",
      "  Average tokens: 2210\n",
      "\n",
      "Token Statistics:\n",
      "----------------------------------------\n",
      "  Mean tokens per document: 2210\n",
      "  Min tokens: 2210\n",
      "  Max tokens: 2210\n",
      "  Total tokens processed: 2,210\n"
     ]
    }
   ],
   "source": [
    "def analyze_results(output_dir: str = \"output\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze the results from the IE pipeline\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    json_files = list(output_path.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        logger.warning(\"No result files found\")\n",
    "        return {}\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_files\": len(json_files),\n",
    "        \"models_performance\": {},\n",
    "        \"token_statistics\": {},\n",
    "        \"event_statistics\": {},\n",
    "        \"error_analysis\": {}\n",
    "    }\n",
    "    \n",
    "    all_docs = []\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        if file_path.name.startswith(\"pipeline_results_\"):\n",
    "            continue  # Skip pipeline summary files\n",
    "            \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                doc_data = json.load(f)\n",
    "                all_docs.append(doc_data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    if not all_docs:\n",
    "        return analysis\n",
    "    \n",
    "    # Analyze model performance\n",
    "    for doc in all_docs:\n",
    "        for annotation in doc.get(\"annotations\", []):\n",
    "            model_name = annotation.get(\"model_name\", \"unknown\")\n",
    "            if model_name not in analysis[\"models_performance\"]:\n",
    "                analysis[\"models_performance\"][model_name] = {\n",
    "                    \"total_docs\": 0,\n",
    "                    \"successful_responses\": 0,\n",
    "                    \"failed_responses\": 0,\n",
    "                    \"avg_tokens\": 0,\n",
    "                    \"total_tokens\": 0\n",
    "                }\n",
    "            \n",
    "            analysis[\"models_performance\"][model_name][\"total_docs\"] += 1\n",
    "            \n",
    "            if annotation.get(\"events\"):\n",
    "                analysis[\"models_performance\"][model_name][\"successful_responses\"] += 1\n",
    "            else:\n",
    "                analysis[\"models_performance\"][model_name][\"failed_responses\"] += 1\n",
    "            \n",
    "            tokens = annotation.get(\"input_tokens\", 0)\n",
    "            analysis[\"models_performance\"][model_name][\"total_tokens\"] += tokens\n",
    "    \n",
    "    # Calculate averages\n",
    "    for model_stats in analysis[\"models_performance\"].values():\n",
    "        if model_stats[\"total_docs\"] > 0:\n",
    "            model_stats[\"avg_tokens\"] = model_stats[\"total_tokens\"] / model_stats[\"total_docs\"]\n",
    "            model_stats[\"success_rate\"] = (model_stats[\"successful_responses\"] / model_stats[\"total_docs\"]) * 100\n",
    "    \n",
    "    # Token statistics\n",
    "    token_counts = [doc.get(\"total_tokens\", 0) for doc in all_docs]\n",
    "    if token_counts:\n",
    "        analysis[\"token_statistics\"] = {\n",
    "            \"mean\": sum(token_counts) / len(token_counts),\n",
    "            \"min\": min(token_counts),\n",
    "            \"max\": max(token_counts),\n",
    "            \"total\": sum(token_counts)\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def display_analysis(analysis: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Display analysis results in a formatted way\n",
    "    \"\"\"\n",
    "    print(\"ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"Total files analyzed: {analysis.get('total_files', 0)}\")\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(\"-\" * 40)\n",
    "    for model, stats in analysis.get(\"models_performance\", {}).items():\n",
    "        print(f\"{model}:\")\n",
    "        print(f\"  Documents processed: {stats['total_docs']}\")\n",
    "        print(f\"  Success rate: {stats.get('success_rate', 0):.1f}%\")\n",
    "        print(f\"  Average tokens: {stats['avg_tokens']:.0f}\")\n",
    "        print()\n",
    "    \n",
    "    token_stats = analysis.get(\"token_statistics\", {})\n",
    "    if token_stats:\n",
    "        print(\"Token Statistics:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Mean tokens per document: {token_stats['mean']:.0f}\")\n",
    "        print(f\"  Min tokens: {token_stats['min']}\")\n",
    "        print(f\"  Max tokens: {token_stats['max']}\")\n",
    "        print(f\"  Total tokens processed: {token_stats['total']:,}\")\n",
    "\n",
    "# Run analysis\n",
    "print(\"\\nAnalyzing results...\")\n",
    "analysis = analyze_results()\n",
    "display_analysis(analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
