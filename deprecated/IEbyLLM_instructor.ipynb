{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gatenlp import Document\n",
    "from gatenlp.corpora import ListCorpus\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "from gatenlp.lib_spacy import AnnSpacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import pandas as pd\n",
    "import instructor\n",
    "\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class LegalEvent(BaseModel):\n",
    "    event_type: str  # \"event_circumstance\" or \"event_procedure\"\n",
    "    event_who: Optional[str]\n",
    "    event_what: Optional[str]\n",
    "    event_when: Optional[str]\n",
    "\n",
    "class LegalEventsExtraction(BaseModel):\n",
    "    events: List[LegalEvent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "user_email = os.getenv(\"USEREMAIL\")  # Enter your email here\n",
    "password = os.getenv(\"PASSWORD\")  # Enter your password here\n",
    "\n",
    "# Fetch Access Token\n",
    "\n",
    "# Define the URL for the authentication endpoint\n",
    "auth_url = \"http://localhost:8080/api/v1/auths/signin\"\n",
    "\n",
    "# Define the payload with user credentials\n",
    "auth_payload = json.dumps({\"email\": user_email, \"password\": \"admin\"})\n",
    "\n",
    "# Define the headers for the authentication request\n",
    "auth_headers = {\"accept\": \"application/json\", \"content-type\": \"application/json\"}\n",
    "\n",
    "# Make the POST request to fetch the access token\n",
    "auth_response = requests.post(auth_url, data=auth_payload, headers=auth_headers)\n",
    "\n",
    "# Extract the access token from the response\n",
    "access_token = auth_response.json().get(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "\n",
    "def askChatbot(model, role, instruction, content):\n",
    "    chat_url = \"http://localhost:11434/api/chat\"\n",
    "    chat_headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "    }\n",
    "    chat_payload = json.dumps(\n",
    "        {\n",
    "            \"stream\": False,\n",
    "            \"model\": model,\n",
    "            \"temperature\": 0.0,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": role},\n",
    "                {\"role\": \"user\", \"content\": f\"{instruction}\\n\\n{content}\"},\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    chat_response = requests.post(chat_url, data=chat_payload, headers=chat_headers)\n",
    "    print(chat_response)\n",
    "    response_content = chat_response.json().get(\"message\", {}).get(\"content\", \"\")\n",
    "    # Use instructor to parse the response into the structured model\n",
    "    try:\n",
    "        structured = LegalEventsExtraction.model_validate_json(response_content)\n",
    "        return structured\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new corpus with an empty list\n",
    "corpus = ListCorpus([])\n",
    "\n",
    "# Define the base directory\n",
    "base_dir = \"input/annotated\"\n",
    "\n",
    "# Walk through the directory and load each XML file\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".xml\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            doc = Document.load(file_path, fmt=\"gatexml\")\n",
    "            # Add the document to the corpus\n",
    "            corpus.append(doc)\n",
    "            print(f\"Loaded {file_path} into corpus\")            \n",
    "                \n",
    "print(\"All documents loaded into the corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.features.get(\"gate.SourceURL\").replace(\"file:/C:/Users/mnavas/\", \"\").replace(\"%20\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_spacy = English()\n",
    "#nlp_spacy.add_pipe('sentencizer')\n",
    "#tokenize = AnnSpacy(nlp_spacy, add_nounchunks=False, add_deps=False, add_entities=False)\n",
    "\n",
    "#for doc in corpus:\n",
    "#    doc = tokenize(doc)\n",
    "#    doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gatenlp.visualization import CorpusViewer\n",
    "\n",
    "viewer = CorpusViewer(corpus)\n",
    "viewer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [#\"gemma3:12b\",\n",
    "          \"GandalfBaum/llama3.1/claude3.7:latest\",\n",
    "          \"chevalblanc/claude-3-haiku:latest\",\n",
    "          \"incept5/llama3.1-claude:latest\",\n",
    "          \"llama3.3:latest\",\n",
    "          \"deepseek-r1:8b\",\n",
    "          \"mistral:latest\",\n",
    "          \"llama3-gradient:latest\"\n",
    "]\n",
    "\n",
    "event_definitions = \"\"\"\n",
    "You are an expert in legal text analysis. Here are the definitions of legal events:\n",
    "- Event: Relates to the extent of text containing contextual event-related information. \n",
    "- Event_who: Corresponds to the subject of the event, which can either be a subject, but also an object (i.e., an application). \n",
    "    Examples: applicant, respondent, judge, witness\n",
    "- Event_what: Corresponds to the main verb reflecting the baseline of all the paragraph. Additionally, we include thereto a complementing verb or object whenever the core verb is not self-explicit or requires an extension to attain a sufficient meaning.\n",
    "    Examples: lodged an application, decided, ordered, dismissed\n",
    "- Event_when: Refers to the date of the event, or to any temporal reference thereto.\n",
    "- Event_circumstance: Meaning that the event correspond to the facts under judgment.\n",
    "- Event_procedure: The events belongs to the procedural dimension of the case.\n",
    "\n",
    "Events contain the annotations event_who, event_what and event_when. Events can be of type event_circumstance and event_procedure.\n",
    "\"\"\"\n",
    "\n",
    "#instruction = \"Analyze the provided text and extract the legal events. Provide the results in a structured format. Obviously, Event_who, Event_what and Event_when can only appear within an Event. If you find an event, also classify it into an event_circumstance or event_procedure. Do not invent additional information.\"\n",
    "\n",
    "instruction = \"\"\"\n",
    "Analyze the provided text and extract the legal events. Provide the results in a structured format.\n",
    "Your output MUST be valid JSON matching this schema:\n",
    "{\n",
    "  \"events\": [\n",
    "    {\n",
    "      \"event_type\": \"event_circumstance or event_procedure\",\n",
    "      \"event_who\": \"string or null\",\n",
    "      \"event_what\": \"string or null\",\n",
    "      \"event_when\": \"string or null\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "Do not include any explanation or text outside the JSON.\n",
    "Obviously, Event_who, Event_what and Event_when can only appear within an Event.\n",
    "If you find an event, also classify it into an event_circumstance or event_procedure.\n",
    "Do not invent additional information. Only return the JSON, no additional text or something else\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [400]>\n",
      "Parsing error: 1 validation error for LegalEventsExtraction\n",
      "  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n"
     ]
    }
   ],
   "source": [
    "combined_procedure_text = \"\"\"1.  The case originated in an application (no. 11236/09) against the Republic of Turkey lodged with the Court under Article 34 of the Convention for the Protection of Human Rights and Fundamental Freedoms (“the Convention”) by a Turkish national, Mr Mehmet Aytunç Altay (“the applicant”), on 17 February 2006. \n",
    "2.  The applicant was represented by Ms G. Tuncer, a lawyer practising in Istanbul. The Turkish Government (“the Government”) were represented by their Agent. \n",
    "3.  The applicant alleged, in particular, that the restriction of the privacy of his consultations with his lawyer was incompatible with his rights under Article 8 of the Convention and that the domestic proceedings with respect to this measure had not complied with the requirements of Article 6 § 1 of the Convention. \n",
    "4.  On 17 October 2017 notice of the above complaints was given to the Government and the remainder of the application was declared inadmissible pursuant to Rule 54 § 3 of the Rules of Court. \"\"\"\n",
    "structured_response = askChatbot(models[0], event_definitions, instruction, combined_procedure_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "results = []\n",
    "# Iterate over documents and models\n",
    "for doc in tqdm(corpus, desc=\"Processing documents\"):\n",
    "    doc_dict = {\"Document\": doc.features.get(\"gate.SourceURL\")}\n",
    "    print(f\"Processing document: {doc.features.get(\"gate.SourceURL\")}\")\n",
    "    \n",
    "    # Combine all procedure texts for the document\n",
    "    procedure_texts = []\n",
    "    annotations = doc.annset(\"Section\")\n",
    "    procedure_annotations = annotations.with_type(\"Procedure\")\n",
    "    for ann in procedure_annotations:\n",
    "        procedure_text = doc.text[ann.start:ann.end]\n",
    "        procedure_texts.append(procedure_text)\n",
    "    combined_procedure_text = \" \".join(procedure_texts)\n",
    "    #print(f\"Combined procedure text: {combined_procedure_text}\")\n",
    "    \n",
    "    # Iterate over models\n",
    "    for model in models:\n",
    "        print(f\"Using model: {model}\")\n",
    "\n",
    "        # Call the chatbot with role, instruction, and content\n",
    "        structured_response = askChatbot(model, event_definitions, instruction, combined_procedure_text)\n",
    "        # Extract and store the response\n",
    "        doc_dict[model] = structured_response.model_dump() if structured_response else \"Parsing failed\"\n",
    "    \n",
    "    # Append the document dictionary to the results list\n",
    "    results.append(doc_dict)\n",
    "\n",
    "# Convert results to a DataFrame and save as CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"chat_responses_with_instructions.csv\", index=False)\n",
    "df.to_excel(\"chat_responses_with_instructions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results list to a pandas DataFrame\n",
    "df = pd.DataFrame(doc_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file for later analysis\n",
    "df.to_csv(\"chat_responses.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
