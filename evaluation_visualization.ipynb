{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13398642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "legendgroup": "models",
         "marker": {
          "color": "#E74C3C"
         },
         "name": "gemma3:1b",
         "showlegend": true,
         "type": "bar",
         "x": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event",
          "Event_who",
          "Event_when",
          "Event_what"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "Hh4eHh4ezj8AAAAAAAAAAAAAAAAAAMA/AAAAAAAAAAATO7ETO7HDPwAAAAAAAAAAVVVVVVVV1T8AAAAAAAAAAA==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "legendgroup": "models",
         "marker": {
          "color": "#3498DB"
         },
         "name": "gemma3:4b",
         "showlegend": false,
         "type": "bar",
         "x": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event",
          "Event_who",
          "Event_when",
          "Event_what"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "ERERERER0T9kIQtZyEK2P5zYiZ3Yic0/HtRBHdRBrT8AAAAAAAAAAEYXXXTRRcc/AAAAAAAAAAAAAAAAAAAAAA==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "legendgroup": "models",
         "marker": {
          "color": "#2ECC71"
         },
         "name": "gemma3:12b",
         "showlegend": false,
         "type": "bar",
         "x": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event",
          "Event_who",
          "Event_when",
          "Event_what"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "////////4z8RERERERHRPwAAAAAAAOI/25WoXYna1T8pr6G8hvLaPx4eHh4eHr4/AAAAAAAAwD8YhmEYhmG4Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "legendgroup": "models",
         "marker": {
          "color": "#F39C12"
         },
         "name": "mistral:latest",
         "showlegend": false,
         "type": "bar",
         "x": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event",
          "Event_who",
          "Event_when",
          "Event_what"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "27Zt27Ztyz8ZhmEYhmG4P7gehetRuM4/Hh4eHh4erj+ZmZmZmZnJPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "<b>%{text}</b><br>Recall: %{x:.3f}<br>Precision: %{y:.3f}<extra></extra>",
         "marker": {
          "color": "#E74C3C",
          "size": 10,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "gemma3:1b",
         "showlegend": false,
         "text": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event",
          "Event_who",
          "Event_when",
          "Event_what"
         ],
         "type": "scatter",
         "x": {
          "bdata": "GIZhGIZhyD8AAAAAAAAAABiGYRiGYbg/AAAAAAAAAAAcx3Ecx3G8PwAAAAAAAAAAAAAAAAAA0D8AAAAAAAAAAA==",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "FDuxEzux0z8AAAAAAAAAAEYXXXTRRcc/AAAAAAAAAAAAAAAAAADQPwAAAAAAAAAAAAAAAAAA4D8AAAAAAAAAAA==",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "<b>%{text}</b><br>Recall: %{x:.3f}<br>Precision: %{y:.3f}<extra></extra>",
         "marker": {
          "color": "#3498DB",
          "size": 10,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "gemma3:4b",
         "showlegend": false,
         "text": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event",
          "Event_who",
          "Event_when",
          "Event_what"
         ],
         "type": "scatter",
         "x": {
          "bdata": "GIZhGIZhyD8or6G8hvKqP5IkSZIkScI/ERERERERoT8AAAAAAAAAAAAAAAAAAMA/AAAAAAAAAAAAAAAAAAAAAA==",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "HMdxHMdx3D8AAAAAAADQPzMzMzMzM+M/mpmZmZmZyT8AAAAAAAAAAFVVVVVVVdU/AAAAAAAAAAAAAAAAAAAAAA==",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "<b>%{text}</b><br>Recall: %{x:.3f}<br>Precision: %{y:.3f}<extra></extra>",
         "marker": {
          "color": "#2ECC71",
          "size": 10,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "gemma3:12b",
         "showlegend": false,
         "text": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event",
          "Event_who",
          "Event_when",
          "Event_what"
         ],
         "type": "scatter",
         "x": {
          "bdata": "nud5nud53j8or6G8hvLKP9u2bdu2bds/3t3d3d3dzT8cx3Ecx3HcPwAAAAAAAMA/AAAAAAAAwD8AAAAAAACwPw==",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "F1100UUX7T9GF1100UXXPy+66KKLLuo/XXTRRRdd5D+amZmZmZnZPxzHcRzHcbw/AAAAAAAAwD+amZmZmZnJPw==",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "<b>%{text}</b><br>Recall: %{x:.3f}<br>Precision: %{y:.3f}<extra></extra>",
         "marker": {
          "color": "#F39C12",
          "size": 10,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "mistral:latest",
         "showlegend": false,
         "text": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event",
          "Event_who",
          "Event_when",
          "Event_what"
         ],
         "type": "scatter",
         "x": {
          "bdata": "kiRJkiRJwj8or6G8hvKqP5IkSZIkScI/ERERERERoT8cx3Ecx3HMPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "27Zt27Zt2z8AAAAAAADgPwAAAAAAAOg/AAAAAAAA0D9GF1100UXHPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "legendgroup": "evaluation_modes",
         "marker": {
          "color": "lightblue"
         },
         "name": "Lenient Evaluation",
         "showlegend": true,
         "type": "bar",
         "x": [
          "gemma3:12b",
          "gemma3:1b",
          "gemma3:4b",
          "mistral:latest"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "5GcfR8Fv1D/3gB73gB67P/DF91npWLo/cC4RPfvduT8=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "legendgroup": "evaluation_modes",
         "marker": {
          "color": "darkblue"
         },
         "name": "Strict Evaluation",
         "showlegend": true,
         "type": "bar",
         "x": [
          "gemma3:12b",
          "gemma3:1b",
          "gemma3:4b",
          "mistral:latest"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "gan7zU63yz/V6CPV6COtP4lfkfOC8rM/xGvvKdJVoD8=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "fill": "toself",
         "line": {
          "color": "#E74C3C"
         },
         "name": "gemma3:1b",
         "r": [
          0.23529411764705882,
          0,
          0.125,
          0,
          0.23529411764705882
         ],
         "showlegend": false,
         "subplot": "polar",
         "theta": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event"
         ],
         "type": "scatterpolar"
        },
        {
         "fill": "toself",
         "line": {
          "color": "#3498DB"
         },
         "name": "gemma3:4b",
         "r": [
          0.26666666666666666,
          0.08695652173913043,
          0.23076923076923073,
          0.05714285714285715,
          0.26666666666666666
         ],
         "showlegend": false,
         "subplot": "polar",
         "theta": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event"
         ],
         "type": "scatterpolar"
        },
        {
         "fill": "toself",
         "line": {
          "color": "#2ECC71"
         },
         "name": "gemma3:12b",
         "r": [
          0.6249999999999999,
          0.26666666666666666,
          0.5625,
          0.34146341463414637,
          0.6249999999999999
         ],
         "showlegend": false,
         "subplot": "polar",
         "theta": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event"
         ],
         "type": "scatterpolar"
        },
        {
         "fill": "toself",
         "line": {
          "color": "#F39C12"
         },
         "name": "mistral:latest",
         "r": [
          0.21428571428571427,
          0.09523809523809525,
          0.24,
          0.058823529411764705,
          0.21428571428571427
         ],
         "showlegend": false,
         "subplot": "polar",
         "theta": [
          "Event",
          "Event_who",
          "Event_when",
          "Event_what",
          "Event"
         ],
         "type": "scatterpolar"
        },
        {
         "legendgroup": "counts",
         "marker": {
          "color": "gold"
         },
         "name": "Gold Standard",
         "showlegend": true,
         "type": "bar",
         "x": [
          "Event",
          "Event_what",
          "Event_when",
          "Event_who"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "FR4VEw==",
          "dtype": "i1"
         },
         "yaxis": "y4"
        },
        {
         "legendgroup": "counts",
         "marker": {
          "color": "silver"
         },
         "name": "Predicted",
         "showlegend": true,
         "type": "bar",
         "x": [
          "Event",
          "Event_what",
          "Event_when",
          "Event_who"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "RB0sMg==",
          "dtype": "i1"
         },
         "yaxis": "y4"
        },
        {
         "colorscale": [
          [
           0,
           "rgb(247,251,255)"
          ],
          [
           0.125,
           "rgb(222,235,247)"
          ],
          [
           0.25,
           "rgb(198,219,239)"
          ],
          [
           0.375,
           "rgb(158,202,225)"
          ],
          [
           0.5,
           "rgb(107,174,214)"
          ],
          [
           0.625,
           "rgb(66,146,198)"
          ],
          [
           0.75,
           "rgb(33,113,181)"
          ],
          [
           0.875,
           "rgb(8,81,156)"
          ],
          [
           1,
           "rgb(8,48,107)"
          ]
         ],
         "hovertemplate": "%{y}<br>%{x}: %{z}<extra></extra>",
         "showlegend": false,
         "showscale": true,
         "type": "heatmap",
         "x": [
          "TP",
          "FP",
          "FN"
         ],
         "xaxis": "x5",
         "y": [
          "Event",
          "Event_what",
          "Event_when",
          "Event_who"
         ],
         "yaxis": "y5",
         "z": {
          "bdata": "EAcOJggIEwkKFg8F",
          "dtype": "i1",
          "shape": "4, 3"
         }
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "F1-Scores by Model and Annotation Type",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Precision vs Recall by Model",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Lenient vs Strict Evaluation Comparison",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.6333333333333333,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Model Performance Radar Chart",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.6333333333333333,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Annotation Type Distribution",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.26666666666666666,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Confusion Matrix Heatmap",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.26666666666666666,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "legend": {
         "orientation": "v",
         "x": 1.02,
         "xanchor": "left",
         "y": 1,
         "yanchor": "top"
        },
        "polar": {
         "domain": {
          "x": [
           0.55,
           1
          ],
          "y": [
           0.3666666666666667,
           0.6333333333333333
          ]
         }
        },
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "LLM Evaluation Results Dashboard<br><sub>Pipeline Results: pipeline_results_20250804_170535</sub>",
         "x": 0.5
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Annotation Type"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Recall"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Model"
         }
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Annotation Type"
         }
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.7333333333333334,
          1
         ],
         "title": {
          "text": "F1 Score"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.7333333333333334,
          1
         ],
         "title": {
          "text": "Precision"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.3666666666666667,
          0.6333333333333333
         ],
         "title": {
          "text": "Average F1 Score"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.26666666666666666
         ],
         "title": {
          "text": "Count"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.26666666666666666
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Single run visualization created successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def load_evaluation_results(results_path):\n",
    "    \"\"\"Load evaluation results from JSON file.\"\"\"\n",
    "    with open(results_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_dataframe_from_results(results):\n",
    "    \"\"\"Convert nested results to a flat DataFrame for analysis.\"\"\"\n",
    "    rows = []\n",
    "    for doc_name, doc_results in results.items():\n",
    "        for model_name, model_results in doc_results.items():\n",
    "            for ann_type, metrics in model_results.items():\n",
    "                # Lenient evaluation\n",
    "                rows.append({\n",
    "                    'Document': doc_name,\n",
    "                    'Model': model_name,\n",
    "                    'Annotation_Type': ann_type,\n",
    "                    'Evaluation_Mode': 'Lenient',\n",
    "                    'Precision': metrics['lenient']['precision'],\n",
    "                    'Recall': metrics['lenient']['recall'],\n",
    "                    'F1_Score': metrics['lenient']['f1_score'],\n",
    "                    'True_Positives': metrics['lenient']['true_positives'],\n",
    "                    'False_Positives': metrics['lenient']['false_positives'],\n",
    "                    'False_Negatives': metrics['lenient']['false_negatives'],\n",
    "                    'Gold_Count': metrics['gold_count'],\n",
    "                    'Predicted_Count': metrics['predicted_count']\n",
    "                })\n",
    "                \n",
    "                # Strict evaluation\n",
    "                rows.append({\n",
    "                    'Document': doc_name,\n",
    "                    'Model': model_name,\n",
    "                    'Annotation_Type': ann_type,\n",
    "                    'Evaluation_Mode': 'Strict',\n",
    "                    'Precision': metrics['strict']['precision'],\n",
    "                    'Recall': metrics['strict']['recall'],\n",
    "                    'F1_Score': metrics['strict']['f1_score'],\n",
    "                    'True_Positives': metrics['strict']['true_positives'],\n",
    "                    'False_Positives': metrics['strict']['false_positives'],\n",
    "                    'False_Negatives': metrics['strict']['false_negatives'],\n",
    "                    'Gold_Count': metrics['gold_count'],\n",
    "                    'Predicted_Count': metrics['predicted_count']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def visualize_single_run(results_path):\n",
    "    \"\"\"Create comprehensive visualizations for a single evaluation run.\"\"\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    results = load_evaluation_results(results_path)\n",
    "    df = create_dataframe_from_results(results)\n",
    "    \n",
    "    # Create an enhanced figure with more subplots for better analysis\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=(\n",
    "            'F1-Scores by Model and Annotation Type (Lenient)', \n",
    "            'Document-Level Performance Heatmap',\n",
    "            'Precision vs Recall by Model (with Document Points)',\n",
    "            'Model Performance Across Documents',\n",
    "            'Lenient vs Strict Evaluation Comparison',\n",
    "            'Individual Document Analysis',\n",
    "            'Annotation Type Performance Distribution',\n",
    "            'Model Consistency Across Documents'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"heatmap\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]],\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.12\n",
    "    )\n",
    "    \n",
    "    # Color palette for models with better contrast\n",
    "    model_colors = {\n",
    "        'gemma3:1b': '#E74C3C',    # Red\n",
    "        'gemma3:4b': '#3498DB',    # Blue  \n",
    "        'gemma3:12b': '#2ECC71',   # Green\n",
    "        'mistral:latest': '#F39C12' # Orange\n",
    "    }\n",
    "    \n",
    "    # Document colors for variety\n",
    "    doc_colors = px.colors.qualitative.Set3\n",
    "    \n",
    "    # Get lenient and strict data\n",
    "    df_lenient = df[df['Evaluation_Mode'] == 'Lenient']\n",
    "    df_strict = df[df['Evaluation_Mode'] == 'Strict']\n",
    "    \n",
    "    # 1. F1-Scores by Model and Annotation Type (Lenient) - Enhanced with proper legend\n",
    "    for i, model in enumerate(df_lenient['Model'].unique()):\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        avg_f1_by_type = model_data.groupby('Annotation_Type')['F1_Score'].mean().reset_index()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=avg_f1_by_type['Annotation_Type'],\n",
    "                y=avg_f1_by_type['F1_Score'],\n",
    "                name=f\"{model}\",\n",
    "                marker_color=model_colors.get(model, '#95A5A6'),\n",
    "                showlegend=True,\n",
    "                legendgroup='models',\n",
    "                hovertemplate=f'<b>{model}</b><br>Type: %{{x}}<br>Avg F1: %{{y:.3f}}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Document-Level Performance Heatmap\n",
    "    doc_model_performance = df_lenient.groupby(['Document', 'Model'])['F1_Score'].mean().reset_index()\n",
    "    heatmap_pivot = doc_model_performance.pivot(index='Document', columns='Model', values='F1_Score')\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=heatmap_pivot.values,\n",
    "            x=heatmap_pivot.columns,\n",
    "            y=heatmap_pivot.index,\n",
    "            colorscale='RdYlBu_r',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"F1 Score\", x=0.48),\n",
    "            hovertemplate='Document: %{y}<br>Model: %{x}<br>F1 Score: %{z:.3f}<extra></extra>',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Precision vs Recall scatter plot with document points\n",
    "    for i, model in enumerate(df_lenient['Model'].unique()):\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_data['Recall'],\n",
    "                y=model_data['Precision'],\n",
    "                mode='markers',\n",
    "                name=f\"{model} (docs)\",\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=model_colors.get(model, '#95A5A6'),\n",
    "                    symbol='circle',\n",
    "                    opacity=0.7\n",
    "                ),\n",
    "                text=model_data['Document'] + '<br>' + model_data['Annotation_Type'],\n",
    "                hovertemplate='<b>%{text}</b><br>Recall: %{x:.3f}<br>Precision: %{y:.3f}<extra></extra>',\n",
    "                showlegend=True,\n",
    "                legendgroup='scatter'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Model Performance Across Documents (Box plot style)\n",
    "    for i, model in enumerate(df_lenient['Model'].unique()):\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        doc_performance = model_data.groupby('Document')['F1_Score'].mean().reset_index()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=doc_performance['Document'],\n",
    "                y=doc_performance['F1_Score'],\n",
    "                name=f\"{model}\",\n",
    "                marker_color=model_colors.get(model, '#95A5A6'),\n",
    "                showlegend=False,\n",
    "                opacity=0.8,\n",
    "                hovertemplate=f'<b>{model}</b><br>Document: %{{x}}<br>Avg F1: %{{y:.3f}}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 5. Lenient vs Strict comparison - Enhanced\n",
    "    df_comparison = df.groupby(['Model', 'Evaluation_Mode'])['F1_Score'].mean().reset_index()\n",
    "    lenient_data = df_comparison[df_comparison['Evaluation_Mode'] == 'Lenient']\n",
    "    strict_data = df_comparison[df_comparison['Evaluation_Mode'] == 'Strict']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=lenient_data['Model'],\n",
    "            y=lenient_data['F1_Score'],\n",
    "            name='Lenient Evaluation',\n",
    "            marker_color='lightblue',\n",
    "            showlegend=True,\n",
    "            legendgroup='evaluation_modes',\n",
    "            hovertemplate='<b>Lenient</b><br>Model: %{x}<br>F1: %{y:.3f}<extra></extra>'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=strict_data['Model'],\n",
    "            y=strict_data['F1_Score'],\n",
    "            name='Strict Evaluation',\n",
    "            marker_color='darkblue',\n",
    "            showlegend=True,\n",
    "            legendgroup='evaluation_modes',\n",
    "            hovertemplate='<b>Strict</b><br>Model: %{x}<br>F1: %{y:.3f}<extra></extra>'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Individual Document Analysis (Box plots showing variance)\n",
    "    for i, annotation_type in enumerate(df_lenient['Annotation_Type'].unique()):\n",
    "        ann_data = df_lenient[df_lenient['Annotation_Type'] == annotation_type]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=ann_data['F1_Score'],\n",
    "                name=annotation_type,\n",
    "                boxpoints='all',\n",
    "                jitter=0.3,\n",
    "                pointpos=-1.8,\n",
    "                marker_color=px.colors.qualitative.Set1[i % len(px.colors.qualitative.Set1)],\n",
    "                showlegend=True,\n",
    "                legendgroup='annotation_types',\n",
    "                hovertemplate=f'<b>{annotation_type}</b><br>F1: %{{y:.3f}}<extra></extra>'\n",
    "            ),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    \n",
    "    # 7. Annotation Type Performance Distribution\n",
    "    df_counts = df_lenient.groupby('Annotation_Type').agg({\n",
    "        'Gold_Count': 'first',\n",
    "        'Predicted_Count': 'mean',\n",
    "        'F1_Score': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_counts['Annotation_Type'],\n",
    "            y=df_counts['Gold_Count'],\n",
    "            name='Gold Standard Count',\n",
    "            marker_color='gold',\n",
    "            showlegend=True,\n",
    "            legendgroup='counts',\n",
    "            hovertemplate='<b>Gold Standard</b><br>Type: %{x}<br>Count: %{y}<extra></extra>'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_counts['Annotation_Type'],\n",
    "            y=df_counts['Predicted_Count'],\n",
    "            name='Avg Predicted Count',\n",
    "            marker_color='silver',\n",
    "            showlegend=True,\n",
    "            legendgroup='counts',\n",
    "            yaxis='y2',\n",
    "            hovertemplate='<b>Predicted</b><br>Type: %{x}<br>Avg Count: %{y:.1f}<extra></extra>'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # 8. Model Consistency Across Documents (Coefficient of Variation)\n",
    "    model_consistency = []\n",
    "    for model in df_lenient['Model'].unique():\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        doc_scores = model_data.groupby('Document')['F1_Score'].mean()\n",
    "        cv = doc_scores.std() / doc_scores.mean() if doc_scores.mean() > 0 else 0\n",
    "        model_consistency.append({'Model': model, 'Consistency': 1 - cv, 'CV': cv})\n",
    "    \n",
    "    consistency_df = pd.DataFrame(model_consistency)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=consistency_df['Model'],\n",
    "            y=consistency_df['Consistency'],\n",
    "            mode='markers+lines',\n",
    "            name='Model Consistency',\n",
    "            marker=dict(\n",
    "                size=12,\n",
    "                color=[model_colors.get(model, '#95A5A6') for model in consistency_df['Model']],\n",
    "                symbol='diamond'\n",
    "            ),\n",
    "            line=dict(color='gray', dash='dash'),\n",
    "            showlegend=True,\n",
    "            legendgroup='consistency',\n",
    "            hovertemplate='<b>%{x}</b><br>Consistency: %{y:.3f}<br>CV: %{customdata:.3f}<extra></extra>',\n",
    "            customdata=consistency_df['CV']\n",
    "        ),\n",
    "        row=4, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout with improved legend and styling\n",
    "    fig.update_layout(\n",
    "        height=1600,  # Increased height for 4 rows\n",
    "        title=f'Enhanced LLM Evaluation Dashboard<br><sub>Pipeline Results: {Path(results_path).parent.name} | Document-Level Analysis</sub>',\n",
    "        title_x=0.5,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\", \n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "            bordercolor=\"rgba(0,0,0,0.2)\",\n",
    "            borderwidth=1,\n",
    "            font=dict(size=10)\n",
    "        ),\n",
    "        font=dict(size=11)\n",
    "    )\n",
    "    \n",
    "    # Update subplot titles and axes with better formatting\n",
    "    fig.update_xaxes(title_text=\"Annotation Type\", row=1, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Average F1 Score\", row=1, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=1, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Document\", row=1, col=2, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Recall\", row=2, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Precision\", row=2, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Document\", row=2, col=2, title_font_size=10, tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=2, col=2, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=3, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=3, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Annotation Type\", row=3, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"F1 Score Distribution\", row=3, col=2, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Annotation Type\", row=4, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=4, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=4, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Consistency Score\", row=4, col=2, title_font_size=10)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_document_comparison_plot(df_lenient):\n",
    "    \"\"\"Create a detailed document comparison visualization.\"\"\"\n",
    "    # Document performance comparison\n",
    "    doc_fig = go.Figure()\n",
    "    \n",
    "    documents = df_lenient['Document'].unique()\n",
    "    models = df_lenient['Model'].unique()\n",
    "    \n",
    "    model_colors = {\n",
    "        'gemma3:1b': '#E74C3C',\n",
    "        'gemma3:4b': '#3498DB', \n",
    "        'gemma3:12b': '#2ECC71',\n",
    "        'mistral:latest': '#F39C12'\n",
    "    }\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        doc_scores = model_data.groupby('Document')['F1_Score'].mean()\n",
    "        \n",
    "        doc_fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=documents,\n",
    "                y=[doc_scores.get(doc, 0) for doc in documents],\n",
    "                mode='lines+markers',\n",
    "                name=model,\n",
    "                line=dict(color=model_colors.get(model, '#95A5A6'), width=3),\n",
    "                marker=dict(size=8),\n",
    "                hovertemplate=f'<b>{model}</b><br>Document: %{{x}}<br>Avg F1: %{{y:.3f}}<extra></extra>'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    doc_fig.update_layout(\n",
    "        title='Model Performance Across Individual Documents',\n",
    "        xaxis_title='Document',\n",
    "        yaxis_title='Average F1 Score',\n",
    "        height=400,\n",
    "        hovermode='x unified',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\", \n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    doc_fig.update_xaxes(tickangle=45)\n",
    "    return doc_fig\n",
    "\n",
    "# Enhanced example usage with document-level analysis\n",
    "results_path = \"output/pipeline_results_20250804_170535/llm_evaluation_results.json\"\n",
    "\n",
    "# Check if file exists\n",
    "if Path(results_path).exists():\n",
    "    # Create main dashboard\n",
    "    fig = visualize_single_run(results_path)\n",
    "    fig.show()\n",
    "    \n",
    "    # Create additional document comparison plot\n",
    "    results = load_evaluation_results(results_path)\n",
    "    df = create_dataframe_from_results(results)\n",
    "    df_lenient = df[df['Evaluation_Mode'] == 'Lenient']\n",
    "    \n",
    "    doc_fig = create_document_comparison_plot(df_lenient)\n",
    "    doc_fig.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"ðŸ“Š EVALUATION SUMMARY:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Model rankings\n",
    "    model_rankings = df_lenient.groupby('Model')['F1_Score'].agg(['mean', 'std']).round(3)\n",
    "    model_rankings = model_rankings.sort_values('mean', ascending=False)\n",
    "    print(\"\\nðŸ† MODEL RANKINGS (by average F1):\")\n",
    "    for i, (model, stats) in enumerate(model_rankings.iterrows(), 1):\n",
    "        print(f\"{i}. {model}: {stats['mean']:.3f} (Â±{stats['std']:.3f})\")\n",
    "    \n",
    "    # Best performing annotation types\n",
    "    ann_performance = df_lenient.groupby('Annotation_Type')['F1_Score'].agg(['mean', 'std']).round(3)\n",
    "    ann_performance = ann_performance.sort_values('mean', ascending=False)\n",
    "    print(\"\\nðŸ“‹ ANNOTATION TYPE PERFORMANCE:\")\n",
    "    for ann_type, stats in ann_performance.iterrows():\n",
    "        print(f\"â€¢ {ann_type}: {stats['mean']:.3f} (Â±{stats['std']:.3f})\")\n",
    "    \n",
    "    # Document difficulty analysis\n",
    "    doc_difficulty = df_lenient.groupby('Document')['F1_Score'].agg(['mean', 'std']).round(3)\n",
    "    doc_difficulty = doc_difficulty.sort_values('mean')\n",
    "    print(f\"\\nðŸ“„ DOCUMENT ANALYSIS:\")\n",
    "    print(f\"Most challenging: {doc_difficulty.index[0]} (avg F1: {doc_difficulty.iloc[0]['mean']:.3f})\")\n",
    "    print(f\"Easiest: {doc_difficulty.index[-1]} (avg F1: {doc_difficulty.iloc[-1]['mean']:.3f})\")\n",
    "    \n",
    "    print(\"\\nâœ… Enhanced single run visualization created successfully!\")\n",
    "else:\n",
    "    print(f\"âŒ Results file not found: {results_path}\")\n",
    "    print(\"Please update the path to your evaluation results file.\")\n",
    "    print(\"\\nAvailable pipeline results:\")\n",
    "    output_dir = Path(\"output\")\n",
    "    if output_dir.exists():\n",
    "        for folder in sorted(output_dir.glob(\"pipeline_results_*\")):\n",
    "            results_file = folder / \"llm_evaluation_results.json\"\n",
    "            if results_file.exists():\n",
    "                print(f\"  - {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24976a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_multiple_runs(results_paths, run_labels=None):\n",
    "    \"\"\"\n",
    "    Compare multiple evaluation runs to see evolution/differences with document-level insights.\n",
    "    \n",
    "    Args:\n",
    "        results_paths: List of paths to llm_evaluation_results.json files\n",
    "        run_labels: Optional list of labels for each run (defaults to folder names)\n",
    "    \"\"\"\n",
    "    \n",
    "    if run_labels is None:\n",
    "        run_labels = [Path(path).parent.name for path in results_paths]\n",
    "    \n",
    "    # Load all results\n",
    "    all_results = {}\n",
    "    all_dfs = []\n",
    "    \n",
    "    for i, (path, label) in enumerate(zip(results_paths, run_labels)):\n",
    "        if Path(path).exists():\n",
    "            results = load_evaluation_results(path)\n",
    "            all_results[label] = results\n",
    "            df = create_dataframe_from_results(results)\n",
    "            df['Run'] = label\n",
    "            df['Run_Order'] = i\n",
    "            all_dfs.append(df)\n",
    "        else:\n",
    "            print(f\"âš ï¸ Warning: File not found: {path}\")\n",
    "    \n",
    "    if not all_dfs:\n",
    "        print(\"âŒ No valid result files found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Create enhanced comparison dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=(\n",
    "            'F1-Score Evolution Across Runs (by Model)', \n",
    "            'Document Performance Evolution',\n",
    "            'Model Performance Comparison (Latest Run)',\n",
    "            'Document Difficulty Ranking Changes',\n",
    "            'Lenient vs Strict Evolution',\n",
    "            'Precision-Recall Evolution by Run',\n",
    "            'Annotation Type Performance Trends',\n",
    "            'Model Consistency Analysis'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"heatmap\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"box\"}]],\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.12\n",
    "    )\n",
    "    \n",
    "    # Enhanced color palettes\n",
    "    model_colors = {\n",
    "        'gemma3:1b': '#E74C3C',    # Red\n",
    "        'gemma3:4b': '#3498DB',    # Blue\n",
    "        'gemma3:12b': '#2ECC71',   # Green\n",
    "        'mistral:latest': '#F39C12' # Orange\n",
    "    }\n",
    "    \n",
    "    annotation_colors = {\n",
    "        'Event': '#9B59B6',       # Purple\n",
    "        'Event_who': '#E67E22',   # Orange\n",
    "        'Event_when': '#1ABC9C',  # Teal\n",
    "        'Event_what': '#E91E63'   # Pink\n",
    "    }\n",
    "    \n",
    "    # Get lenient data for main analysis\n",
    "    df_lenient = combined_df[combined_df['Evaluation_Mode'] == 'Lenient']\n",
    "    \n",
    "    # 1. F1-Score Evolution Across Runs (by Model) - Enhanced with confidence bands\n",
    "    for model in df_lenient['Model'].unique():\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        model_evolution = model_data.groupby('Run_Order')['F1_Score'].agg(['mean', 'std']).reset_index()\n",
    "        model_evolution['Run_Labels'] = [run_labels[i] for i in model_evolution['Run_Order']]\n",
    "        \n",
    "        # Main line\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_evolution['Run_Order'],\n",
    "                y=model_evolution['mean'],\n",
    "                mode='lines+markers',\n",
    "                name=f\"{model}\",\n",
    "                line=dict(color=model_colors.get(model, '#95A5A6'), width=3),\n",
    "                marker=dict(size=10),\n",
    "                hovertemplate=f'<b>{model}</b><br>Run: %{{customdata}}<br>Avg F1: %{{y:.3f}}Â±%{{error_y.array:.3f}}<extra></extra>',\n",
    "                customdata=model_evolution['Run_Labels'],\n",
    "                error_y=dict(\n",
    "                    type='data',\n",
    "                    array=model_evolution['std'],\n",
    "                    visible=True,\n",
    "                    color=model_colors.get(model, '#95A5A6'),\n",
    "                    thickness=1.5\n",
    "                ),\n",
    "                showlegend=True,\n",
    "                legendgroup='models'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Document Performance Evolution - Show how individual documents perform across runs\n",
    "    documents = df_lenient['Document'].unique()\n",
    "    for i, doc in enumerate(documents[:5]):  # Limit to first 5 documents for clarity\n",
    "        doc_evolution = df_lenient[df_lenient['Document'] == doc].groupby('Run_Order')['F1_Score'].mean().reset_index()\n",
    "        doc_evolution['Run_Labels'] = [run_labels[i] for i in doc_evolution['Run_Order']]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=doc_evolution['Run_Order'],\n",
    "                y=doc_evolution['F1_Score'],\n",
    "                mode='lines+markers',\n",
    "                name=f\"Doc: {doc[:15]}...\" if len(doc) > 15 else f\"Doc: {doc}\",\n",
    "                line=dict(width=2, dash='dash'),\n",
    "                marker=dict(size=6),\n",
    "                hovertemplate=f'<b>{doc}</b><br>Run: %{{customdata}}<br>Avg F1: %{{y:.3f}}<extra></extra>',\n",
    "                customdata=doc_evolution['Run_Labels'],\n",
    "                showlegend=True,\n",
    "                legendgroup='documents',\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Model Performance Comparison (Latest Run) - Enhanced with both evaluation modes\n",
    "    latest_run_data = combined_df[combined_df['Run_Order'] == combined_df['Run_Order'].max()]\n",
    "    latest_lenient = latest_run_data[latest_run_data['Evaluation_Mode'] == 'Lenient']\n",
    "    latest_strict = latest_run_data[latest_run_data['Evaluation_Mode'] == 'Strict']\n",
    "    \n",
    "    model_avg_lenient = latest_lenient.groupby('Model')['F1_Score'].mean()\n",
    "    model_avg_strict = latest_strict.groupby('Model')['F1_Score'].mean()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=model_avg_lenient.index,\n",
    "            y=model_avg_lenient.values,\n",
    "            name='Lenient (Latest)',\n",
    "            marker_color='lightblue',\n",
    "            showlegend=True,\n",
    "            legendgroup='evaluation_latest',\n",
    "            hovertemplate='<b>Lenient</b><br>Model: %{x}<br>F1: %{y:.3f}<extra></extra>'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=model_avg_strict.index,\n",
    "            y=model_avg_strict.values,\n",
    "            name='Strict (Latest)',\n",
    "            marker_color='darkblue',\n",
    "            showlegend=True,\n",
    "            legendgroup='evaluation_latest',\n",
    "            hovertemplate='<b>Strict</b><br>Model: %{x}<br>F1: %{y:.3f}<extra></extra>'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Document Difficulty Ranking Changes - Heatmap showing how document difficulty changes\n",
    "    doc_difficulty_matrix = []\n",
    "    for run_order in sorted(combined_df['Run_Order'].unique()):\n",
    "        run_data = df_lenient[df_lenient['Run_Order'] == run_order]\n",
    "        doc_scores = run_data.groupby('Document')['F1_Score'].mean()\n",
    "        doc_difficulty_matrix.append(doc_scores.values)\n",
    "    \n",
    "    if doc_difficulty_matrix:\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=np.array(doc_difficulty_matrix).T,\n",
    "                x=[f\"Run {i}\" for i in range(len(run_labels))],\n",
    "                y=documents,\n",
    "                colorscale='RdYlBu_r',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Avg F1\", x=0.48),\n",
    "                hovertemplate='Run: %{x}<br>Document: %{y}<br>Avg F1: %{z:.3f}<extra></extra>',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 5. Lenient vs Strict Evolution - Enhanced with trend analysis\n",
    "    for mode in ['Lenient', 'Strict']:\n",
    "        mode_evolution = combined_df[combined_df['Evaluation_Mode'] == mode].groupby('Run_Order')['F1_Score'].agg(['mean', 'std']).reset_index()\n",
    "        mode_evolution['Run_Labels'] = [run_labels[i] for i in mode_evolution['Run_Order']]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=mode_evolution['Run_Order'],\n",
    "                y=mode_evolution['mean'],\n",
    "                mode='lines+markers',\n",
    "                name=f'{mode} Evaluation',\n",
    "                line=dict(width=4, dash='solid' if mode == 'Lenient' else 'dash'),\n",
    "                marker=dict(size=12),\n",
    "                error_y=dict(\n",
    "                    type='data',\n",
    "                    array=mode_evolution['std'],\n",
    "                    visible=True\n",
    "                ),\n",
    "                hovertemplate=f'<b>{mode}</b><br>Run: %{{customdata}}<br>Avg F1: %{{y:.3f}}Â±%{{error_y.array:.3f}}<extra></extra>',\n",
    "                customdata=mode_evolution['Run_Labels'],\n",
    "                showlegend=True,\n",
    "                legendgroup='evaluation_modes'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # 6. Precision-Recall Evolution by Run - Show trajectory over runs\n",
    "    for run_order in sorted(df_lenient['Run_Order'].unique()):\n",
    "        run_data = df_lenient[df_lenient['Run_Order'] == run_order]\n",
    "        model_pr = run_data.groupby('Model')[['Precision', 'Recall']].mean().reset_index()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_pr['Recall'],\n",
    "                y=model_pr['Precision'],\n",
    "                mode='markers+text',\n",
    "                name=f\"{run_labels[run_order]}\",\n",
    "                marker=dict(\n",
    "                    size=12,\n",
    "                    symbol='circle',\n",
    "                    opacity=0.7\n",
    "                ),\n",
    "                text=model_pr['Model'],\n",
    "                textposition=\"top center\",\n",
    "                hovertemplate=f'<b>Run: {run_labels[run_order]}</b><br>Model: %{{text}}<br>Recall: %{{x:.3f}}<br>Precision: %{{y:.3f}}<extra></extra>',\n",
    "                showlegend=True,\n",
    "                legendgroup='runs'\n",
    "            ),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    \n",
    "    # 7. Annotation Type Performance Trends - Enhanced with confidence intervals\n",
    "    for ann_type in df_lenient['Annotation_Type'].unique():\n",
    "        ann_evolution = df_lenient[df_lenient['Annotation_Type'] == ann_type].groupby('Run_Order')['F1_Score'].agg(['mean', 'std']).reset_index()\n",
    "        ann_evolution['Run_Labels'] = [run_labels[i] for i in ann_evolution['Run_Order']]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ann_evolution['Run_Order'],\n",
    "                y=ann_evolution['mean'],\n",
    "                mode='lines+markers',\n",
    "                name=f\"{ann_type}\",\n",
    "                line=dict(color=annotation_colors.get(ann_type, '#95A5A6'), width=3),\n",
    "                marker=dict(size=8),\n",
    "                error_y=dict(\n",
    "                    type='data',\n",
    "                    array=ann_evolution['std'],\n",
    "                    visible=True,\n",
    "                    color=annotation_colors.get(ann_type, '#95A5A6')\n",
    "                ),\n",
    "                hovertemplate=f'<b>{ann_type}</b><br>Run: %{{customdata}}<br>Avg F1: %{{y:.3f}}Â±%{{error_y.array:.3f}}<extra></extra>',\n",
    "                customdata=ann_evolution['Run_Labels'],\n",
    "                showlegend=True,\n",
    "                legendgroup='annotations'\n",
    "            ),\n",
    "            row=4, col=1\n",
    "        )\n",
    "    \n",
    "    # 8. Model Consistency Analysis - Box plots showing variance across runs\n",
    "    for model in df_lenient['Model'].unique():\n",
    "        model_all_runs = df_lenient[df_lenient['Model'] == model]['F1_Score']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=model_all_runs,\n",
    "                name=f\"{model}\",\n",
    "                boxpoints='all',\n",
    "                jitter=0.3,\n",
    "                pointpos=-1.8,\n",
    "                marker_color=model_colors.get(model, '#95A5A6'),\n",
    "                showlegend=True,\n",
    "                legendgroup='model_consistency',\n",
    "                hovertemplate=f'<b>{model}</b><br>F1: %{{y:.3f}}<extra></extra>'\n",
    "            ),\n",
    "            row=4, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout with enhanced styling\n",
    "    fig.update_layout(\n",
    "        height=1600,  # Increased for 4 rows\n",
    "        title='Enhanced Multi-Run LLM Evaluation Comparison<br><sub>Document-Level Analysis & Performance Evolution</sub>',\n",
    "        title_x=0.5,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "            bordercolor=\"rgba(0,0,0,0.2)\",\n",
    "            borderwidth=1,\n",
    "            font=dict(size=10)\n",
    "        ),\n",
    "        font=dict(size=11)\n",
    "    )\n",
    "    \n",
    "    # Update axes labels with better formatting\n",
    "    fig.update_xaxes(title_text=\"Run Number\", row=1, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Average F1 Score\", row=1, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Run Number\", row=1, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=1, col=2, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=2, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=2, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Run\", row=2, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Document\", row=2, col=2, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Run Number\", row=3, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Average F1 Score\", row=3, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Recall\", row=3, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Precision\", row=3, col=2, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Run Number\", row=4, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=4, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=4, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"F1 Score Distribution\", row=4, col=2, title_font_size=10)\n",
    "    \n",
    "    # Add run labels to x-axes where appropriate\n",
    "    for row_col in [(1, 1), (1, 2), (3, 1), (4, 1)]:\n",
    "        fig.update_xaxes(\n",
    "            tickmode='array',\n",
    "            tickvals=list(range(len(run_labels))),\n",
    "            ticktext=run_labels,\n",
    "            row=row_col[0], col=row_col[1],\n",
    "            tickangle=45 if len(max(run_labels, key=len)) > 10 else 0\n",
    "        )\n",
    "    \n",
    "    return fig, combined_df\n",
    "\n",
    "def create_detailed_comparison_report(combined_df, run_labels):\n",
    "    \"\"\"Create a detailed statistical comparison report.\"\"\"\n",
    "    print(\"\\nðŸ“Š DETAILED MULTI-RUN COMPARISON REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_lenient = combined_df[combined_df['Evaluation_Mode'] == 'Lenient']\n",
    "    \n",
    "    # Overall improvement analysis\n",
    "    first_run = df_lenient[df_lenient['Run_Order'] == 0]['F1_Score'].mean()\n",
    "    last_run = df_lenient[df_lenient['Run_Order'] == df_lenient['Run_Order'].max()]['F1_Score'].mean()\n",
    "    improvement = ((last_run - first_run) / first_run) * 100\n",
    "    \n",
    "    print(f\"\\nðŸš€ OVERALL IMPROVEMENT:\")\n",
    "    print(f\"First run average F1: {first_run:.3f}\")\n",
    "    print(f\"Last run average F1: {last_run:.3f}\")\n",
    "    print(f\"Improvement: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Best performing run per model\n",
    "    print(f\"\\nðŸ† BEST PERFORMING RUNS BY MODEL:\")\n",
    "    for model in df_lenient['Model'].unique():\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        best_run = model_data.groupby('Run_Order')['F1_Score'].mean().idxmax()\n",
    "        best_score = model_data.groupby('Run_Order')['F1_Score'].mean().max()\n",
    "        print(f\"â€¢ {model}: Run {best_run} ({run_labels[best_run]}) - F1: {best_score:.3f}\")\n",
    "    \n",
    "    # Document performance stability\n",
    "    print(f\"\\nðŸ“„ DOCUMENT PERFORMANCE STABILITY:\")\n",
    "    doc_stability = []\n",
    "    for doc in df_lenient['Document'].unique():\n",
    "        doc_data = df_lenient[df_lenient['Document'] == doc]\n",
    "        doc_scores = doc_data.groupby('Run_Order')['F1_Score'].mean()\n",
    "        cv = doc_scores.std() / doc_scores.mean() if doc_scores.mean() > 0 else 0\n",
    "        doc_stability.append({'Document': doc, 'CV': cv, 'Avg_F1': doc_scores.mean()})\n",
    "    \n",
    "    doc_stability_df = pd.DataFrame(doc_stability).sort_values('CV')\n",
    "    print(f\"Most stable: {doc_stability_df.iloc[0]['Document'][:30]}... (CV: {doc_stability_df.iloc[0]['CV']:.3f})\")\n",
    "    print(f\"Most variable: {doc_stability_df.iloc[-1]['Document'][:30]}... (CV: {doc_stability_df.iloc[-1]['CV']:.3f})\")\n",
    "    \n",
    "    return doc_stability_df\n",
    "\n",
    "def create_summary_table(combined_df):\n",
    "    \"\"\"Create a summary table for the comparison.\"\"\"\n",
    "    summary = combined_df.groupby(['Run', 'Model', 'Evaluation_Mode']).agg({\n",
    "        'F1_Score': ['mean', 'std'],\n",
    "        'Precision': 'mean',\n",
    "        'Recall': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    summary.columns = ['F1_Mean', 'F1_Std', 'Precision_Mean', 'Recall_Mean']\n",
    "    return summary.reset_index()\n",
    "\n",
    "# Enhanced example usage for multiple runs comparison with detailed analysis\n",
    "results_paths = [\n",
    "    \"output/pipeline_results_20250804_170535/llm_evaluation_results.json\",\n",
    "    # Add more paths here for comparison, e.g.:\n",
    "    # \"output/pipeline_results_20250805_120000/llm_evaluation_results.json\",\n",
    "    # \"output/pipeline_results_20250806_150000/llm_evaluation_results.json\"\n",
    "]\n",
    "\n",
    "run_labels = [\n",
    "    \"Baseline Run\",\n",
    "    # Add corresponding labels, e.g.:\n",
    "    # \"Improved Prompts\",\n",
    "    # \"Fine-tuned Models\"\n",
    "]\n",
    "\n",
    "# Check if we have multiple runs to compare\n",
    "if len(results_paths) > 1:\n",
    "    print(\"ðŸ”„ Analyzing multiple evaluation runs...\")\n",
    "    fig_comparison, df_comparison = compare_multiple_runs(results_paths, run_labels)\n",
    "    \n",
    "    if fig_comparison is not None:\n",
    "        fig_comparison.show()\n",
    "        \n",
    "        # Show enhanced summary table\n",
    "        summary_table = create_summary_table(df_comparison)\n",
    "        print(\"\\nðŸ“Š STATISTICAL SUMMARY TABLE:\")\n",
    "        print(summary_table.to_string(index=False))\n",
    "        \n",
    "        # Create detailed comparison report\n",
    "        doc_stability = create_detailed_comparison_report(df_comparison, run_labels)\n",
    "        \n",
    "        print(\"\\nâœ… Enhanced multi-run comparison visualization created successfully!\")\n",
    "    else:\n",
    "        print(\"âŒ Failed to create comparison - check file paths.\")\n",
    "        \n",
    "else:\n",
    "    print(\"ðŸ“ For multi-run comparison, add more result file paths to 'results_paths' list above.\")\n",
    "    print(\"\\nðŸ” Available pipeline results:\")\n",
    "    output_dir = Path(\"output\")\n",
    "    if output_dir.exists():\n",
    "        available_results = []\n",
    "        for folder in sorted(output_dir.glob(\"pipeline_results_*\")):\n",
    "            results_file = folder / \"llm_evaluation_results.json\"\n",
    "            if results_file.exists():\n",
    "                available_results.append(str(results_file))\n",
    "                print(f\"  âœ“ {results_file}\")\n",
    "        \n",
    "        if len(available_results) > 1:\n",
    "            print(f\"\\nðŸ’¡ Copy these paths to compare multiple runs:\")\n",
    "            for i, path in enumerate(available_results):\n",
    "                print(f'    \"{path}\",')\n",
    "    else:\n",
    "        print(\"  âŒ No output directory found.\")\n",
    "        \n",
    "    print(\"\\nðŸŽ¯ Example configuration for multiple runs:\")\n",
    "    print('''results_paths = [\n",
    "    \"output/pipeline_results_20250804_170535/llm_evaluation_results.json\",\n",
    "    \"output/pipeline_results_20250805_120000/llm_evaluation_results.json\",\n",
    "    \"output/pipeline_results_20250806_150000/llm_evaluation_results.json\"\n",
    "]\n",
    "\n",
    "run_labels = [\n",
    "    \"Baseline\",\n",
    "    \"Improved Prompts\", \n",
    "    \"Fine-tuned Models\"\n",
    "]''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
