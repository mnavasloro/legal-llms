{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebcc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process evaluation data from multiple pipeline results\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Find all pipeline_results folders\n",
    "results_folder = \"output\"\n",
    "pipeline_folders = [f for f in os.listdir(results_folder) \n",
    "                    if f.startswith(\"pipeline_results_\") and os.path.isdir(os.path.join(results_folder, f))]\n",
    "\n",
    "# Filter folders that have evaluation files\n",
    "valid_folders = []\n",
    "for folder in pipeline_folders:\n",
    "    evaluation_file = os.path.join(results_folder, folder, \"llm_evaluation_results.json\")\n",
    "    if os.path.exists(evaluation_file):\n",
    "        valid_folders.append(folder)\n",
    "    else:\n",
    "        print(f\"Warning: No evaluation file found in {folder}\")\n",
    "\n",
    "print(f\"Found {len(valid_folders)} pipeline_results folders:\")\n",
    "for folder in sorted(valid_folders):\n",
    "    print(f\"  - {folder}\")\n",
    "\n",
    "# Create multi-select widget for runs\n",
    "run_selector = widgets.SelectMultiple(\n",
    "    options=[(folder, folder) for folder in sorted(valid_folders)],\n",
    "    description='Select runs:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='50%', height='150px')\n",
    ")\n",
    "\n",
    "# Auto-select first 3 runs for testing\n",
    "# if len(valid_folders) >= 3:\n",
    "#     run_selector.value = tuple(sorted(valid_folders)[:3])\n",
    "#     print(f\"\\nAuto-selected first 3 runs: {list(run_selector.value)}\")\n",
    "# else:\n",
    "#     run_selector.value = tuple(sorted(valid_folders))\n",
    "#     print(f\"\\nAuto-selected all available runs: {list(run_selector.value)}\")\n",
    "\n",
    "# Load selected data\n",
    "def load_selected_runs():\n",
    "    selected_runs = run_selector.value\n",
    "    if not selected_runs:\n",
    "        print(\"‚ùå No runs selected!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loading {len(selected_runs)} selected runs...\")\n",
    "    \n",
    "    global evaluation_data\n",
    "    evaluation_data = {}\n",
    "    \n",
    "    for run_name in selected_runs:\n",
    "        evaluation_file = os.path.join(results_folder, run_name, \"llm_evaluation_results.json\")\n",
    "        \n",
    "        try:\n",
    "            with open(evaluation_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                evaluation_data[run_name] = data\n",
    "                print(f\"‚úÖ Loaded {run_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {run_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully loaded {len(evaluation_data)} runs\")\n",
    "    return evaluation_data\n",
    "\n",
    "# Load the data immediately\n",
    "evaluation_data = load_selected_runs()\n",
    "\n",
    "display(run_selector)\n",
    "\n",
    "# Button for manual reloading if needed\n",
    "load_button = widgets.Button(\n",
    "    description='Load Selected Data',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        load_selected_runs()\n",
    "\n",
    "load_button.on_click(on_button_click)\n",
    "\n",
    "display(load_button)\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbef9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_evaluation_data(evaluation_data):\n",
    "    \"\"\"Process evaluation data into a structured format for visualization with annotation type details\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for run_name, data in evaluation_data.items():\n",
    "        # The data structure is: {document: {model: {annotation_type: {lenient/strict: metrics}}}}\n",
    "        for doc_name, doc_data in data.items():\n",
    "            if not isinstance(doc_data, dict):\n",
    "                continue\n",
    "                \n",
    "            for model_name, model_data in doc_data.items():\n",
    "                if not isinstance(model_data, dict):\n",
    "                    continue\n",
    "                \n",
    "                # Process each annotation type separately\n",
    "                annotation_types = ['Event', 'Event_who', 'Event_when', 'Event_what']\n",
    "                \n",
    "                # Calculate overall metrics (aggregated across all annotation types)\n",
    "                total_tp_lenient = 0\n",
    "                total_fp_lenient = 0\n",
    "                total_fn_lenient = 0\n",
    "                total_tp_strict = 0\n",
    "                total_fp_strict = 0\n",
    "                total_fn_strict = 0\n",
    "                \n",
    "                annotation_type_count = 0\n",
    "                \n",
    "                # Add individual annotation type metrics\n",
    "                for ann_type in annotation_types:\n",
    "                    if ann_type in model_data and isinstance(model_data[ann_type], dict):\n",
    "                        ann_data = model_data[ann_type]\n",
    "                        annotation_type_count += 1\n",
    "                        \n",
    "                        # Extract lenient metrics\n",
    "                        if 'lenient' in ann_data:\n",
    "                            lenient_metrics = ann_data['lenient']\n",
    "                            precision_l = lenient_metrics.get('precision', 0)\n",
    "                            recall_l = lenient_metrics.get('recall', 0)\n",
    "                            f1_score_l = lenient_metrics.get('f1_score', 0)\n",
    "                            tp_l = lenient_metrics.get('true_positives', 0)\n",
    "                            fp_l = lenient_metrics.get('false_positives', 0)\n",
    "                            fn_l = lenient_metrics.get('false_negatives', 0)\n",
    "                            \n",
    "                            total_tp_lenient += tp_l\n",
    "                            total_fp_lenient += fp_l\n",
    "                            total_fn_lenient += fn_l\n",
    "                        else:\n",
    "                            precision_l = recall_l = f1_score_l = 0\n",
    "                            tp_l = fp_l = fn_l = 0\n",
    "                        \n",
    "                        # Extract strict metrics\n",
    "                        if 'strict' in ann_data:\n",
    "                            strict_metrics = ann_data['strict']\n",
    "                            precision_s = strict_metrics.get('precision', 0)\n",
    "                            recall_s = strict_metrics.get('recall', 0)\n",
    "                            f1_score_s = strict_metrics.get('f1_score', 0)\n",
    "                            tp_s = strict_metrics.get('true_positives', 0)\n",
    "                            fp_s = strict_metrics.get('false_positives', 0)\n",
    "                            fn_s = strict_metrics.get('false_negatives', 0)\n",
    "                            \n",
    "                            total_tp_strict += tp_s\n",
    "                            total_fp_strict += fp_s\n",
    "                            total_fn_strict += fn_s\n",
    "                        else:\n",
    "                            precision_s = recall_s = f1_score_s = 0\n",
    "                            tp_s = fp_s = fn_s = 0\n",
    "                        \n",
    "                        # Add record for this specific annotation type\n",
    "                        processed_data.append({\n",
    "                            'run': run_name,\n",
    "                            'model': model_name,\n",
    "                            'document': doc_name,\n",
    "                            'annotation_type': ann_type,\n",
    "                            'evaluation_mode': 'lenient',\n",
    "                            'precision': precision_l,\n",
    "                            'recall': recall_l,\n",
    "                            'f1_score': f1_score_l,\n",
    "                            'true_positives': tp_l,\n",
    "                            'false_positives': fp_l,\n",
    "                            'false_negatives': fn_l,\n",
    "                            'gold_count': ann_data.get('gold_count', 0),\n",
    "                            'predicted_count': ann_data.get('predicted_count', 0)\n",
    "                        })\n",
    "                        \n",
    "                        processed_data.append({\n",
    "                            'run': run_name,\n",
    "                            'model': model_name,\n",
    "                            'document': doc_name,\n",
    "                            'annotation_type': ann_type,\n",
    "                            'evaluation_mode': 'strict',\n",
    "                            'precision': precision_s,\n",
    "                            'recall': recall_s,\n",
    "                            'f1_score': f1_score_s,\n",
    "                            'true_positives': tp_s,\n",
    "                            'false_positives': fp_s,\n",
    "                            'false_negatives': fn_s,\n",
    "                            'gold_count': ann_data.get('gold_count', 0),\n",
    "                            'predicted_count': ann_data.get('predicted_count', 0)\n",
    "                        })\n",
    "                \n",
    "                # Add overall metrics (aggregated across all annotation types)\n",
    "                if annotation_type_count > 0:\n",
    "                    # Calculate overall lenient metrics\n",
    "                    overall_precision_l = total_tp_lenient / (total_tp_lenient + total_fp_lenient) if (total_tp_lenient + total_fp_lenient) > 0 else 0\n",
    "                    overall_recall_l = total_tp_lenient / (total_tp_lenient + total_fn_lenient) if (total_tp_lenient + total_fn_lenient) > 0 else 0\n",
    "                    overall_f1_l = 2 * (overall_precision_l * overall_recall_l) / (overall_precision_l + overall_recall_l) if (overall_precision_l + overall_recall_l) > 0 else 0\n",
    "                    \n",
    "                    # Calculate overall strict metrics\n",
    "                    overall_precision_s = total_tp_strict / (total_tp_strict + total_fp_strict) if (total_tp_strict + total_fp_strict) > 0 else 0\n",
    "                    overall_recall_s = total_tp_strict / (total_tp_strict + total_fn_strict) if (total_tp_strict + total_fn_strict) > 0 else 0\n",
    "                    overall_f1_s = 2 * (overall_precision_s * overall_recall_s) / (overall_precision_s + overall_recall_s) if (overall_precision_s + overall_recall_s) > 0 else 0\n",
    "                    \n",
    "                    # Add overall records\n",
    "                    processed_data.append({\n",
    "                        'run': run_name,\n",
    "                        'model': model_name,\n",
    "                        'document': doc_name,\n",
    "                        'annotation_type': 'Overall',\n",
    "                        'evaluation_mode': 'lenient',\n",
    "                        'precision': overall_precision_l,\n",
    "                        'recall': overall_recall_l,\n",
    "                        'f1_score': overall_f1_l,\n",
    "                        'true_positives': total_tp_lenient,\n",
    "                        'false_positives': total_fp_lenient,\n",
    "                        'false_negatives': total_fn_lenient,\n",
    "                        'gold_count': 0,  # Not meaningful for overall\n",
    "                        'predicted_count': 0  # Not meaningful for overall\n",
    "                    })\n",
    "                    \n",
    "                    processed_data.append({\n",
    "                        'run': run_name,\n",
    "                        'model': model_name,\n",
    "                        'document': doc_name,\n",
    "                        'annotation_type': 'Overall',\n",
    "                        'evaluation_mode': 'strict',\n",
    "                        'precision': overall_precision_s,\n",
    "                        'recall': overall_recall_s,\n",
    "                        'f1_score': overall_f1_s,\n",
    "                        'true_positives': total_tp_strict,\n",
    "                        'false_positives': total_fp_strict,\n",
    "                        'false_negatives': total_fn_strict,\n",
    "                        'gold_count': 0,  # Not meaningful for overall\n",
    "                        'predicted_count': 0  # Not meaningful for overall\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "def create_annotation_type_heatmap(df, evaluation_mode='lenient', metric='f1_score'):\n",
    "    \"\"\"Create a heatmap showing performance by annotation type across models\"\"\"\n",
    "    \n",
    "    # Filter data for the specified evaluation mode\n",
    "    df_filtered = df[df['evaluation_mode'] == evaluation_mode].copy()\n",
    "    \n",
    "    # Exclude 'Overall' for this specific view\n",
    "    df_filtered = df_filtered[df_filtered['annotation_type'] != 'Overall']\n",
    "    \n",
    "    # Create pivot table with annotation types as rows and models as columns\n",
    "    pivot_data = df_filtered.pivot_table(\n",
    "        index='annotation_type', \n",
    "        columns='model', \n",
    "        values=metric, \n",
    "        aggfunc='mean'  # Average across documents and runs\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Reorder annotation types for better display\n",
    "    desired_order = ['Event', 'Event_who', 'Event_when', 'Event_what']\n",
    "    available_types = [t for t in desired_order if t in pivot_data.index]\n",
    "    pivot_data = pivot_data.reindex(available_types)\n",
    "    \n",
    "    num_runs = len(df['run'].unique())\n",
    "    title_text = f\"Annotation Type Performance - {metric.replace('_', ' ').title()} ({evaluation_mode.title()})\"\n",
    "    if num_runs > 1:\n",
    "        title_text += f\" (Average across {num_runs} runs)\"\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=pivot_data.values,\n",
    "        x=pivot_data.columns,\n",
    "        y=pivot_data.index,\n",
    "        colorscale='RdYlGn',\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "        text=pivot_data.values.round(3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12},\n",
    "        colorbar=dict(\n",
    "            title=metric.replace('_', ' ').title()\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title_text,\n",
    "            font=dict(size=18, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Models\",\n",
    "            tickfont=dict(size=12)\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Annotation Types\",\n",
    "            tickfont=dict(size=12)\n",
    "        ),\n",
    "        height=400,\n",
    "        width=max(800, len(pivot_data.columns) * 120),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_annotation_type_difficulty_chart(df, evaluation_mode='lenient'):\n",
    "    \"\"\"Create a chart showing which annotation types are most difficult across all models\"\"\"\n",
    "    \n",
    "    # Filter data for the specified evaluation mode and exclude 'Overall'\n",
    "    df_filtered = df[(df['evaluation_mode'] == evaluation_mode) & (df['annotation_type'] != 'Overall')].copy()\n",
    "    \n",
    "    # Calculate average F1 score per annotation type across all models and documents\n",
    "    ann_stats = df_filtered.groupby('annotation_type').agg({\n",
    "        'f1_score': ['mean', 'std', 'min', 'max', 'count'],\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    ann_stats.columns = ['_'.join(col).strip() for col in ann_stats.columns]\n",
    "    ann_stats = ann_stats.reset_index()\n",
    "    ann_stats = ann_stats.sort_values('f1_score_mean', ascending=True)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar chart with error bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=ann_stats['f1_score_mean'],\n",
    "        y=ann_stats['annotation_type'],\n",
    "        orientation='h',\n",
    "        error_x=dict(\n",
    "            type='data',\n",
    "            array=ann_stats['f1_score_std'],\n",
    "            visible=True\n",
    "        ),\n",
    "        marker=dict(\n",
    "            color=ann_stats['f1_score_mean'],\n",
    "            colorscale='RdYlGn',\n",
    "            cmin=0,\n",
    "            cmax=1,\n",
    "            colorbar=dict(title=\"Average F1-Score\")\n",
    "        ),\n",
    "        text=ann_stats['f1_score_mean'].round(3),\n",
    "        textposition='auto',\n",
    "        width=0.6,  # Make bars thinner\n",
    "        customdata=np.column_stack((ann_stats['precision_mean'], ann_stats['recall_mean'], ann_stats['f1_score_count'])),\n",
    "        hovertemplate='<b>%{y}</b><br>' +\n",
    "                     'F1-Score: %{x:.3f}<br>' +\n",
    "                     'Precision: %{customdata[0]:.3f}<br>' +\n",
    "                     'Recall: %{customdata[1]:.3f}<br>' +\n",
    "                     'Data Points: %{customdata[2]}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    title_text = f\"Annotation Type Difficulty Ranking ({evaluation_mode.title()} Evaluation)\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title_text,\n",
    "            font=dict(size=16, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Average F1-Score\",\n",
    "            range=[0, 1]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Annotation Types\"\n",
    "        ),\n",
    "        height=300,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_model_comparison_by_annotation_type(df, evaluation_mode='lenient'):\n",
    "    \"\"\"Create a detailed comparison of models for each annotation type\"\"\"\n",
    "    \n",
    "    # Filter data for the specified evaluation mode and exclude 'Overall'\n",
    "    df_filtered = df[(df['evaluation_mode'] == evaluation_mode) & (df['annotation_type'] != 'Overall')].copy()\n",
    "    \n",
    "    # Get unique annotation types and models\n",
    "    annotation_types = ['Event', 'Event_who', 'Event_when', 'Event_what']\n",
    "    available_types = [t for t in annotation_types if t in df_filtered['annotation_type'].unique()]\n",
    "    models = sorted(df_filtered['model'].unique())\n",
    "    \n",
    "    # Create subplots for each annotation type\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=available_types,\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    colors = px.colors.qualitative.Set1[:len(models)]\n",
    "    \n",
    "    for i, ann_type in enumerate(available_types):\n",
    "        row = (i // 2) + 1\n",
    "        col = (i % 2) + 1\n",
    "        \n",
    "        # Filter data for this annotation type\n",
    "        type_data = df_filtered[df_filtered['annotation_type'] == ann_type]\n",
    "        \n",
    "        # Calculate average metrics per model for this annotation type\n",
    "        model_stats = type_data.groupby('model').agg({\n",
    "            'precision': ['mean', 'std'],\n",
    "            'recall': ['mean', 'std'],\n",
    "            'f1_score': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        \n",
    "        model_stats.columns = ['_'.join(col).strip() for col in model_stats.columns]\n",
    "        model_stats = model_stats.reset_index()\n",
    "        \n",
    "        # Add bars for each metric\n",
    "        metrics = ['precision_mean', 'recall_mean', 'f1_score_mean']\n",
    "        metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "        metric_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for j, (metric, name, color) in enumerate(zip(metrics, metric_names, metric_colors)):\n",
    "            show_legend = (i == 0)  # Only show legend for first subplot\n",
    "            \n",
    "            fig.add_trace(go.Bar(\n",
    "                name=name,\n",
    "                x=model_stats['model'],\n",
    "                y=model_stats[metric],\n",
    "                marker_color=color,\n",
    "                text=model_stats[metric].round(3),\n",
    "                textposition='auto',\n",
    "                showlegend=show_legend,\n",
    "                legendgroup=name,\n",
    "                width=0.6  # Make bars thinner\n",
    "            ), row=row, col=col)\n",
    "    \n",
    "    title_text = f\"Model Performance by Annotation Type ({evaluation_mode.title()} Evaluation)\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title_text,\n",
    "            font=dict(size=18, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        height=600,\n",
    "        template='plotly_white',\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    # Update all y-axes to have same range\n",
    "    for i in range(1, 3):\n",
    "        for j in range(1, 3):\n",
    "            fig.update_yaxes(range=[0, 1], row=i, col=j)\n",
    "            fig.update_xaxes(tickangle=45, row=i, col=j)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fbafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about loaded runs and data aggregation\n",
    "if 'evaluation_data' in globals() and evaluation_data:\n",
    "    print(\"üìã LOADED RUNS INFORMATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of runs loaded: {len(evaluation_data)}\")\n",
    "    print(\"Runs included in analysis:\")\n",
    "    for i, run_name in enumerate(evaluation_data.keys(), 1):\n",
    "        print(f\"  {i}. {run_name}\")\n",
    "    \n",
    "    print(\"\\nüîÑ DATA PRESENTATION METHOD:\")\n",
    "    if len(evaluation_data) == 1:\n",
    "        print(\"- Single run: Results shown directly without aggregation\")\n",
    "    else:\n",
    "        print(\"- Multiple runs: Results shown SEPARATELY for each run\")\n",
    "        print(\"- Each run is displayed in its own section/subplot\")\n",
    "        print(\"- NO AVERAGING - you can see the evolution across runs!\")\n",
    "    print(\"- This allows you to compare how performance changes with different settings\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(\"‚ùå No evaluation data loaded yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the evaluation data into a DataFrame\n",
    "if 'evaluation_data' in globals() and evaluation_data:\n",
    "    print(\"üîÑ Processing evaluation data...\")\n",
    "    df = process_evaluation_data(evaluation_data)\n",
    "    \n",
    "    # Filter for 'Overall' annotation type and 'lenient' evaluation mode for main visualizations\n",
    "    df_main = df[(df['annotation_type'] == 'Overall') & (df['evaluation_mode'] == 'lenient')].copy()\n",
    "    \n",
    "    print(f\"‚úÖ Processed data into DataFrame:\")\n",
    "    print(f\"   - Total records: {len(df)}\")\n",
    "    print(f\"   - Main visualization records: {len(df_main)}\")\n",
    "    print(f\"   - Runs: {df['run'].nunique()}\")\n",
    "    print(f\"   - Models: {df['model'].nunique()}\")\n",
    "    print(f\"   - Documents: {df['document'].nunique()}\")\n",
    "    print(f\"   - Annotation types: {sorted(df['annotation_type'].unique())}\")\n",
    "else:\n",
    "    print(\"‚ùå No evaluation data to process. Please load data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_comparison_chart(df):\n",
    "    \"\"\"Create a model comparison chart showing average performance with error bars\"\"\"\n",
    "    \n",
    "    # Calculate statistics for each model\n",
    "    model_stats = df.groupby('model').agg({\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1_score': ['mean', 'std'],\n",
    "        'document': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    model_stats.columns = ['_'.join(col).strip() for col in model_stats.columns]\n",
    "    model_stats = model_stats.reset_index()\n",
    "    model_stats = model_stats.sort_values('f1_score_mean', ascending=False)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Colors for different metrics\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    metrics = ['precision_mean', 'recall_mean', 'f1_score_mean']\n",
    "    metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "    error_cols = ['precision_std', 'recall_std', 'f1_score_std']\n",
    "    \n",
    "    for i, (metric, name, color, error_col) in enumerate(zip(metrics, metric_names, colors, error_cols)):\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=name,\n",
    "            x=model_stats['model'],\n",
    "            y=model_stats[metric],\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                array=model_stats[error_col],\n",
    "                visible=True\n",
    "            ),\n",
    "            marker_color=color,\n",
    "            text=model_stats[metric].round(3),\n",
    "            textposition='auto',\n",
    "            width=0.6  # Make bars thinner\n",
    "        ))\n",
    "    \n",
    "    num_runs = len(df['run'].unique())\n",
    "    title_text = \"Model Performance Comparison\"\n",
    "    if num_runs > 1:\n",
    "        title_text += f\" (Average across {num_runs} runs)\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title_text,\n",
    "            font=dict(size=18, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Models\",\n",
    "            tickangle=45\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Score\",\n",
    "            range=[0, 1]\n",
    "        ),\n",
    "        height=500,\n",
    "        template='plotly_white',\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_document_difficulty_chart(df):\n",
    "    \"\"\"Create a chart showing document difficulty ranking\"\"\"\n",
    "    \n",
    "    # Calculate average F1 score per document\n",
    "    doc_stats = df.groupby('document').agg({\n",
    "        'f1_score': ['mean', 'std', 'min', 'max', 'count'],\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    doc_stats.columns = ['_'.join(col).strip() for col in doc_stats.columns]\n",
    "    doc_stats = doc_stats.reset_index()\n",
    "    doc_stats = doc_stats.sort_values('f1_score_mean', ascending=True)  # Hardest first\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar chart with error bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=doc_stats['f1_score_mean'],\n",
    "        y=doc_stats['document'],\n",
    "        orientation='h',\n",
    "        error_x=dict(\n",
    "            type='data',\n",
    "            array=doc_stats['f1_score_std'],\n",
    "            visible=True\n",
    "        ),\n",
    "        marker=dict(\n",
    "            color=doc_stats['f1_score_mean'],\n",
    "            colorscale='RdYlGn',\n",
    "            cmin=0,\n",
    "            cmax=1,\n",
    "            colorbar=dict(title=\"Average F1-Score\")\n",
    "        ),\n",
    "        text=doc_stats['f1_score_mean'].round(3),\n",
    "        textposition='auto',\n",
    "        width=0.6,  # Make bars thinner\n",
    "        customdata=np.column_stack((doc_stats['precision_mean'], doc_stats['recall_mean'], doc_stats['f1_score_count'])),\n",
    "        hovertemplate='<b>%{y}</b><br>' +\n",
    "                     'F1-Score: %{x:.3f}<br>' +\n",
    "                     'Precision: %{customdata[0]:.3f}<br>' +\n",
    "                     'Recall: %{customdata[1]:.3f}<br>' +\n",
    "                     'Data Points: %{customdata[2]}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    num_runs = len(df['run'].unique())\n",
    "    title_text = \"Document Difficulty Ranking (Hardest First)\"\n",
    "    if num_runs > 1:\n",
    "        title_text += f\" (Average across {num_runs} runs)\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title_text,\n",
    "            font=dict(size=16, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Average F1-Score\",\n",
    "            range=[0, 1]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Documents\"\n",
    "        ),\n",
    "        height=max(400, len(doc_stats) * 30),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531718d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_heatmap(df, metric, title_suffix):\n",
    "    \"\"\"Create a heatmap showing model performance across documents for each run separately\"\"\"\n",
    "    \n",
    "    # Check if we have multiple runs\n",
    "    num_runs = len(df['run'].unique())\n",
    "    \n",
    "    if num_runs == 1:\n",
    "        # Single run - create simple heatmap\n",
    "        pivot_data = df.pivot_table(\n",
    "            index='model', \n",
    "            columns='document', \n",
    "            values=metric, \n",
    "            aggfunc='first'  # Take the single value\n",
    "        ).fillna(0)\n",
    "        \n",
    "        title_text = f\"Model Performance Heatmap - {title_suffix} ({df['run'].iloc[0]})\"\n",
    "        \n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=pivot_data.values,\n",
    "            x=pivot_data.columns,\n",
    "            y=pivot_data.index,\n",
    "            colorscale='RdYlGn',\n",
    "            zmin=0,\n",
    "            zmax=1,\n",
    "            text=pivot_data.values.round(3),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 10},\n",
    "            colorbar=dict(title=title_suffix)\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(text=title_text, font=dict(size=20, color='#2E2E2E'), x=0.5),\n",
    "            xaxis=dict(title=\"Documents\", tickangle=45, tickfont=dict(size=10)),\n",
    "            yaxis=dict(title=\"Models\", tickfont=dict(size=12)),\n",
    "            height=max(400, len(pivot_data.index) * 50),\n",
    "            width=max(800, len(pivot_data.columns) * 80),\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    else:\n",
    "        # Multiple runs - create subplots for each run\n",
    "        runs = sorted(df['run'].unique())\n",
    "        cols = min(3, len(runs))  # Max 3 columns\n",
    "        rows = (len(runs) + cols - 1) // cols  # Calculate rows needed\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=rows, cols=cols,\n",
    "            subplot_titles=runs,\n",
    "            shared_xaxes=True,\n",
    "            shared_yaxes=True,\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        for i, run in enumerate(runs):\n",
    "            row = i // cols + 1\n",
    "            col = i % cols + 1\n",
    "            \n",
    "            run_data = df[df['run'] == run]\n",
    "            pivot_data = run_data.pivot_table(\n",
    "                index='model', \n",
    "                columns='document', \n",
    "                values=metric, \n",
    "                aggfunc='first'\n",
    "            ).fillna(0)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=pivot_data.values,\n",
    "                    x=pivot_data.columns,\n",
    "                    y=pivot_data.index,\n",
    "                    colorscale='RdYlGn',\n",
    "                    zmin=0,\n",
    "                    zmax=1,\n",
    "                    text=pivot_data.values.round(3),\n",
    "                    texttemplate=\"%{text}\",\n",
    "                    textfont={\"size\": 8},\n",
    "                    showscale=(i == 0),  # Only show colorbar for first subplot\n",
    "                    colorbar=dict(title=title_suffix) if i == 0 else None\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        \n",
    "        title_text = f\"Model Performance Comparison Across Runs - {title_suffix}\"\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(\n",
    "                text=title_text,\n",
    "                font=dict(size=18, color='#2E2E2E'),\n",
    "                x=0.5\n",
    "            ),\n",
    "            height=max(400, rows * 300),\n",
    "            width=max(1200, cols * 400),\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a41e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_main' in globals() and not df_main.empty:\n",
    "    print(\"üéØ Creating Performance Visualizations...\")\n",
    "    \n",
    "    # 1. F1-Score Heatmap - Shows model performance across documents at a glance\n",
    "    print(\"üìä 1. Model Performance Heatmap (F1-Score)\")\n",
    "    f1_heatmap = create_performance_heatmap(df_main, 'f1_score', 'F1-Score')\n",
    "    f1_heatmap.show()\n",
    "    \n",
    "    # 2. Model Comparison Chart - Shows average performance with error bars\n",
    "    print(\"üìà 2. Model Performance Comparison\")\n",
    "    model_comparison = create_model_comparison_chart(df_main)\n",
    "    model_comparison.show()\n",
    "    \n",
    "    # 3. Document Difficulty Ranking\n",
    "    print(\"üìã 3. Document Difficulty Ranking\")\n",
    "    doc_difficulty = create_document_difficulty_chart(df_main)\n",
    "    doc_difficulty.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Please load and process evaluation data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_run_comparison_dashboard(df):\n",
    "    \"\"\"Create a comprehensive dashboard comparing performance across different runs\"\"\"\n",
    "    \n",
    "    # Check if we have multiple runs\n",
    "    runs = df['run'].unique()\n",
    "    if len(runs) <= 1:\n",
    "        print(\"Only one run available - no comparison needed.\")\n",
    "        return None\n",
    "    \n",
    "    # Filter for overall metrics and lenient evaluation\n",
    "    df_overall = df[(df['annotation_type'] == 'Overall') & (df['evaluation_mode'] == 'lenient')].copy()\n",
    "    \n",
    "    if df_overall.empty:\n",
    "        print(\"No overall metrics found for comparison.\")\n",
    "        return None\n",
    "    \n",
    "    # Create subplots: 2 rows, 2 columns\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            \"Average F1-Score by Run\",\n",
    "            \"Model Performance Across Runs\", \n",
    "            \"Run-to-Run Performance Change\",\n",
    "            \"Document Difficulty Across Runs\"\n",
    "        ],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # 1. Average F1-Score by Run (Top Left)\n",
    "    run_stats = df_overall.groupby('run').agg({\n",
    "        'f1_score': ['mean', 'std'],\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean'\n",
    "    }).round(4)\n",
    "    run_stats.columns = ['f1_mean', 'f1_std', 'precision_mean', 'recall_mean']\n",
    "    run_stats = run_stats.reset_index()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=run_stats['run'],\n",
    "        y=run_stats['f1_mean'],\n",
    "        error_y=dict(type='data', array=run_stats['f1_std']),\n",
    "        name='F1-Score',\n",
    "        marker_color='#45B7D1',\n",
    "        text=run_stats['f1_mean'].round(3),\n",
    "        textposition='auto',\n",
    "        showlegend=False\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # 2. Model Performance Across Runs (Top Right)\n",
    "    models = sorted(df_overall['model'].unique())\n",
    "    colors = px.colors.qualitative.Set1[:len(models)]\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df_overall[df_overall['model'] == model]\n",
    "        model_run_stats = model_data.groupby('run')['f1_score'].mean().reset_index()\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=model_run_stats['run'],\n",
    "            y=model_run_stats['f1_score'],\n",
    "            mode='lines+markers',\n",
    "            name=model,\n",
    "            line=dict(color=colors[i], width=3),\n",
    "            marker=dict(size=8),\n",
    "            showlegend=(i < 5)  # Limit legend entries\n",
    "        ), row=1, col=2)\n",
    "    \n",
    "    # 3. Run-to-Run Performance Change (Bottom Left)\n",
    "    if len(runs) >= 2:\n",
    "        sorted_runs = sorted(runs)\n",
    "        changes = []\n",
    "        run_pairs = []\n",
    "        \n",
    "        for i in range(1, len(sorted_runs)):\n",
    "            prev_run = sorted_runs[i-1]\n",
    "            curr_run = sorted_runs[i]\n",
    "            \n",
    "            prev_avg = df_overall[df_overall['run'] == prev_run]['f1_score'].mean()\n",
    "            curr_avg = df_overall[df_overall['run'] == curr_run]['f1_score'].mean()\n",
    "            \n",
    "            change = curr_avg - prev_avg\n",
    "            changes.append(change)\n",
    "            run_pairs.append(f\"{prev_run} ‚Üí {curr_run}\")\n",
    "        \n",
    "        colors_change = ['green' if x >= 0 else 'red' for x in changes]\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=run_pairs,\n",
    "            y=changes,\n",
    "            marker_color=colors_change,\n",
    "            text=[f\"{x:+.3f}\" for x in changes],\n",
    "            textposition='auto',\n",
    "            showlegend=False,\n",
    "            name='Performance Change'\n",
    "        ), row=2, col=1)\n",
    "    \n",
    "    # 4. Document Difficulty Across Runs (Bottom Right)\n",
    "    doc_run_stats = df_overall.groupby(['document', 'run'])['f1_score'].mean().reset_index()\n",
    "    doc_variance = doc_run_stats.groupby('document')['f1_score'].agg(['mean', 'std']).reset_index()\n",
    "    doc_variance = doc_variance.sort_values('std', ascending=False).head(10)  # Top 10 most variable\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=doc_variance['document'],\n",
    "        y=doc_variance['std'],\n",
    "        marker=dict(\n",
    "            color=doc_variance['std'],\n",
    "            colorscale='Reds',\n",
    "            colorbar=dict(title=\"Std Dev\")\n",
    "        ),\n",
    "        text=doc_variance['std'].round(3),\n",
    "        textposition='auto',\n",
    "        showlegend=False,\n",
    "        name='Performance Variance'\n",
    "    ), row=2, col=2)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"Run Comparison Dashboard ({len(runs)} runs)\",\n",
    "            font=dict(size=20, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1400,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Runs\", row=1, col=1, tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"Average F1-Score\", row=1, col=1, range=[0, 1])\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Runs\", row=1, col=2, tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"F1-Score\", row=1, col=2, range=[0, 1])\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Run Transitions\", row=2, col=1, tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"F1-Score Change\", row=2, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Documents\", row=2, col=2, tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"Performance Std Dev\", row=2, col=2)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in globals() and not df.empty:\n",
    "    \n",
    "    # 4. Run Comparison (if multiple runs selected)\n",
    "    if len(df['run'].unique()) > 1:\n",
    "        print(\"üîÑ 4. Run Comparison Dashboard\")\n",
    "        run_comparison = create_run_comparison_dashboard(df)\n",
    "        if run_comparison:\n",
    "            run_comparison.show()\n",
    "    \n",
    "    # 5. Additional Heatmaps for Precision and Recall\n",
    "    print(\"üìä 5. Additional Performance Heatmaps\")\n",
    "    \n",
    "    # Filter for main visualization data (Overall + lenient)\n",
    "    df_main = df[(df['annotation_type'] == 'Overall') & (df['evaluation_mode'] == 'lenient')].copy()\n",
    "    \n",
    "    # Precision heatmap\n",
    "    precision_heatmap = create_performance_heatmap(df_main, 'precision', 'Precision')\n",
    "    precision_heatmap.show()\n",
    "    \n",
    "    # Recall heatmap  \n",
    "    recall_heatmap = create_performance_heatmap(df_main, 'recall', 'Recall')\n",
    "    recall_heatmap.show()\n",
    "    \n",
    "    # 6. Summary Statistics Table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best model per document\n",
    "    print(\"\\nüèÜ BEST MODEL PER DOCUMENT (by F1-Score):\")\n",
    "    best_per_doc = df.loc[df.groupby('document')['f1_score'].idxmax()][['document', 'model', 'f1_score', 'precision', 'recall']]\n",
    "    best_per_doc = best_per_doc.sort_values('f1_score', ascending=False)\n",
    "    display(best_per_doc)\n",
    "    \n",
    "    # Overall model ranking\n",
    "    print(\"\\nü•á OVERALL MODEL RANKING:\")\n",
    "    model_ranking = df.groupby('model').agg({\n",
    "        'f1_score': ['mean', 'std', 'min', 'max'],\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean',\n",
    "        'document': 'count'\n",
    "    }).round(4)\n",
    "    model_ranking.columns = ['F1_Mean', 'F1_Std', 'F1_Min', 'F1_Max', 'Precision_Mean', 'Recall_Mean', 'Documents']\n",
    "    model_ranking = model_ranking.sort_values('F1_Mean', ascending=False)\n",
    "    display(model_ranking)\n",
    "    \n",
    "    # Document difficulty ranking\n",
    "    print(\"\\nüìã DOCUMENT DIFFICULTY RANKING (Hardest First):\")\n",
    "    doc_ranking = df.groupby('document').agg({\n",
    "        'f1_score': ['mean', 'std', 'min', 'max'],\n",
    "        'model': 'count'\n",
    "    }).round(4)\n",
    "    doc_ranking.columns = ['F1_Mean', 'F1_Std', 'F1_Min', 'F1_Max', 'Models_Tested']\n",
    "    doc_ranking = doc_ranking.sort_values('F1_Mean', ascending=True)\n",
    "    display(doc_ranking)\n",
    "    \n",
    "    if len(df['run'].unique()) > 1:\n",
    "        print(\"\\nüîÑ RUN COMPARISON:\")\n",
    "        run_summary = df.groupby('run').agg({\n",
    "            'f1_score': ['mean', 'std'],\n",
    "            'precision': 'mean',\n",
    "            'recall': 'mean',\n",
    "            'model': 'nunique',\n",
    "            'document': 'nunique'\n",
    "        }).round(4)\n",
    "        run_summary.columns = ['F1_Mean', 'F1_Std', 'Precision_Mean', 'Recall_Mean', 'Models', 'Documents']\n",
    "        run_summary = run_summary.sort_values('F1_Mean', ascending=False)\n",
    "        display(run_summary)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Please load and process evaluation data first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
