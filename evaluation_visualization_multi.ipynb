{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebcc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pipeline_results_folders(base_path=\"output\"):\n",
    "    \"\"\"Find all pipeline_results folders in the output directory\"\"\"\n",
    "    pipeline_folders = []\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"Output directory '{base_path}' not found!\")\n",
    "        return []\n",
    "    \n",
    "    for item in base_path.iterdir():\n",
    "        if item.is_dir() and item.name.startswith(\"pipeline_results_\"):\n",
    "            # Check for evaluation file (try both possible names)\n",
    "            eval_json_path1 = item / \"llm_evaluation_results.json\"\n",
    "            eval_json_path2 = item / \"llm_evaluation.json\"\n",
    "            \n",
    "            eval_path = None\n",
    "            if eval_json_path1.exists():\n",
    "                eval_path = eval_json_path1\n",
    "            elif eval_json_path2.exists():\n",
    "                eval_path = eval_json_path2\n",
    "            \n",
    "            if eval_path:\n",
    "                pipeline_folders.append({\n",
    "                    'name': item.name,\n",
    "                    'path': str(item),\n",
    "                    'eval_path': str(eval_path)\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: No evaluation file found in {item.name}\")\n",
    "    \n",
    "    return pipeline_folders\n",
    "\n",
    "def load_evaluation_data(eval_path):\n",
    "    \"\"\"Load evaluation data from JSON file\"\"\"\n",
    "    try:\n",
    "        with open(eval_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {eval_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Find available pipeline results\n",
    "available_folders = find_pipeline_results_folders()\n",
    "\n",
    "if not available_folders:\n",
    "    print(\"No pipeline_results folders found in the output directory!\")\n",
    "else:\n",
    "    print(f\"Found {len(available_folders)} pipeline_results folders:\")\n",
    "    for folder in available_folders:\n",
    "        print(f\"  - {folder['name']}\")\n",
    "\n",
    "# Create selection widget\n",
    "folder_options = [(folder['name'], folder) for folder in available_folders]\n",
    "folder_selector = widgets.SelectMultiple(\n",
    "    options=folder_options,\n",
    "    description='Select runs:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%', height='150px')\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(\n",
    "    description='Load Selected Data',\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def load_data(button):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        selected_folders = list(folder_selector.value)\n",
    "        \n",
    "        if not selected_folders:\n",
    "            print(\"Please select at least one folder!\")\n",
    "            return\n",
    "        \n",
    "        global evaluation_data\n",
    "        evaluation_data = {}\n",
    "        \n",
    "        for folder in selected_folders:\n",
    "            print(f\"Loading data from {folder['name']}...\")\n",
    "            data = load_evaluation_data(folder['eval_path'])\n",
    "            if data:\n",
    "                evaluation_data[folder['name']] = data\n",
    "        \n",
    "        if evaluation_data:\n",
    "            print(f\"\\nSuccessfully loaded data from {len(evaluation_data)} runs:\")\n",
    "            for run_name in evaluation_data.keys():\n",
    "                print(f\"  ✓ {run_name}\")\n",
    "        else:\n",
    "            print(\"No data could be loaded!\")\n",
    "\n",
    "load_button.on_click(load_data)\n",
    "\n",
    "display(folder_selector)\n",
    "display(load_button)\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbef9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_evaluation_data(evaluation_data):\n",
    "    \"\"\"Process evaluation data into a structured format for visualization with annotation type details\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for run_name, data in evaluation_data.items():\n",
    "        # The data structure is: {document: {model: {annotation_type: {lenient/strict: metrics}}}}\n",
    "        for doc_name, doc_data in data.items():\n",
    "            if not isinstance(doc_data, dict):\n",
    "                continue\n",
    "                \n",
    "            for model_name, model_data in doc_data.items():\n",
    "                if not isinstance(model_data, dict):\n",
    "                    continue\n",
    "                \n",
    "                # Process each annotation type separately\n",
    "                annotation_types = ['Event', 'Event_who', 'Event_when', 'Event_what']\n",
    "                \n",
    "                # Calculate overall metrics (aggregated across all annotation types)\n",
    "                total_tp_lenient = 0\n",
    "                total_fp_lenient = 0\n",
    "                total_fn_lenient = 0\n",
    "                total_tp_strict = 0\n",
    "                total_fp_strict = 0\n",
    "                total_fn_strict = 0\n",
    "                \n",
    "                annotation_type_count = 0\n",
    "                \n",
    "                # Add individual annotation type metrics\n",
    "                for ann_type in annotation_types:\n",
    "                    if ann_type in model_data and isinstance(model_data[ann_type], dict):\n",
    "                        ann_data = model_data[ann_type]\n",
    "                        annotation_type_count += 1\n",
    "                        \n",
    "                        # Extract lenient metrics\n",
    "                        if 'lenient' in ann_data:\n",
    "                            lenient_metrics = ann_data['lenient']\n",
    "                            precision_l = lenient_metrics.get('precision', 0)\n",
    "                            recall_l = lenient_metrics.get('recall', 0)\n",
    "                            f1_score_l = lenient_metrics.get('f1_score', 0)\n",
    "                            tp_l = lenient_metrics.get('true_positives', 0)\n",
    "                            fp_l = lenient_metrics.get('false_positives', 0)\n",
    "                            fn_l = lenient_metrics.get('false_negatives', 0)\n",
    "                            \n",
    "                            total_tp_lenient += tp_l\n",
    "                            total_fp_lenient += fp_l\n",
    "                            total_fn_lenient += fn_l\n",
    "                        else:\n",
    "                            precision_l = recall_l = f1_score_l = 0\n",
    "                            tp_l = fp_l = fn_l = 0\n",
    "                        \n",
    "                        # Extract strict metrics\n",
    "                        if 'strict' in ann_data:\n",
    "                            strict_metrics = ann_data['strict']\n",
    "                            precision_s = strict_metrics.get('precision', 0)\n",
    "                            recall_s = strict_metrics.get('recall', 0)\n",
    "                            f1_score_s = strict_metrics.get('f1_score', 0)\n",
    "                            tp_s = strict_metrics.get('true_positives', 0)\n",
    "                            fp_s = strict_metrics.get('false_positives', 0)\n",
    "                            fn_s = strict_metrics.get('false_negatives', 0)\n",
    "                            \n",
    "                            total_tp_strict += tp_s\n",
    "                            total_fp_strict += fp_s\n",
    "                            total_fn_strict += fn_s\n",
    "                        else:\n",
    "                            precision_s = recall_s = f1_score_s = 0\n",
    "                            tp_s = fp_s = fn_s = 0\n",
    "                        \n",
    "                        # Add record for this specific annotation type\n",
    "                        processed_data.append({\n",
    "                            'run': run_name,\n",
    "                            'model': model_name,\n",
    "                            'document': doc_name,\n",
    "                            'annotation_type': ann_type,\n",
    "                            'evaluation_mode': 'lenient',\n",
    "                            'precision': precision_l,\n",
    "                            'recall': recall_l,\n",
    "                            'f1_score': f1_score_l,\n",
    "                            'true_positives': tp_l,\n",
    "                            'false_positives': fp_l,\n",
    "                            'false_negatives': fn_l,\n",
    "                            'gold_count': ann_data.get('gold_count', 0),\n",
    "                            'predicted_count': ann_data.get('predicted_count', 0)\n",
    "                        })\n",
    "                        \n",
    "                        processed_data.append({\n",
    "                            'run': run_name,\n",
    "                            'model': model_name,\n",
    "                            'document': doc_name,\n",
    "                            'annotation_type': ann_type,\n",
    "                            'evaluation_mode': 'strict',\n",
    "                            'precision': precision_s,\n",
    "                            'recall': recall_s,\n",
    "                            'f1_score': f1_score_s,\n",
    "                            'true_positives': tp_s,\n",
    "                            'false_positives': fp_s,\n",
    "                            'false_negatives': fn_s,\n",
    "                            'gold_count': ann_data.get('gold_count', 0),\n",
    "                            'predicted_count': ann_data.get('predicted_count', 0)\n",
    "                        })\n",
    "                \n",
    "                # Add overall metrics (aggregated across all annotation types)\n",
    "                if annotation_type_count > 0:\n",
    "                    # Calculate overall lenient metrics\n",
    "                    overall_precision_l = total_tp_lenient / (total_tp_lenient + total_fp_lenient) if (total_tp_lenient + total_fp_lenient) > 0 else 0\n",
    "                    overall_recall_l = total_tp_lenient / (total_tp_lenient + total_fn_lenient) if (total_tp_lenient + total_fn_lenient) > 0 else 0\n",
    "                    overall_f1_l = 2 * (overall_precision_l * overall_recall_l) / (overall_precision_l + overall_recall_l) if (overall_precision_l + overall_recall_l) > 0 else 0\n",
    "                    \n",
    "                    # Calculate overall strict metrics\n",
    "                    overall_precision_s = total_tp_strict / (total_tp_strict + total_fp_strict) if (total_tp_strict + total_fp_strict) > 0 else 0\n",
    "                    overall_recall_s = total_tp_strict / (total_tp_strict + total_fn_strict) if (total_tp_strict + total_fn_strict) > 0 else 0\n",
    "                    overall_f1_s = 2 * (overall_precision_s * overall_recall_s) / (overall_precision_s + overall_recall_s) if (overall_precision_s + overall_recall_s) > 0 else 0\n",
    "                    \n",
    "                    # Add overall records\n",
    "                    processed_data.append({\n",
    "                        'run': run_name,\n",
    "                        'model': model_name,\n",
    "                        'document': doc_name,\n",
    "                        'annotation_type': 'Overall',\n",
    "                        'evaluation_mode': 'lenient',\n",
    "                        'precision': overall_precision_l,\n",
    "                        'recall': overall_recall_l,\n",
    "                        'f1_score': overall_f1_l,\n",
    "                        'true_positives': total_tp_lenient,\n",
    "                        'false_positives': total_fp_lenient,\n",
    "                        'false_negatives': total_fn_lenient,\n",
    "                        'gold_count': 0,  # Not meaningful for overall\n",
    "                        'predicted_count': 0  # Not meaningful for overall\n",
    "                    })\n",
    "                    \n",
    "                    processed_data.append({\n",
    "                        'run': run_name,\n",
    "                        'model': model_name,\n",
    "                        'document': doc_name,\n",
    "                        'annotation_type': 'Overall',\n",
    "                        'evaluation_mode': 'strict',\n",
    "                        'precision': overall_precision_s,\n",
    "                        'recall': overall_recall_s,\n",
    "                        'f1_score': overall_f1_s,\n",
    "                        'true_positives': total_tp_strict,\n",
    "                        'false_positives': total_fp_strict,\n",
    "                        'false_negatives': total_fn_strict,\n",
    "                        'gold_count': 0,  # Not meaningful for overall\n",
    "                        'predicted_count': 0  # Not meaningful for overall\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "def create_annotation_type_heatmap(df, evaluation_mode='lenient', metric='f1_score'):\n",
    "    \"\"\"Create a heatmap showing performance by annotation type across models\"\"\"\n",
    "    \n",
    "    # Filter data for the specified evaluation mode\n",
    "    df_filtered = df[df['evaluation_mode'] == evaluation_mode].copy()\n",
    "    \n",
    "    # Exclude 'Overall' for this specific view\n",
    "    df_filtered = df_filtered[df_filtered['annotation_type'] != 'Overall']\n",
    "    \n",
    "    # Create pivot table with annotation types as rows and models as columns\n",
    "    pivot_data = df_filtered.pivot_table(\n",
    "        index='annotation_type', \n",
    "        columns='model', \n",
    "        values=metric, \n",
    "        aggfunc='mean'  # Average across documents and runs\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Reorder annotation types for better display\n",
    "    desired_order = ['Event', 'Event_who', 'Event_when', 'Event_what']\n",
    "    available_types = [t for t in desired_order if t in pivot_data.index]\n",
    "    pivot_data = pivot_data.reindex(available_types)\n",
    "    \n",
    "    num_runs = len(df['run'].unique())\n",
    "    title_text = f\"Annotation Type Performance - {metric.replace('_', ' ').title()} ({evaluation_mode.title()})\"\n",
    "    if num_runs > 1:\n",
    "        title_text += f\" (Average across {num_runs} runs)\"\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=pivot_data.values,\n",
    "        x=pivot_data.columns,\n",
    "        y=pivot_data.index,\n",
    "        colorscale='RdYlGn',\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "        text=pivot_data.values.round(3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12},\n",
    "        colorbar=dict(\n",
    "            title=metric.replace('_', ' ').title()\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title_text,\n",
    "            font=dict(size=18, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Models\",\n",
    "            tickfont=dict(size=12)\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Annotation Types\",\n",
    "            tickfont=dict(size=12)\n",
    "        ),\n",
    "        height=400,\n",
    "        width=max(800, len(pivot_data.columns) * 120),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_annotation_type_difficulty_chart(df, evaluation_mode='lenient'):\n",
    "    \"\"\"Create a chart showing which annotation types are most difficult across all models\"\"\"\n",
    "    \n",
    "    # Filter data for the specified evaluation mode and exclude 'Overall'\n",
    "    df_filtered = df[(df['evaluation_mode'] == evaluation_mode) & (df['annotation_type'] != 'Overall')].copy()\n",
    "    \n",
    "    # Calculate average F1 score per annotation type across all models and documents\n",
    "    ann_stats = df_filtered.groupby('annotation_type').agg({\n",
    "        'f1_score': ['mean', 'std', 'min', 'max', 'count'],\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    ann_stats.columns = ['_'.join(col).strip() for col in ann_stats.columns]\n",
    "    ann_stats = ann_stats.reset_index()\n",
    "    ann_stats = ann_stats.sort_values('f1_score_mean', ascending=True)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar chart with error bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=ann_stats['f1_score_mean'],\n",
    "        y=ann_stats['annotation_type'],\n",
    "        orientation='h',\n",
    "        error_x=dict(\n",
    "            type='data',\n",
    "            array=ann_stats['f1_score_std'],\n",
    "            visible=True\n",
    "        ),\n",
    "        marker=dict(\n",
    "            color=ann_stats['f1_score_mean'],\n",
    "            colorscale='RdYlGn',\n",
    "            cmin=0,\n",
    "            cmax=1,\n",
    "            colorbar=dict(title=\"Average F1-Score\")\n",
    "        ),\n",
    "        text=ann_stats['f1_score_mean'].round(3),\n",
    "        textposition='auto',\n",
    "        customdata=np.column_stack((ann_stats['precision_mean'], ann_stats['recall_mean'], ann_stats['f1_score_count'])),\n",
    "        hovertemplate='<b>%{y}</b><br>' +\n",
    "                     'F1-Score: %{x:.3f}<br>' +\n",
    "                     'Precision: %{customdata[0]:.3f}<br>' +\n",
    "                     'Recall: %{customdata[1]:.3f}<br>' +\n",
    "                     'Data Points: %{customdata[2]}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    title_text = f\"Annotation Type Difficulty Ranking ({evaluation_mode.title()} Evaluation)\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title_text,\n",
    "            font=dict(size=16, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Average F1-Score\",\n",
    "            range=[0, 1]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Annotation Types\"\n",
    "        ),\n",
    "        height=300,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_model_comparison_by_annotation_type(df, evaluation_mode='lenient'):\n",
    "    \"\"\"Create a detailed comparison of models for each annotation type\"\"\"\n",
    "    \n",
    "    # Filter data for the specified evaluation mode and exclude 'Overall'\n",
    "    df_filtered = df[(df['evaluation_mode'] == evaluation_mode) & (df['annotation_type'] != 'Overall')].copy()\n",
    "    \n",
    "    # Get unique annotation types and models\n",
    "    annotation_types = ['Event', 'Event_who', 'Event_when', 'Event_what']\n",
    "    available_types = [t for t in annotation_types if t in df_filtered['annotation_type'].unique()]\n",
    "    models = sorted(df_filtered['model'].unique())\n",
    "    \n",
    "    # Create subplots for each annotation type\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=available_types,\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    colors = px.colors.qualitative.Set1[:len(models)]\n",
    "    \n",
    "    for i, ann_type in enumerate(available_types):\n",
    "        row = (i // 2) + 1\n",
    "        col = (i % 2) + 1\n",
    "        \n",
    "        # Filter data for this annotation type\n",
    "        type_data = df_filtered[df_filtered['annotation_type'] == ann_type]\n",
    "        \n",
    "        # Calculate average metrics per model for this annotation type\n",
    "        model_stats = type_data.groupby('model').agg({\n",
    "            'precision': ['mean', 'std'],\n",
    "            'recall': ['mean', 'std'],\n",
    "            'f1_score': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        \n",
    "        model_stats.columns = ['_'.join(col).strip() for col in model_stats.columns]\n",
    "        model_stats = model_stats.reset_index()\n",
    "        \n",
    "        # Add bars for each metric\n",
    "        metrics = ['precision_mean', 'recall_mean', 'f1_score_mean']\n",
    "        metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "        metric_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for j, (metric, name, color) in enumerate(zip(metrics, metric_names, metric_colors)):\n",
    "            show_legend = (i == 0)  # Only show legend for first subplot\n",
    "            \n",
    "            fig.add_trace(go.Bar(\n",
    "                name=name,\n",
    "                x=model_stats['model'],\n",
    "                y=model_stats[metric],\n",
    "                marker_color=color,\n",
    "                text=model_stats[metric].round(3),\n",
    "                textposition='auto',\n",
    "                showlegend=show_legend,\n",
    "                legendgroup=name\n",
    "            ), row=row, col=col)\n",
    "    \n",
    "    title_text = f\"Model Performance by Annotation Type ({evaluation_mode.title()} Evaluation)\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title_text,\n",
    "            font=dict(size=18, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        height=600,\n",
    "        template='plotly_white',\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    # Update all y-axes to have same range\n",
    "    for i in range(1, 3):\n",
    "        for j in range(1, 3):\n",
    "            fig.update_yaxes(range=[0, 1], row=i, col=j)\n",
    "            fig.update_xaxes(tickangle=45, row=i, col=j)\n",
    "    \n",
    "    return fig\n",
    "    \"\"\"Create a heatmap showing model performance across documents for each run separately\"\"\"\n",
    "    \n",
    "    # Check if we have multiple runs\n",
    "    num_runs = len(df['run'].unique())\n",
    "    \n",
    "    if num_runs == 1:\n",
    "        # Single run - create simple heatmap\n",
    "        pivot_data = df.pivot_table(\n",
    "            index='model', \n",
    "            columns='document', \n",
    "            values=metric, \n",
    "            aggfunc='first'  # Take the single value\n",
    "        ).fillna(0)\n",
    "        \n",
    "        title_text = f\"Model Performance Heatmap - {title_suffix} ({df['run'].iloc[0]})\"\n",
    "        \n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=pivot_data.values,\n",
    "            x=pivot_data.columns,\n",
    "            y=pivot_data.index,\n",
    "            colorscale='RdYlGn',\n",
    "            zmin=0,\n",
    "            zmax=1,\n",
    "            text=pivot_data.values.round(3),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 10},\n",
    "            colorbar=dict(title=title_suffix)\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(text=title_text, font=dict(size=20, color='#2E2E2E'), x=0.5),\n",
    "            xaxis=dict(title=\"Documents\", tickangle=45, tickfont=dict(size=10)),\n",
    "            yaxis=dict(title=\"Models\", tickfont=dict(size=12)),\n",
    "            height=max(400, len(pivot_data.index) * 50),\n",
    "            width=max(800, len(pivot_data.columns) * 80),\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    else:\n",
    "        # Multiple runs - create subplots for each run\n",
    "        from plotly.subplots import make_subplots\n",
    "        \n",
    "        runs = sorted(df['run'].unique())\n",
    "        cols = min(3, len(runs))  # Max 3 columns\n",
    "        rows = (len(runs) + cols - 1) // cols  # Calculate rows needed\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=rows, cols=cols,\n",
    "            subplot_titles=runs,\n",
    "            shared_xaxes=True,\n",
    "            shared_yaxes=True,\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        for i, run in enumerate(runs):\n",
    "            row = i // cols + 1\n",
    "            col = i % cols + 1\n",
    "            \n",
    "            run_data = df[df['run'] == run]\n",
    "            pivot_data = run_data.pivot_table(\n",
    "                index='model', \n",
    "                columns='document', \n",
    "                values=metric, \n",
    "                aggfunc='first'\n",
    "            ).fillna(0)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=pivot_data.values,\n",
    "                    x=pivot_data.columns,\n",
    "                    y=pivot_data.index,\n",
    "                    colorscale='RdYlGn',\n",
    "                    zmin=0,\n",
    "                    zmax=1,\n",
    "                    text=pivot_data.values.round(3),\n",
    "                    texttemplate=\"%{text}\",\n",
    "                    textfont={\"size\": 8},\n",
    "                    showscale=(i == 0),  # Only show colorbar for first subplot\n",
    "                    colorbar=dict(title=title_suffix) if i == 0 else None\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        \n",
    "        title_text = f\"Model Performance Comparison Across Runs - {title_suffix}\"\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(\n",
    "                text=title_text,\n",
    "                font=dict(size=18, color='#2E2E2E'),\n",
    "                x=0.5\n",
    "            ),\n",
    "            height=max(400, rows * 300),\n",
    "            width=max(1200, cols * 400),\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def create_model_comparison_chart(df):\n",
    "    \"\"\"Create a chart comparing models across all documents, showing each run separately\"\"\"\n",
    "    \n",
    "    num_runs = len(df['run'].unique())\n",
    "    \n",
    "    if num_runs == 1:\n",
    "        # Single run - original behavior\n",
    "        model_stats = df.groupby('model').agg({\n",
    "            'precision': ['mean', 'std'],\n",
    "            'recall': ['mean', 'std'], \n",
    "            'f1_score': ['mean', 'std'],\n",
    "            'document': 'count'\n",
    "        }).round(4)\n",
    "        \n",
    "        model_stats.columns = ['_'.join(col).strip() for col in model_stats.columns]\n",
    "        model_stats = model_stats.reset_index()\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        metrics = ['precision_mean', 'recall_mean', 'f1_score_mean']\n",
    "        metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for i, (metric, name, color) in enumerate(zip(metrics, metric_names, colors)):\n",
    "            error_metric = metric.replace('_mean', '_std')\n",
    "            \n",
    "            fig.add_trace(go.Bar(\n",
    "                name=name,\n",
    "                x=model_stats['model'],\n",
    "                y=model_stats[metric],\n",
    "                error_y=dict(type='data', array=model_stats[error_metric], visible=True),\n",
    "                marker_color=color,\n",
    "                text=model_stats[metric].round(3),\n",
    "                textposition='auto'\n",
    "            ))\n",
    "        \n",
    "        title_text = f\"Model Performance Comparison ({df['run'].iloc[0]})\"\n",
    "        \n",
    "    else:\n",
    "        # Multiple runs - show each run side by side\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        runs = sorted(df['run'].unique())\n",
    "        metrics = ['precision', 'recall', 'f1_score']\n",
    "        metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        models = sorted(df['model'].unique())\n",
    "        \n",
    "        # Create data for each metric and run combination\n",
    "        for metric_idx, (metric, metric_name, color) in enumerate(zip(metrics, metric_names, colors)):\n",
    "            for run_idx, run in enumerate(runs):\n",
    "                run_data = df[df['run'] == run]\n",
    "                model_means = run_data.groupby('model')[metric].mean().reindex(models, fill_value=0)\n",
    "                \n",
    "                # Create x positions for grouped bars\n",
    "                x_positions = [f\"{model}_{run}\" for model in models]\n",
    "                \n",
    "                fig.add_trace(go.Bar(\n",
    "                    name=f\"{metric_name} ({run})\",\n",
    "                    x=x_positions,\n",
    "                    y=model_means.values,\n",
    "                    marker_color=color,\n",
    "                    opacity=0.7 + (run_idx * 0.3 / len(runs)),  # Vary opacity by run\n",
    "                    text=model_means.round(3),\n",
    "                    textposition='auto',\n",
    "                    offsetgroup=metric_idx,\n",
    "                    legendgroup=metric_name,\n",
    "                    showlegend=(run_idx == 0)  # Only show legend for first run of each metric\n",
    "                ))\n",
    "        \n",
    "        title_text = f\"Model Performance Comparison Across {len(runs)} Runs\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(text=title_text, font=dict(size=18, color='#2E2E2E'), x=0.5),\n",
    "        xaxis=dict(title=\"Models\" if num_runs == 1 else \"Models by Run\", tickangle=45),\n",
    "        yaxis=dict(title=\"Score\", range=[0, 1]),\n",
    "        barmode='group',\n",
    "        template='plotly_white',\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_document_difficulty_chart(df):\n",
    "    \"\"\"Create a chart showing which documents are hardest/easiest for models\"\"\"\n",
    "    \n",
    "    # Calculate average F1 score per document across all models\n",
    "    doc_stats = df.groupby('document').agg({\n",
    "        'f1_score': ['mean', 'std', 'min', 'max'],\n",
    "        'model': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    doc_stats.columns = ['_'.join(col).strip() for col in doc_stats.columns]\n",
    "    doc_stats = doc_stats.reset_index()\n",
    "    doc_stats = doc_stats.sort_values('f1_score_mean', ascending=True)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar chart with error bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=doc_stats['f1_score_mean'],\n",
    "        y=doc_stats['document'],\n",
    "        orientation='h',\n",
    "        error_x=dict(\n",
    "            type='data',\n",
    "            array=doc_stats['f1_score_std'],\n",
    "            visible=True\n",
    "        ),\n",
    "        marker=dict(\n",
    "            color=doc_stats['f1_score_mean'],\n",
    "            colorscale='RdYlGn',\n",
    "            cmin=0,\n",
    "            cmax=1,\n",
    "            colorbar=dict(title=\"Average F1-Score\")\n",
    "        ),\n",
    "        text=doc_stats['f1_score_mean'].round(3),\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"Document Difficulty Ranking (by Average F1-Score Across Models)\",\n",
    "            font=dict(size=16, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Average F1-Score\",\n",
    "            range=[0, 1]\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Documents\"\n",
    "        ),\n",
    "        height=max(400, len(doc_stats) * 25),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_run_comparison_dashboard(df):\n",
    "    \"\"\"Create a dashboard comparing different runs\"\"\"\n",
    "    \n",
    "    if len(df['run'].unique()) < 2:\n",
    "        print(\"Need at least 2 runs for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'F1-Score by Run', \n",
    "            'Precision by Run',\n",
    "            'Recall by Run', \n",
    "            'Model Performance Variation'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    runs = df['run'].unique()\n",
    "    colors = px.colors.qualitative.Set1[:len(runs)]\n",
    "    \n",
    "    # Calculate stats by run\n",
    "    run_stats = df.groupby('run').agg({\n",
    "        'f1_score': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    run_stats.columns = ['_'.join(col).strip() for col in run_stats.columns]\n",
    "    run_stats = run_stats.reset_index()\n",
    "    \n",
    "    # Add traces for each metric\n",
    "    metrics = [\n",
    "        ('f1_score_mean', 'f1_score_std', (1, 1)),\n",
    "        ('precision_mean', 'precision_std', (1, 2)),\n",
    "        ('recall_mean', 'recall_std', (2, 1))\n",
    "    ]\n",
    "    \n",
    "    for metric_mean, metric_std, (row, col) in metrics:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                name=metric_mean.split('_')[0].title(),\n",
    "                x=run_stats['run'],\n",
    "                y=run_stats[metric_mean],\n",
    "                error_y=dict(\n",
    "                    type='data',\n",
    "                    array=run_stats[metric_std],\n",
    "                    visible=True\n",
    "                ),\n",
    "                text=run_stats[metric_mean].round(3),\n",
    "                textposition='auto',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    # Model performance variation (coefficient of variation)\n",
    "    model_variation = df.groupby(['run', 'model'])['f1_score'].mean().groupby('run').std()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name='F1 Std Dev',\n",
    "            x=model_variation.index,\n",
    "            y=model_variation.values,\n",
    "            text=model_variation.values.round(3),\n",
    "            textposition='auto',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"Run Comparison Dashboard\",\n",
    "            font=dict(size=20, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        height=800,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Check if evaluation_data exists and process it\n",
    "if 'evaluation_data' in globals() and evaluation_data:\n",
    "    df = process_evaluation_data(evaluation_data)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"✅ Data processed successfully!\")\n",
    "        print(f\"📊 Data shape: {df.shape}\")\n",
    "        print(f\"🏃 Runs: {list(df['run'].unique())}\")\n",
    "        print(f\"🤖 Models: {list(df['model'].unique())}\")\n",
    "        print(f\"📄 Documents: {len(df['document'].unique())} documents\")\n",
    "        print(f\"📈 Average F1-Score: {df['f1_score'].mean():.3f}\")\n",
    "    else:\n",
    "        print(\"❌ No data to process. Please load evaluation data first.\")\n",
    "else:\n",
    "    print(\"❌ No evaluation data loaded. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fbafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about loaded runs and data aggregation\n",
    "if 'evaluation_data' in globals() and evaluation_data:\n",
    "    print(\"📋 LOADED RUNS INFORMATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of runs loaded: {len(evaluation_data)}\")\n",
    "    print(\"Runs included in analysis:\")\n",
    "    for i, run_name in enumerate(evaluation_data.keys(), 1):\n",
    "        print(f\"  {i}. {run_name}\")\n",
    "    \n",
    "    print(\"\\n🔄 DATA PRESENTATION METHOD:\")\n",
    "    if len(evaluation_data) == 1:\n",
    "        print(\"- Single run: Results shown directly without aggregation\")\n",
    "    else:\n",
    "        print(\"- Multiple runs: Results shown SEPARATELY for each run\")\n",
    "        print(\"- Each run is displayed in its own section/subplot\")\n",
    "        print(\"- NO AVERAGING - you can see the evolution across runs!\")\n",
    "    print(\"- This allows you to compare how performance changes with different settings\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(\"❌ No evaluation data loaded yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a41e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in globals() and not df.empty:\n",
    "    print(\"🎯 Creating Performance Visualizations...\")\n",
    "    \n",
    "    # 1. F1-Score Heatmap - Shows model performance across documents at a glance\n",
    "    print(\"📊 1. Model Performance Heatmap (F1-Score)\")\n",
    "    f1_heatmap = create_performance_heatmap(df, 'f1_score', 'F1-Score')\n",
    "    f1_heatmap.show()\n",
    "    \n",
    "    # 2. Model Comparison Chart - Shows average performance with error bars\n",
    "    print(\"📈 2. Model Performance Comparison\")\n",
    "    model_comparison = create_model_comparison_chart(df)\n",
    "    model_comparison.show()\n",
    "    \n",
    "    # 3. Document Difficulty Ranking\n",
    "    print(\"📋 3. Document Difficulty Ranking\")\n",
    "    doc_difficulty = create_document_difficulty_chart(df)\n",
    "    doc_difficulty.show()\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Please load and process evaluation data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_heatmap(df, metric, title_suffix):\n",
    "    \"\"\"Create a heatmap showing model performance across documents for each run separately\"\"\"\n",
    "    \n",
    "    # Check if we have multiple runs\n",
    "    num_runs = len(df['run'].unique())\n",
    "    \n",
    "    if num_runs == 1:\n",
    "        # Single run - create simple heatmap\n",
    "        pivot_data = df.pivot_table(\n",
    "            index='model', \n",
    "            columns='document', \n",
    "            values=metric, \n",
    "            aggfunc='first'  # Take the single value\n",
    "        ).fillna(0)\n",
    "        \n",
    "        title_text = f\"Model Performance Heatmap - {title_suffix} ({df['run'].iloc[0]})\"\n",
    "        \n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=pivot_data.values,\n",
    "            x=pivot_data.columns,\n",
    "            y=pivot_data.index,\n",
    "            colorscale='RdYlGn',\n",
    "            zmin=0,\n",
    "            zmax=1,\n",
    "            text=pivot_data.values.round(3),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 10},\n",
    "            colorbar=dict(title=title_suffix)\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(text=title_text, font=dict(size=20, color='#2E2E2E'), x=0.5),\n",
    "            xaxis=dict(title=\"Documents\", tickangle=45, tickfont=dict(size=10)),\n",
    "            yaxis=dict(title=\"Models\", tickfont=dict(size=12)),\n",
    "            height=max(400, len(pivot_data.index) * 50),\n",
    "            width=max(800, len(pivot_data.columns) * 80),\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    else:\n",
    "        # Multiple runs - create subplots for each run\n",
    "        runs = sorted(df['run'].unique())\n",
    "        cols = min(3, len(runs))  # Max 3 columns\n",
    "        rows = (len(runs) + cols - 1) // cols  # Calculate rows needed\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=rows, cols=cols,\n",
    "            subplot_titles=runs,\n",
    "            shared_xaxes=True,\n",
    "            shared_yaxes=True,\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        for i, run in enumerate(runs):\n",
    "            row = i // cols + 1\n",
    "            col = i % cols + 1\n",
    "            \n",
    "            run_data = df[df['run'] == run]\n",
    "            pivot_data = run_data.pivot_table(\n",
    "                index='model', \n",
    "                columns='document', \n",
    "                values=metric, \n",
    "                aggfunc='first'\n",
    "            ).fillna(0)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=pivot_data.values,\n",
    "                    x=pivot_data.columns,\n",
    "                    y=pivot_data.index,\n",
    "                    colorscale='RdYlGn',\n",
    "                    zmin=0,\n",
    "                    zmax=1,\n",
    "                    text=pivot_data.values.round(3),\n",
    "                    texttemplate=\"%{text}\",\n",
    "                    textfont={\"size\": 8},\n",
    "                    showscale=(i == 0),  # Only show colorbar for first subplot\n",
    "                    colorbar=dict(title=title_suffix) if i == 0 else None\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        \n",
    "        title_text = f\"Model Performance Comparison Across Runs - {title_suffix}\"\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(\n",
    "                text=title_text,\n",
    "                font=dict(size=18, color='#2E2E2E'),\n",
    "                x=0.5\n",
    "            ),\n",
    "            height=max(400, rows * 300),\n",
    "            width=max(1200, cols * 400),\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in globals() and not df.empty:\n",
    "    \n",
    "    # 4. Run Comparison (if multiple runs selected)\n",
    "    if len(df['run'].unique()) > 1:\n",
    "        print(\"🔄 4. Run Comparison Dashboard\")\n",
    "        run_comparison = create_run_comparison_dashboard(df)\n",
    "        if run_comparison:\n",
    "            run_comparison.show()\n",
    "    \n",
    "    # 5. Additional Heatmaps for Precision and Recall\n",
    "    print(\"📊 5. Additional Performance Heatmaps\")\n",
    "    \n",
    "    # Precision heatmap\n",
    "    precision_heatmap = create_performance_heatmap(df, 'precision', 'Precision')\n",
    "    precision_heatmap.show()\n",
    "    \n",
    "    # Recall heatmap  \n",
    "    recall_heatmap = create_performance_heatmap(df, 'recall', 'Recall')\n",
    "    recall_heatmap.show()\n",
    "    \n",
    "    # 6. Summary Statistics Table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best model per document\n",
    "    print(\"\\n🏆 BEST MODEL PER DOCUMENT (by F1-Score):\")\n",
    "    best_per_doc = df.loc[df.groupby('document')['f1_score'].idxmax()][['document', 'model', 'f1_score', 'precision', 'recall']]\n",
    "    best_per_doc = best_per_doc.sort_values('f1_score', ascending=False)\n",
    "    display(best_per_doc)\n",
    "    \n",
    "    # Overall model ranking\n",
    "    print(\"\\n🥇 OVERALL MODEL RANKING:\")\n",
    "    model_ranking = df.groupby('model').agg({\n",
    "        'f1_score': ['mean', 'std', 'min', 'max'],\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean',\n",
    "        'document': 'count'\n",
    "    }).round(4)\n",
    "    model_ranking.columns = ['F1_Mean', 'F1_Std', 'F1_Min', 'F1_Max', 'Precision_Mean', 'Recall_Mean', 'Documents']\n",
    "    model_ranking = model_ranking.sort_values('F1_Mean', ascending=False)\n",
    "    display(model_ranking)\n",
    "    \n",
    "    # Document difficulty ranking\n",
    "    print(\"\\n📋 DOCUMENT DIFFICULTY RANKING (Hardest First):\")\n",
    "    doc_ranking = df.groupby('document').agg({\n",
    "        'f1_score': ['mean', 'std', 'min', 'max'],\n",
    "        'model': 'count'\n",
    "    }).round(4)\n",
    "    doc_ranking.columns = ['F1_Mean', 'F1_Std', 'F1_Min', 'F1_Max', 'Models_Tested']\n",
    "    doc_ranking = doc_ranking.sort_values('F1_Mean', ascending=True)\n",
    "    display(doc_ranking)\n",
    "    \n",
    "    if len(df['run'].unique()) > 1:\n",
    "        print(\"\\n🔄 RUN COMPARISON:\")\n",
    "        run_summary = df.groupby('run').agg({\n",
    "            'f1_score': ['mean', 'std'],\n",
    "            'precision': 'mean',\n",
    "            'recall': 'mean',\n",
    "            'model': 'nunique',\n",
    "            'document': 'nunique'\n",
    "        }).round(4)\n",
    "        run_summary.columns = ['F1_Mean', 'F1_Std', 'Precision_Mean', 'Recall_Mean', 'Models', 'Documents']\n",
    "        run_summary = run_summary.sort_values('F1_Mean', ascending=False)\n",
    "        display(run_summary)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Please load and process evaluation data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d83c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_per_document_charts(df):\n",
    "    \"\"\"Create individual charts for each document showing all models with all metrics\"\"\"\n",
    "    \n",
    "    documents = df['document'].unique()\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_data = df[df['document'] == doc].copy()\n",
    "        \n",
    "        if doc_data.empty:\n",
    "            continue\n",
    "            \n",
    "        # Sort models by F1-score for better visualization\n",
    "        doc_data = doc_data.sort_values('f1_score', ascending=True)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add bars for each metric\n",
    "        metrics = ['precision', 'recall', 'f1_score']\n",
    "        metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for i, (metric, name, color) in enumerate(zip(metrics, metric_names, colors)):\n",
    "            fig.add_trace(go.Bar(\n",
    "                name=name,\n",
    "                x=doc_data[metric],\n",
    "                y=doc_data['model'],\n",
    "                orientation='h',\n",
    "                marker_color=color,\n",
    "                text=doc_data[metric].round(3),\n",
    "                textposition='auto',\n",
    "                offsetgroup=i,\n",
    "                width=0.25  # Make bars thinner so they don't overlap\n",
    "            ))\n",
    "        \n",
    "        # Find the best model for this document\n",
    "        best_model = doc_data.loc[doc_data['f1_score'].idxmax()]\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(\n",
    "                text=f\"Model Performance on Document: {doc}<br><span style='font-size:14px'>Best Model: {best_model['model']} (F1: {best_model['f1_score']:.3f})</span>\",\n",
    "                font=dict(size=16, color='#2E2E2E'),\n",
    "                x=0.5\n",
    "            ),\n",
    "            xaxis=dict(\n",
    "                title=\"Score\",\n",
    "                range=[0, 1],\n",
    "                tickfont=dict(size=12)\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title=\"Models\",\n",
    "                tickfont=dict(size=12)\n",
    "            ),\n",
    "            barmode='group',\n",
    "            template='plotly_white',\n",
    "            height=max(300, len(doc_data) * 40),\n",
    "            width=800,\n",
    "            showlegend=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "if 'df' in globals() and not df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📄 DETAILED PERFORMANCE BY DOCUMENT\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Individual charts for each document showing all models with precision, recall, and F1-score\")\n",
    "    print()\n",
    "    \n",
    "    create_per_document_charts(df)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Please load and process evaluation data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_overview(df):\n",
    "    \"\"\"Create a comprehensive overview chart with all models and documents for each run\"\"\"\n",
    "    \n",
    "    runs = sorted(df['run'].unique())\n",
    "    num_runs = len(runs)\n",
    "    \n",
    "    if num_runs == 1:\n",
    "        # Single run - create 3-panel view (Precision, Recall, F1-Score)\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=3,\n",
    "            subplot_titles=('Precision', 'Recall', 'F1-Score'),\n",
    "            shared_yaxes=True,\n",
    "            horizontal_spacing=0.08\n",
    "        )\n",
    "        \n",
    "        models = sorted(df['model'].unique())\n",
    "        documents = sorted(df['document'].unique())\n",
    "        colors = px.colors.qualitative.Set1[:len(models)]\n",
    "        if len(models) > len(colors):\n",
    "            colors = colors * (len(models) // len(colors) + 1)\n",
    "        \n",
    "        metrics = ['precision', 'recall', 'f1_score']\n",
    "        \n",
    "        for col_idx, metric in enumerate(metrics, 1):\n",
    "            pivot_data = df.pivot_table(\n",
    "                index='document', \n",
    "                columns='model', \n",
    "                values=metric, \n",
    "                aggfunc='first'\n",
    "            ).fillna(0)\n",
    "            \n",
    "            for model_idx, model in enumerate(models):\n",
    "                if model in pivot_data.columns:\n",
    "                    values = pivot_data[model].values\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            name=model if col_idx == 1 else None,\n",
    "                            x=values,\n",
    "                            y=documents,\n",
    "                            orientation='h',\n",
    "                            marker_color=colors[model_idx],\n",
    "                            text=[f\"{val:.3f}\" if val > 0 else \"\" for val in values],\n",
    "                            textposition='auto',\n",
    "                            textfont=dict(size=8),\n",
    "                            showlegend=(col_idx == 1),\n",
    "                            offsetgroup=model_idx,\n",
    "                            width=0.8/len(models)\n",
    "                        ),\n",
    "                        row=1, col=col_idx\n",
    "                    )\n",
    "        \n",
    "        title_text = f\"📊 Complete Performance Overview - {runs[0]}\"\n",
    "        height = max(600, len(documents) * 25)\n",
    "        width = 1400\n",
    "        \n",
    "    else:\n",
    "        # Multiple runs - create grid with runs as rows and metrics as columns\n",
    "        fig = make_subplots(\n",
    "            rows=num_runs, cols=3,\n",
    "            subplot_titles=[f\"{run} - {metric}\" for run in runs for metric in ['Precision', 'Recall', 'F1-Score']],\n",
    "            shared_yaxes=True,\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.08\n",
    "        )\n",
    "        \n",
    "        models = sorted(df['model'].unique())\n",
    "        documents = sorted(df['document'].unique())\n",
    "        colors = px.colors.qualitative.Set1[:len(models)]\n",
    "        if len(models) > len(colors):\n",
    "            colors = colors * (len(models) // len(colors) + 1)\n",
    "        \n",
    "        metrics = ['precision', 'recall', 'f1_score']\n",
    "        \n",
    "        for run_idx, run in enumerate(runs, 1):\n",
    "            run_data = df[df['run'] == run]\n",
    "            \n",
    "            for col_idx, metric in enumerate(metrics, 1):\n",
    "                pivot_data = run_data.pivot_table(\n",
    "                    index='document', \n",
    "                    columns='model', \n",
    "                    values=metric, \n",
    "                    aggfunc='first'\n",
    "                ).fillna(0)\n",
    "                \n",
    "                for model_idx, model in enumerate(models):\n",
    "                    if model in pivot_data.columns:\n",
    "                        values = pivot_data[model].values\n",
    "                        show_legend = (run_idx == 1 and col_idx == 1)  # Only show legend on first subplot\n",
    "                        \n",
    "                        fig.add_trace(\n",
    "                            go.Bar(\n",
    "                                name=model if show_legend else None,\n",
    "                                x=values,\n",
    "                                y=documents,\n",
    "                                orientation='h',\n",
    "                                marker_color=colors[model_idx],\n",
    "                                text=[f\"{val:.3f}\" if val > 0 else \"\" for val in values],\n",
    "                                textposition='auto',\n",
    "                                textfont=dict(size=6),\n",
    "                                showlegend=show_legend,\n",
    "                                offsetgroup=model_idx,\n",
    "                                width=0.8/len(models),\n",
    "                                legendgroup=model  # Group legend items by model\n",
    "                            ),\n",
    "                            row=run_idx, col=col_idx\n",
    "                        )\n",
    "        \n",
    "        title_text = f\"📊 Performance Comparison Across {num_runs} Runs\"\n",
    "        height = max(800, num_runs * 300)\n",
    "        width = 1400\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title_text,\n",
    "            font=dict(size=20, color='#2E2E2E'),\n",
    "            x=0.5\n",
    "        ),\n",
    "        height=height,\n",
    "        width=width,\n",
    "        template='plotly_white',\n",
    "        barmode='group',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.15,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            font=dict(size=10)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update x-axes for all subplots\n",
    "    for row in range(1, (num_runs if num_runs > 1 else 1) + 1):\n",
    "        for col in range(1, 4):\n",
    "            fig.update_xaxes(\n",
    "                title=\"Score\",\n",
    "                range=[0, 1],\n",
    "                tickfont=dict(size=10),\n",
    "                row=row, col=col\n",
    "            )\n",
    "    \n",
    "    # Update y-axis (only for first column since shared)\n",
    "    for row in range(1, (num_runs if num_runs > 1 else 1) + 1):\n",
    "        fig.update_yaxes(\n",
    "            title=\"Documents\" if row == 1 else \"\",\n",
    "            tickfont=dict(size=10),\n",
    "            row=row, col=1\n",
    "        )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Add comprehensive annotation type analysis\n",
    "if 'df' in globals() and not df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎯 ANNOTATION TYPE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Detailed analysis of performance by annotation type (Event, Event_who, Event_when, Event_what)\")\n",
    "    print()\n",
    "    \n",
    "    # 1. Annotation Type Performance Heatmaps\n",
    "    print(\"📊 1. Performance Heatmaps by Annotation Type\")\n",
    "    \n",
    "    # Lenient evaluation heatmap\n",
    "    print(\"   📈 Lenient Evaluation (50% overlap threshold)\")\n",
    "    f1_ann_heatmap_lenient = create_annotation_type_heatmap(df, 'lenient', 'f1_score')\n",
    "    f1_ann_heatmap_lenient.show()\n",
    "    \n",
    "    precision_ann_heatmap_lenient = create_annotation_type_heatmap(df, 'lenient', 'precision')\n",
    "    precision_ann_heatmap_lenient.show()\n",
    "    \n",
    "    recall_ann_heatmap_lenient = create_annotation_type_heatmap(df, 'lenient', 'recall')\n",
    "    recall_ann_heatmap_lenient.show()\n",
    "    \n",
    "    # Strict evaluation heatmap\n",
    "    print(\"\\n   🎯 Strict Evaluation (90% overlap threshold)\")\n",
    "    f1_ann_heatmap_strict = create_annotation_type_heatmap(df, 'strict', 'f1_score')\n",
    "    f1_ann_heatmap_strict.show()\n",
    "    \n",
    "    # 2. Annotation Type Difficulty Analysis\n",
    "    print(\"\\n📋 2. Annotation Type Difficulty Ranking\")\n",
    "    \n",
    "    ann_difficulty_lenient = create_annotation_type_difficulty_chart(df, 'lenient')\n",
    "    ann_difficulty_lenient.show()\n",
    "    \n",
    "    ann_difficulty_strict = create_annotation_type_difficulty_chart(df, 'strict')\n",
    "    ann_difficulty_strict.show()\n",
    "    \n",
    "    # 3. Model Comparison by Annotation Type\n",
    "    print(\"\\n🤖 3. Model Performance by Annotation Type\")\n",
    "    \n",
    "    model_ann_comparison_lenient = create_model_comparison_by_annotation_type(df, 'lenient')\n",
    "    model_ann_comparison_lenient.show()\n",
    "    \n",
    "    model_ann_comparison_strict = create_model_comparison_by_annotation_type(df, 'strict')\n",
    "    model_ann_comparison_strict.show()\n",
    "    \n",
    "    # 4. Summary Statistics by Annotation Type\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 ANNOTATION TYPE SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best model per annotation type (lenient)\n",
    "    print(\"\\n🏆 BEST MODEL PER ANNOTATION TYPE (Lenient Evaluation):\")\n",
    "    df_lenient = df[(df['evaluation_mode'] == 'lenient') & (df['annotation_type'] != 'Overall')]\n",
    "    if not df_lenient.empty:\n",
    "        best_per_ann_type = df_lenient.loc[df_lenient.groupby('annotation_type')['f1_score'].idxmax()][['annotation_type', 'model', 'f1_score', 'precision', 'recall']]\n",
    "        best_per_ann_type = best_per_ann_type.sort_values('f1_score', ascending=False)\n",
    "        display(best_per_ann_type)\n",
    "    \n",
    "    # Annotation type ranking (lenient)\n",
    "    print(\"\\n📋 ANNOTATION TYPE DIFFICULTY RANKING (Lenient - Hardest First):\")\n",
    "    if not df_lenient.empty:\n",
    "        ann_ranking = df_lenient.groupby('annotation_type').agg({\n",
    "            'f1_score': ['mean', 'std', 'min', 'max'],\n",
    "            'precision': 'mean',\n",
    "            'recall': 'mean',\n",
    "            'model': 'nunique'\n",
    "        }).round(4)\n",
    "        ann_ranking.columns = ['F1_Mean', 'F1_Std', 'F1_Min', 'F1_Max', 'Precision_Mean', 'Recall_Mean', 'Models_Tested']\n",
    "        ann_ranking = ann_ranking.sort_values('F1_Mean', ascending=True)\n",
    "        display(ann_ranking)\n",
    "    \n",
    "    # Best model per annotation type (strict)\n",
    "    print(\"\\n🎯 BEST MODEL PER ANNOTATION TYPE (Strict Evaluation):\")\n",
    "    df_strict = df[(df['evaluation_mode'] == 'strict') & (df['annotation_type'] != 'Overall')]\n",
    "    if not df_strict.empty:\n",
    "        best_per_ann_type_strict = df_strict.loc[df_strict.groupby('annotation_type')['f1_score'].idxmax()][['annotation_type', 'model', 'f1_score', 'precision', 'recall']]\n",
    "        best_per_ann_type_strict = best_per_ann_type_strict.sort_values('f1_score', ascending=False)\n",
    "        display(best_per_ann_type_strict)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Please load and process evaluation data first.\")\n",
    "\n",
    "if 'df' in globals() and not df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎯 COMPREHENSIVE OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show which runs are included\n",
    "    unique_runs = df['run'].unique()\n",
    "    if len(unique_runs) == 1:\n",
    "        print(f\"📊 Displaying results for run: {unique_runs[0]}\")\n",
    "        print(\"Single run analysis with detailed metrics breakdown\")\n",
    "    else:\n",
    "        print(f\"📊 Comparing {len(unique_runs)} runs side by side:\")\n",
    "        for i, run in enumerate(unique_runs, 1):\n",
    "            print(f\"    {i}. {run}\")\n",
    "        print(\"🔍 Each run shown separately - NO AVERAGING!\")\n",
    "        print(\"📈 Perfect for seeing how performance evolves across different settings\")\n",
    "    \n",
    "    print(\"Complete view of all models across all documents for all metrics\")\n",
    "    print()\n",
    "    \n",
    "    overview_fig = create_comprehensive_overview(df)\n",
    "    overview_fig.show()\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Please load and process evaluation data first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
