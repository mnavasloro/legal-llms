{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb8033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Results Folder Selector\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "def find_pipeline_results_folders(base_path=\"output\"):\n",
    "    \"\"\"Find all pipeline_results folders in the output directory\"\"\"\n",
    "    pipeline_folders = []\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"Output directory '{base_path}' not found!\")\n",
    "        return []\n",
    "    \n",
    "    for item in base_path.iterdir():\n",
    "        if item.is_dir() and item.name.startswith(\"pipeline_results_\"):\n",
    "            # Check for evaluation file\n",
    "            eval_json_path = item / \"llm_evaluation_results.json\"\n",
    "            \n",
    "            if eval_json_path.exists():\n",
    "                pipeline_folders.append({\n",
    "                    'name': item.name,\n",
    "                    'path': str(item),\n",
    "                    'eval_path': str(eval_json_path)\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: No evaluation file found in {item.name}\")\n",
    "    \n",
    "    return sorted(pipeline_folders, key=lambda x: x['name'], reverse=True)  # Most recent first\n",
    "\n",
    "# Find available pipeline results\n",
    "available_folders = find_pipeline_results_folders()\n",
    "\n",
    "if not available_folders:\n",
    "    print(\"‚ùå No pipeline_results folders found in the output directory!\")\n",
    "    print(\"Please run the evaluation pipeline first.\")\n",
    "    selected_pipeline_folder = None\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(available_folders)} pipeline_results folders:\")\n",
    "    for i, folder in enumerate(available_folders[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {folder['name']}\")\n",
    "    \n",
    "    # Create selection widget\n",
    "    folder_options = [(folder['name'], folder) for folder in available_folders]\n",
    "    \n",
    "    # Pre-select the most recent folder\n",
    "    folder_selector = widgets.Dropdown(\n",
    "        options=folder_options,\n",
    "        value=available_folders[0] if available_folders else None,\n",
    "        description='Pipeline Run:',\n",
    "        style={'description_width': '120px'},\n",
    "        layout=widgets.Layout(width='600px')\n",
    "    )\n",
    "    \n",
    "    # Set button\n",
    "    set_button = widgets.Button(\n",
    "        description='üìÅ Set Pipeline Folder',\n",
    "        button_style='success',\n",
    "        icon='check',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def set_pipeline_folder(button):\n",
    "        global selected_pipeline_folder, results_path\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            selected_folder = folder_selector.value\n",
    "            \n",
    "            if selected_folder:\n",
    "                selected_pipeline_folder = selected_folder['path']\n",
    "                results_path = selected_folder['eval_path']\n",
    "                \n",
    "                print(f\"‚úÖ Selected pipeline folder: {selected_folder['name']}\")\n",
    "                print(f\"üìä Results file: {results_path}\")\n",
    "                print(f\"üìÅ Full path: {selected_pipeline_folder}\")\n",
    "                print(\"\\nüöÄ You can now run the visualization cells below!\")\n",
    "            else:\n",
    "                print(\"‚ùå No folder selected!\")\n",
    "    \n",
    "    set_button.on_click(set_pipeline_folder)\n",
    "    \n",
    "    print(f\"\\nüìã Select a pipeline results folder:\")\n",
    "    display(widgets.HBox([folder_selector, set_button]))\n",
    "    display(output_area)\n",
    "    \n",
    "    # Set default\n",
    "    if available_folders:\n",
    "        selected_pipeline_folder = available_folders[0]['path']\n",
    "        results_path = available_folders[0]['eval_path']\n",
    "        print(f\"\\nüí° Default: Using most recent run - {available_folders[0]['name']}\")\n",
    "    else:\n",
    "        selected_pipeline_folder = None\n",
    "        results_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13398642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_results(results_path):\n",
    "    \"\"\"Load evaluation results from JSON file.\"\"\"\n",
    "    with open(results_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_dataframe_from_results(results):\n",
    "    \"\"\"Convert nested results to a flat DataFrame for analysis.\"\"\"\n",
    "    rows = []\n",
    "    for doc_name, doc_results in results.items():\n",
    "        for model_name, model_results in doc_results.items():\n",
    "            for ann_type, metrics in model_results.items():\n",
    "                # Lenient evaluation\n",
    "                rows.append({\n",
    "                    'Document': doc_name,\n",
    "                    'Model': model_name,\n",
    "                    'Annotation_Type': ann_type,\n",
    "                    'Evaluation_Mode': 'Lenient',\n",
    "                    'Precision': metrics['lenient']['precision'],\n",
    "                    'Recall': metrics['lenient']['recall'],\n",
    "                    'F1_Score': metrics['lenient']['f1_score'],\n",
    "                    'True_Positives': metrics['lenient']['true_positives'],\n",
    "                    'False_Positives': metrics['lenient']['false_positives'],\n",
    "                    'False_Negatives': metrics['lenient']['false_negatives'],\n",
    "                    'Gold_Count': metrics['gold_count'],\n",
    "                    'Predicted_Count': metrics['predicted_count']\n",
    "                })\n",
    "                \n",
    "                # Strict evaluation\n",
    "                rows.append({\n",
    "                    'Document': doc_name,\n",
    "                    'Model': model_name,\n",
    "                    'Annotation_Type': ann_type,\n",
    "                    'Evaluation_Mode': 'Strict',\n",
    "                    'Precision': metrics['strict']['precision'],\n",
    "                    'Recall': metrics['strict']['recall'],\n",
    "                    'F1_Score': metrics['strict']['f1_score'],\n",
    "                    'True_Positives': metrics['strict']['true_positives'],\n",
    "                    'False_Positives': metrics['strict']['false_positives'],\n",
    "                    'False_Negatives': metrics['strict']['false_negatives'],\n",
    "                    'Gold_Count': metrics['gold_count'],\n",
    "                    'Predicted_Count': metrics['predicted_count']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def visualize_single_run(results_path):\n",
    "    \"\"\"Create comprehensive visualizations for a single evaluation run.\"\"\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    results = load_evaluation_results(results_path)\n",
    "    df = create_dataframe_from_results(results)\n",
    "    \n",
    "    # Create an enhanced figure with more subplots for better analysis\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=(\n",
    "            'F1-Scores by Model and Annotation Type (Lenient)', \n",
    "            'Document-Level Performance Heatmap',\n",
    "            'Precision vs Recall by Model (with Document Points)',\n",
    "            'Model Performance Across Documents',\n",
    "            'Lenient vs Strict Evaluation Comparison',\n",
    "            'Individual Document Analysis',\n",
    "            'Annotation Type Performance Distribution',\n",
    "            'Model Consistency Across Documents'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"heatmap\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]],\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.12\n",
    "    )\n",
    "    \n",
    "    # Dynamic color palette for all available models\n",
    "    color_palette = [\n",
    "        '#E74C3C',   # Red\n",
    "        '#3498DB',   # Blue\n",
    "        '#2ECC71',   # Green\n",
    "        '#F39C12',   # Orange\n",
    "        '#9B59B6',   # Purple\n",
    "        '#1ABC9C',   # Teal\n",
    "        '#F1C40F',   # Yellow\n",
    "        '#E67E22',   # Dark Orange\n",
    "        '#95A5A6',   # Gray\n",
    "        '#34495E',   # Dark Blue Gray\n",
    "        '#16A085',   # Dark Teal\n",
    "        '#8E44AD',   # Dark Purple\n",
    "        '#C0392B',   # Dark Red\n",
    "        '#2980B9',   # Dark Blue\n",
    "        '#27AE60',   # Dark Green\n",
    "        '#D35400'    # Dark Orange Red\n",
    "    ]\n",
    "    \n",
    "    # Get unique models and assign colors dynamically\n",
    "    unique_models = df['Model'].unique()\n",
    "    model_colors = {}\n",
    "    for i, model in enumerate(unique_models):\n",
    "        model_colors[model] = color_palette[i % len(color_palette)]\n",
    "    \n",
    "    # Document colors for variety\n",
    "    doc_colors = px.colors.qualitative.Set3\n",
    "    \n",
    "    # Get lenient and strict data\n",
    "    df_lenient = df[df['Evaluation_Mode'] == 'Lenient']\n",
    "    df_strict = df[df['Evaluation_Mode'] == 'Strict']\n",
    "    \n",
    "    # 1. F1-Scores by Model and Annotation Type (Lenient) - Enhanced with proper legend\n",
    "    for i, model in enumerate(df_lenient['Model'].unique()):\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        avg_f1_by_type = model_data.groupby('Annotation_Type')['F1_Score'].mean().reset_index()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=avg_f1_by_type['Annotation_Type'],\n",
    "                y=avg_f1_by_type['F1_Score'],\n",
    "                name=f\"{model}\",\n",
    "                marker_color=model_colors.get(model, '#95A5A6'),\n",
    "                showlegend=True,\n",
    "                legendgroup='models',\n",
    "                hovertemplate=f'<b>{model}</b><br>Type: %{{x}}<br>Avg F1: %{{y:.3f}}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Document-Level Performance Heatmap\n",
    "    doc_model_performance = df_lenient.groupby(['Document', 'Model'])['F1_Score'].mean().reset_index()\n",
    "    heatmap_pivot = doc_model_performance.pivot(index='Document', columns='Model', values='F1_Score')\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=heatmap_pivot.values,\n",
    "            x=heatmap_pivot.columns,\n",
    "            y=heatmap_pivot.index,\n",
    "            colorscale='RdYlBu_r',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"F1 Score\", x=0.48),\n",
    "            hovertemplate='Document: %{y}<br>Model: %{x}<br>F1 Score: %{z:.3f}<extra></extra>',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Precision vs Recall scatter plot with document points\n",
    "    for i, model in enumerate(df_lenient['Model'].unique()):\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_data['Recall'],\n",
    "                y=model_data['Precision'],\n",
    "                mode='markers',\n",
    "                name=f\"{model} (docs)\",\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=model_colors.get(model, '#95A5A6'),\n",
    "                    symbol='circle',\n",
    "                    opacity=0.7\n",
    "                ),\n",
    "                text=model_data['Document'] + '<br>' + model_data['Annotation_Type'],\n",
    "                hovertemplate='<b>%{text}</b><br>Recall: %{x:.3f}<br>Precision: %{y:.3f}<extra></extra>',\n",
    "                showlegend=True,\n",
    "                legendgroup='scatter'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Model Performance Across Documents (Box plot style)\n",
    "    for i, model in enumerate(df_lenient['Model'].unique()):\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        doc_performance = model_data.groupby('Document')['F1_Score'].mean().reset_index()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=doc_performance['Document'],\n",
    "                y=doc_performance['F1_Score'],\n",
    "                name=f\"{model}\",\n",
    "                marker_color=model_colors.get(model, '#95A5A6'),\n",
    "                showlegend=False,\n",
    "                opacity=0.8,\n",
    "                hovertemplate=f'<b>{model}</b><br>Document: %{{x}}<br>Avg F1: %{{y:.3f}}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 5. Lenient vs Strict comparison - Enhanced\n",
    "    df_comparison = df.groupby(['Model', 'Evaluation_Mode'])['F1_Score'].mean().reset_index()\n",
    "    lenient_data = df_comparison[df_comparison['Evaluation_Mode'] == 'Lenient']\n",
    "    strict_data = df_comparison[df_comparison['Evaluation_Mode'] == 'Strict']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=lenient_data['Model'],\n",
    "            y=lenient_data['F1_Score'],\n",
    "            name='Lenient Evaluation',\n",
    "            marker_color='lightblue',\n",
    "            showlegend=True,\n",
    "            legendgroup='evaluation_modes',\n",
    "            hovertemplate='<b>Lenient</b><br>Model: %{x}<br>F1: %{y:.3f}<extra></extra>'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=strict_data['Model'],\n",
    "            y=strict_data['F1_Score'],\n",
    "            name='Strict Evaluation',\n",
    "            marker_color='darkblue',\n",
    "            showlegend=True,\n",
    "            legendgroup='evaluation_modes',\n",
    "            hovertemplate='<b>Strict</b><br>Model: %{x}<br>F1: %{y:.3f}<extra></extra>'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Individual Document Analysis (Box plots showing variance)\n",
    "    for i, annotation_type in enumerate(df_lenient['Annotation_Type'].unique()):\n",
    "        ann_data = df_lenient[df_lenient['Annotation_Type'] == annotation_type]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=ann_data['F1_Score'],\n",
    "                name=annotation_type,\n",
    "                boxpoints='all',\n",
    "                jitter=0.3,\n",
    "                pointpos=-1.8,\n",
    "                marker_color=px.colors.qualitative.Set1[i % len(px.colors.qualitative.Set1)],\n",
    "                showlegend=True,\n",
    "                legendgroup='annotation_types',\n",
    "                hovertemplate=f'<b>{annotation_type}</b><br>F1: %{{y:.3f}}<extra></extra>'\n",
    "            ),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    \n",
    "    # 7. Annotation Type Performance Distribution\n",
    "    df_counts = df_lenient.groupby('Annotation_Type').agg({\n",
    "        'Gold_Count': 'first',\n",
    "        'Predicted_Count': 'mean',\n",
    "        'F1_Score': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_counts['Annotation_Type'],\n",
    "            y=df_counts['Gold_Count'],\n",
    "            name='Gold Standard Count',\n",
    "            marker_color='gold',\n",
    "            showlegend=True,\n",
    "            legendgroup='counts',\n",
    "            hovertemplate='<b>Gold Standard</b><br>Type: %{x}<br>Count: %{y}<extra></extra>'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_counts['Annotation_Type'],\n",
    "            y=df_counts['Predicted_Count'],\n",
    "            name='Avg Predicted Count',\n",
    "            marker_color='silver',\n",
    "            showlegend=True,\n",
    "            legendgroup='counts',\n",
    "            yaxis='y2',\n",
    "            hovertemplate='<b>Predicted</b><br>Type: %{x}<br>Avg Count: %{y:.1f}<extra></extra>'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # 8. Model Consistency Across Documents (Coefficient of Variation)\n",
    "    model_consistency = []\n",
    "    for model in df_lenient['Model'].unique():\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        doc_scores = model_data.groupby('Document')['F1_Score'].mean()\n",
    "        cv = doc_scores.std() / doc_scores.mean() if doc_scores.mean() > 0 else 0\n",
    "        model_consistency.append({'Model': model, 'Consistency': 1 - cv, 'CV': cv})\n",
    "    \n",
    "    consistency_df = pd.DataFrame(model_consistency)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=consistency_df['Model'],\n",
    "            y=consistency_df['Consistency'],\n",
    "            mode='markers+lines',\n",
    "            name='Model Consistency',\n",
    "            marker=dict(\n",
    "                size=12,\n",
    "                color=[model_colors.get(model, '#95A5A6') for model in consistency_df['Model']],\n",
    "                symbol='diamond'\n",
    "            ),\n",
    "            line=dict(color='gray', dash='dash'),\n",
    "            showlegend=True,\n",
    "            legendgroup='consistency',\n",
    "            hovertemplate='<b>%{x}</b><br>Consistency: %{y:.3f}<br>CV: %{customdata:.3f}<extra></extra>',\n",
    "            customdata=consistency_df['CV']\n",
    "        ),\n",
    "        row=4, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout with improved legend and styling\n",
    "    fig.update_layout(\n",
    "        height=1600,  # Increased height for 4 rows\n",
    "        title=f'Enhanced LLM Evaluation Dashboard<br><sub>Pipeline Results: {Path(results_path).parent.name} | Document-Level Analysis</sub>',\n",
    "        title_x=0.5,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\", \n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "            bordercolor=\"rgba(0,0,0,0.2)\",\n",
    "            borderwidth=1,\n",
    "            font=dict(size=10)\n",
    "        ),\n",
    "        font=dict(size=11)\n",
    "    )\n",
    "    \n",
    "    # Update subplot titles and axes with better formatting\n",
    "    fig.update_xaxes(title_text=\"Annotation Type\", row=1, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Average F1 Score\", row=1, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=1, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Document\", row=1, col=2, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Recall\", row=2, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Precision\", row=2, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Document\", row=2, col=2, title_font_size=10, tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=2, col=2, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=3, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=3, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Annotation Type\", row=3, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"F1 Score Distribution\", row=3, col=2, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Annotation Type\", row=4, col=1, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=4, col=1, title_font_size=10)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=4, col=2, title_font_size=10)\n",
    "    fig.update_yaxes(title_text=\"Consistency Score\", row=4, col=2, title_font_size=10)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_document_comparison_plot(df_lenient):\n",
    "    \"\"\"Create a detailed document comparison visualization.\"\"\"\n",
    "    # Document performance comparison\n",
    "    doc_fig = go.Figure()\n",
    "    \n",
    "    documents = df_lenient['Document'].unique()\n",
    "    models = df_lenient['Model'].unique()\n",
    "    \n",
    "    # Dynamic color palette for all available models\n",
    "    color_palette = [\n",
    "        '#E74C3C',   # Red\n",
    "        '#3498DB',   # Blue\n",
    "        '#2ECC71',   # Green\n",
    "        '#F39C12',   # Orange\n",
    "        '#9B59B6',   # Purple\n",
    "        '#1ABC9C',   # Teal\n",
    "        '#F1C40F',   # Yellow\n",
    "        '#E67E22',   # Dark Orange\n",
    "        '#95A5A6',   # Gray\n",
    "        '#34495E',   # Dark Blue Gray\n",
    "        '#16A085',   # Dark Teal\n",
    "        '#8E44AD',   # Dark Purple\n",
    "        '#C0392B',   # Dark Red\n",
    "        '#2980B9',   # Dark Blue\n",
    "        '#27AE60',   # Dark Green\n",
    "        '#D35400'    # Dark Orange Red\n",
    "    ]\n",
    "    \n",
    "    # Get unique models and assign colors dynamically\n",
    "    unique_models = df_lenient['Model'].unique()\n",
    "    model_colors = {}\n",
    "    for i, model in enumerate(unique_models):\n",
    "        model_colors[model] = color_palette[i % len(color_palette)]\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = df_lenient[df_lenient['Model'] == model]\n",
    "        doc_scores = model_data.groupby('Document')['F1_Score'].mean()\n",
    "        \n",
    "        doc_fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=documents,\n",
    "                y=[doc_scores.get(doc, 0) for doc in documents],\n",
    "                mode='lines+markers',\n",
    "                name=model,\n",
    "                line=dict(color=model_colors.get(model, '#95A5A6'), width=3),\n",
    "                marker=dict(size=8),\n",
    "                hovertemplate=f'<b>{model}</b><br>Document: %{{x}}<br>Avg F1: %{{y:.3f}}<extra></extra>'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    doc_fig.update_layout(\n",
    "        title='Model Performance Across Individual Documents',\n",
    "        xaxis_title='Document',\n",
    "        yaxis_title='Average F1 Score',\n",
    "        height=400,\n",
    "        hovermode='x unified',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\", \n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    doc_fig.update_xaxes(tickangle=45)\n",
    "    return doc_fig\n",
    "\n",
    "# Enhanced example usage with document-level analysis\n",
    "# Use the selected pipeline folder from above\n",
    "if 'results_path' in globals() and results_path and Path(results_path).exists():\n",
    "    # Create main dashboard\n",
    "    fig = visualize_single_run(results_path)\n",
    "    fig.show()\n",
    "    \n",
    "    # Create additional document comparison plot\n",
    "    results = load_evaluation_results(results_path)\n",
    "    df = create_dataframe_from_results(results)\n",
    "    df_lenient = df[df['Evaluation_Mode'] == 'Lenient']\n",
    "    \n",
    "    doc_fig = create_document_comparison_plot(df_lenient)\n",
    "    doc_fig.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"üìä EVALUATION SUMMARY:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Model rankings\n",
    "    model_rankings = df_lenient.groupby('Model')['F1_Score'].agg(['mean', 'std']).round(3)\n",
    "    model_rankings = model_rankings.sort_values('mean', ascending=False)\n",
    "    print(\"\\nüèÜ MODEL RANKINGS (by average F1):\")\n",
    "    for i, (model, stats) in enumerate(model_rankings.iterrows(), 1):\n",
    "        print(f\"{i}. {model}: {stats['mean']:.3f} (¬±{stats['std']:.3f})\")\n",
    "    \n",
    "    # Best performing annotation types\n",
    "    ann_performance = df_lenient.groupby('Annotation_Type')['F1_Score'].agg(['mean', 'std']).round(3)\n",
    "    ann_performance = ann_performance.sort_values('mean', ascending=False)\n",
    "    print(\"\\nüìã ANNOTATION TYPE PERFORMANCE:\")\n",
    "    for ann_type, stats in ann_performance.iterrows():\n",
    "        print(f\"‚Ä¢ {ann_type}: {stats['mean']:.3f} (¬±{stats['std']:.3f})\")\n",
    "    \n",
    "    # Document difficulty analysis\n",
    "    doc_difficulty = df_lenient.groupby('Document')['F1_Score'].agg(['mean', 'std']).round(3)\n",
    "    doc_difficulty = doc_difficulty.sort_values('mean')\n",
    "    print(f\"\\nüìÑ DOCUMENT ANALYSIS:\")\n",
    "    print(f\"Most challenging: {doc_difficulty.index[0]} (avg F1: {doc_difficulty.iloc[0]['mean']:.3f})\")\n",
    "    print(f\"Easiest: {doc_difficulty.index[-1]} (avg F1: {doc_difficulty.iloc[-1]['mean']:.3f})\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Enhanced single run visualization created successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Results file not found or not selected!\")\n",
    "    if 'results_path' not in globals() or not results_path:\n",
    "        print(\"Please run the pipeline folder selector at the top first.\")\n",
    "    else:\n",
    "        print(f\"File path: {results_path}\")\n",
    "    print(\"\\nAvailable pipeline results:\")\n",
    "    output_dir = Path(\"output\")\n",
    "    if output_dir.exists():\n",
    "        for folder in sorted(output_dir.glob(\"pipeline_results_*\")):\n",
    "            results_file = folder / \"llm_evaluation_results.json\"\n",
    "            if results_file.exists():\n",
    "                print(f\"  - {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline_timing_data(pipeline_folder):\n",
    "    \"\"\"Load timing data from pipeline results including overall and per-document timing.\"\"\"\n",
    "    pipeline_folder = Path(pipeline_folder)\n",
    "    \n",
    "    # Load main pipeline results\n",
    "    main_results_file = pipeline_folder / f\"{pipeline_folder.name}.json\"\n",
    "    with open(main_results_file, 'r', encoding='utf-8') as f:\n",
    "        pipeline_info = json.load(f)\n",
    "    \n",
    "    # Load evaluation results for performance data\n",
    "    eval_results_file = pipeline_folder / \"llm_evaluation_results.json\"\n",
    "    with open(eval_results_file, 'r', encoding='utf-8') as f:\n",
    "        eval_results = json.load(f)\n",
    "    \n",
    "    return {\n",
    "        'pipeline_info': pipeline_info,\n",
    "        'evaluation_results': eval_results\n",
    "    }\n",
    "\n",
    "def calculate_runtime_metrics(timing_data, eval_df):\n",
    "    \"\"\"Calculate comprehensive runtime and efficiency metrics using actual timing data.\"\"\"\n",
    "    \n",
    "    # Parse pipeline timing\n",
    "    pipeline_info = timing_data['pipeline_info']\n",
    "    \n",
    "    # Get actual total processing time\n",
    "    total_processing_time = pipeline_info.get('total_processing_time', '0:00:00')\n",
    "    \n",
    "    # Parse the time string \"H:MM:SS.microseconds\" format\n",
    "    if isinstance(total_processing_time, str):\n",
    "        time_parts = total_processing_time.split(':')\n",
    "        if len(time_parts) == 3:\n",
    "            hours = int(time_parts[0])\n",
    "            minutes = int(time_parts[1])\n",
    "            seconds = float(time_parts[2])\n",
    "            total_pipeline_time = hours * 3600 + minutes * 60 + seconds\n",
    "        else:\n",
    "            total_pipeline_time = 0\n",
    "    else:\n",
    "        total_pipeline_time = 0\n",
    "    \n",
    "    # Get evaluation data\n",
    "    eval_lenient = eval_df[eval_df['Evaluation_Mode'] == 'Lenient'].copy()\n",
    "    \n",
    "    # Group evaluation data by document and model\n",
    "    eval_summary = eval_lenient.groupby(['Document', 'Model']).agg({\n",
    "        'F1_Score': 'mean',\n",
    "        'Precision': 'mean',\n",
    "        'Recall': 'mean',\n",
    "        'True_Positives': 'sum',\n",
    "        'False_Positives': 'sum',\n",
    "        'False_Negatives': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create actual timing data - since we don't have individual model timings,\n",
    "    # distribute the total time evenly across all model-document combinations\n",
    "    models = pipeline_info['models_used']\n",
    "    docs = eval_summary['Document'].unique()\n",
    "    \n",
    "    # Calculate actual average time per document per model\n",
    "    avg_time_per_doc_per_model = total_pipeline_time / (len(models) * len(docs)) if len(models) > 0 and len(docs) > 0 else 0\n",
    "    \n",
    "    actual_timing = []\n",
    "    for doc in docs:\n",
    "        for model in models:\n",
    "            # Get annotation count for this doc-model combo\n",
    "            doc_eval = eval_summary[(eval_summary['Document'] == doc) & (eval_summary['Model'] == model)]\n",
    "            if not doc_eval.empty:\n",
    "                annotation_count = doc_eval['True_Positives'].iloc[0] + doc_eval['False_Positives'].iloc[0]\n",
    "            else:\n",
    "                annotation_count = 1  # Default minimum\n",
    "            \n",
    "            actual_timing.append({\n",
    "                'document': doc,\n",
    "                'model': model,\n",
    "                'processing_time': avg_time_per_doc_per_model,\n",
    "                'annotation_count': max(annotation_count, 1),  # Ensure at least 1\n",
    "                'actual': True\n",
    "            })\n",
    "    \n",
    "    doc_timing = pd.DataFrame(actual_timing)\n",
    "    \n",
    "    # Merge timing with evaluation metrics\n",
    "    merged_data = pd.merge(\n",
    "        eval_summary, \n",
    "        doc_timing, \n",
    "        left_on=['Document', 'Model'], \n",
    "        right_on=['document', 'model'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calculate efficiency metrics (avoid division by zero)\n",
    "    merged_data['efficiency_score'] = merged_data['F1_Score'] / (merged_data['processing_time'] + 0.001)\n",
    "    merged_data['annotations_per_second'] = merged_data['annotation_count'] / (merged_data['processing_time'] + 0.001)\n",
    "    merged_data['time_per_annotation'] = merged_data['processing_time'] / (merged_data['annotation_count'] + 1)\n",
    "    \n",
    "    return {\n",
    "        'merged_data': merged_data,\n",
    "        'total_pipeline_time': total_pipeline_time,\n",
    "        'pipeline_info': pipeline_info,\n",
    "        'models_used': pipeline_info['models_used'],\n",
    "        'documents_processed': len(docs),\n",
    "        'avg_time_per_doc_per_model': avg_time_per_doc_per_model\n",
    "    }\n",
    "\n",
    "def create_runtime_dashboard(pipeline_folder):\n",
    "    \"\"\"Create comprehensive runtime analysis dashboard.\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    timing_data = load_pipeline_timing_data(pipeline_folder)\n",
    "    eval_results = load_evaluation_results(pipeline_folder / \"llm_evaluation_results.json\")\n",
    "    eval_df = create_dataframe_from_results(eval_results)\n",
    "    \n",
    "    # Calculate runtime metrics\n",
    "    runtime_metrics = calculate_runtime_metrics(timing_data, eval_df)\n",
    "    merged_data = runtime_metrics['merged_data']\n",
    "    \n",
    "    # Create dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Processing Time by Model and Document',\n",
    "            'F1 Score vs Processing Time (Performance vs Speed)',\n",
    "            'Model Efficiency Score (F1/Time)',\n",
    "            'Annotations per Second by Model',\n",
    "            'Time per Annotation Distribution',\n",
    "            'Runtime vs Performance Trade-off',\n",
    "            'Model Speed Comparison',\n",
    "            'Document Processing Complexity'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
    "               [{\"type\": \"violin\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"heatmap\"}]],\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.12\n",
    "    )\n",
    "    \n",
    "    # Use dynamic colors based on available models\n",
    "    model_colors = {}\n",
    "    models = merged_data['Model'].unique()\n",
    "    color_palette = ['#E74C3C', '#3498DB', '#2ECC71', '#F39C12', '#9B59B6', '#1ABC9C', \n",
    "                    '#F1C40F', '#E67E22', '#95A5A6', '#34495E', '#16A085', '#8E44AD']\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_colors[model] = color_palette[i % len(color_palette)]\n",
    "    \n",
    "    # 1. Processing Time by Model and Document\n",
    "    for model in merged_data['Model'].unique():\n",
    "        model_data = merged_data[merged_data['Model'] == model]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=model_data['Document'],\n",
    "                y=model_data['processing_time'],\n",
    "                name=f'{model}',\n",
    "                marker_color=model_colors.get(model, '#95A5A6'),\n",
    "                showlegend=True,\n",
    "                legendgroup='models',\n",
    "                hovertemplate=f'<b>{model}</b><br>Document: %{{x}}<br>Time: %{{y:.2f}}s<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. F1 Score vs Processing Time\n",
    "    for model in merged_data['Model'].unique():\n",
    "        model_data = merged_data[merged_data['Model'] == model]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_data['processing_time'],\n",
    "                y=model_data['F1_Score'],\n",
    "                mode='markers',\n",
    "                name=f'{model}',\n",
    "                marker=dict(\n",
    "                    size=12,\n",
    "                    color=model_colors.get(model, '#95A5A6'),\n",
    "                    symbol='circle',\n",
    "                    opacity=0.8\n",
    "                ),\n",
    "                text=model_data['Document'],\n",
    "                showlegend=False,\n",
    "                legendgroup='models',\n",
    "                hovertemplate=f'<b>{model}</b><br>Time: %{{x:.2f}}s<br>F1: %{{y:.3f}}<br>Doc: %{{text}}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Model Efficiency Score\n",
    "    efficiency_data = merged_data.groupby('Model')['efficiency_score'].mean().reset_index()\n",
    "    efficiency_data = efficiency_data.sort_values('efficiency_score', ascending=True)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=efficiency_data['efficiency_score'],\n",
    "            y=efficiency_data['Model'],\n",
    "            orientation='h',\n",
    "            name='Efficiency',\n",
    "            marker_color=[model_colors.get(model, '#95A5A6') for model in efficiency_data['Model']],\n",
    "            showlegend=False,\n",
    "            hovertemplate='<b>%{y}</b><br>Efficiency: %{x:.4f} F1/sec<extra></extra>'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Annotations per Second by Model\n",
    "    for model in merged_data['Model'].unique():\n",
    "        model_data = merged_data[merged_data['Model'] == model]\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=model_data['annotations_per_second'],\n",
    "                name=model,\n",
    "                boxpoints='all',\n",
    "                jitter=0.3,\n",
    "                marker_color=model_colors.get(model, '#95A5A6'),\n",
    "                showlegend=False,\n",
    "                hovertemplate=f'<b>{model}</b><br>Ann/sec: %{{y:.2f}}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 5. Time per Annotation Distribution\n",
    "    for model in merged_data['Model'].unique():\n",
    "        model_data = merged_data[merged_data['Model'] == model]\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                y=model_data['time_per_annotation'],\n",
    "                name=model,\n",
    "                side='positive',\n",
    "                marker_color=model_colors.get(model, '#95A5A6'),\n",
    "                showlegend=False,\n",
    "                hovertemplate=f'<b>{model}</b><br>Time/Ann: %{{y:.3f}}s<extra></extra>'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # 6. Runtime vs Performance Trade-off (Bubble chart)\n",
    "    bubble_data = merged_data.groupby('Model').agg({\n",
    "        'F1_Score': 'mean',\n",
    "        'processing_time': 'mean',\n",
    "        'annotation_count': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bubble_data['processing_time'],\n",
    "            y=bubble_data['F1_Score'],\n",
    "            mode='markers+text',\n",
    "            text=bubble_data['Model'],\n",
    "            textposition='top center',\n",
    "            marker=dict(\n",
    "                size=bubble_data['annotation_count'],\n",
    "                color=[model_colors.get(model, '#95A5A6') for model in bubble_data['Model']],\n",
    "                opacity=0.7,\n",
    "                sizemode='diameter',\n",
    "                sizemin=15,\n",
    "                sizeref=2.*max(bubble_data['annotation_count'])/(40.**2)\n",
    "            ),\n",
    "            name='Performance vs Speed',\n",
    "            showlegend=False,\n",
    "            hovertemplate='<b>%{text}</b><br>Time: %{x:.2f}s<br>F1: %{y:.3f}<br>Annotations: %{marker.size}<extra></extra>'\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # 7. Model Speed Comparison\n",
    "    speed_data = merged_data.groupby('Model')['processing_time'].mean().reset_index()\n",
    "    speed_data = speed_data.sort_values('processing_time')\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=speed_data['Model'],\n",
    "            y=speed_data['processing_time'],\n",
    "            name='Avg Processing Time',\n",
    "            marker_color=[model_colors.get(model, '#95A5A6') for model in speed_data['Model']],\n",
    "            showlegend=False,\n",
    "            hovertemplate='<b>%{x}</b><br>Avg Time: %{y:.2f}s<extra></extra>'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # 8. Document Processing Complexity Heatmap\n",
    "    complexity_pivot = merged_data.pivot(index='Document', columns='Model', values='processing_time')\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=complexity_pivot.values,\n",
    "            x=complexity_pivot.columns,\n",
    "            y=complexity_pivot.index,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            hovertemplate='Document: %{y}<br>Model: %{x}<br>Time: %{z:.2f}s<extra></extra>',\n",
    "            colorbar=dict(title=\"Processing Time (s)\", x=1.02)\n",
    "        ),\n",
    "        row=4, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1600,\n",
    "        title=f'üöÄ Runtime Analysis Dashboard<br><sub>Pipeline: {Path(pipeline_folder).name} | Total Time: {runtime_metrics[\"total_pipeline_time\"]/60:.1f} minutes</sub>',\n",
    "        title_x=0.5,\n",
    "        showlegend=True,\n",
    "        font=dict(size=11)\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Documents\", row=1, col=1, tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"Processing Time (s)\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Processing Time (s)\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Efficiency Score (F1/s)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Model\", row=2, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Annotations per Second\", row=2, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Time per Annotation (s)\", row=3, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Processing Time (s)\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=3, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"Processing Time (s)\", row=4, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model\", row=4, col=2)\n",
    "    fig.update_yaxes(title_text=\"Document\", row=4, col=2)\n",
    "    \n",
    "    return fig, runtime_metrics\n",
    "\n",
    "def print_runtime_summary(runtime_metrics):\n",
    "    \"\"\"Print comprehensive runtime analysis summary.\"\"\"\n",
    "    merged_data = runtime_metrics['merged_data']\n",
    "    pipeline_info = runtime_metrics['pipeline_info']\n",
    "    total_time = runtime_metrics['total_pipeline_time']\n",
    "    \n",
    "    print(\"\\nüöÄ RUNTIME ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Pipeline overview\n",
    "    print(f\"\\n‚è±Ô∏è PIPELINE OVERVIEW:\")\n",
    "    print(f\"Total processing time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"Documents processed: {pipeline_info['processed_documents']}\")\n",
    "    print(f\"Models used: {len(pipeline_info['models_used'])}\")\n",
    "    print(f\"Start time: {pipeline_info['start_time']}\")\n",
    "    print(f\"End time: {pipeline_info['end_time']}\")\n",
    "    \n",
    "    # Model performance summary\n",
    "    model_stats = merged_data.groupby('Model').agg({\n",
    "        'processing_time': ['mean', 'std', 'sum'],\n",
    "        'F1_Score': 'mean',\n",
    "        'efficiency_score': 'mean',\n",
    "        'annotations_per_second': 'mean',\n",
    "        'time_per_annotation': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(f\"\\nüìä MODEL PERFORMANCE SUMMARY:\")\n",
    "    print(f\"{'Model':<15} {'Avg Time (s)':<12} {'Total Time (s)':<14} {'Avg F1':<8} {'Efficiency':<10} {'Ann/sec':<8}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for model in model_stats.index:\n",
    "        avg_time = model_stats.loc[model, ('processing_time', 'mean')]\n",
    "        total_time_model = model_stats.loc[model, ('processing_time', 'sum')]\n",
    "        avg_f1 = model_stats.loc[model, ('F1_Score', 'mean')]\n",
    "        efficiency = model_stats.loc[model, ('efficiency_score', 'mean')]\n",
    "        ann_per_sec = model_stats.loc[model, ('annotations_per_second', 'mean')]\n",
    "        \n",
    "        print(f\"{model:<15} {avg_time:<12.2f} {total_time_model:<14.1f} {avg_f1:<8.3f} {efficiency:<10.4f} {ann_per_sec:<8.2f}\")\n",
    "    \n",
    "    # Speed rankings\n",
    "    speed_ranking = merged_data.groupby('Model')['processing_time'].mean().sort_values()\n",
    "    efficiency_ranking = merged_data.groupby('Model')['efficiency_score'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nüèÉ SPEED RANKINGS (fastest to slowest):\")\n",
    "    for i, (model, time) in enumerate(speed_ranking.items(), 1):\n",
    "        print(f\"{i}. {model}: {time:.2f}s avg per document\")\n",
    "    \n",
    "    print(f\"\\nüéØ EFFICIENCY RANKINGS (F1/time, best to worst):\")\n",
    "    for i, (model, eff) in enumerate(efficiency_ranking.items(), 1):\n",
    "        print(f\"{i}. {model}: {eff:.4f} F1 per second\")\n",
    "    \n",
    "    # Document analysis\n",
    "    doc_stats = merged_data.groupby('Document').agg({\n",
    "        'processing_time': ['mean', 'std'],\n",
    "        'F1_Score': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(f\"\\nüìÑ DOCUMENT PROCESSING ANALYSIS:\")\n",
    "    print(f\"{'Document':<40} {'Avg Time (s)':<12} {'Std (s)':<10} {'Avg F1':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for doc in doc_stats.index:\n",
    "        doc_short = doc[:35] + \"...\" if len(doc) > 35 else doc\n",
    "        avg_time = doc_stats.loc[doc, ('processing_time', 'mean')]\n",
    "        std_time = doc_stats.loc[doc, ('processing_time', 'std')]\n",
    "        avg_f1 = doc_stats.loc[doc, ('F1_Score', 'mean')]\n",
    "        \n",
    "        print(f\"{doc_short:<40} {avg_time:<12.2f} {std_time:<10.2f} {avg_f1:<8.3f}\")\n",
    "    \n",
    "    # Key insights\n",
    "    fastest_model = speed_ranking.index[0]\n",
    "    slowest_model = speed_ranking.index[-1]\n",
    "    most_efficient = efficiency_ranking.index[0]\n",
    "    \n",
    "    print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "    print(f\"‚Ä¢ Fastest model: {fastest_model} ({speed_ranking[fastest_model]:.2f}s avg)\")\n",
    "    print(f\"‚Ä¢ Slowest model: {slowest_model} ({speed_ranking[slowest_model]:.2f}s avg)\")\n",
    "    print(f\"‚Ä¢ Most efficient: {most_efficient} ({efficiency_ranking[most_efficient]:.4f} F1/sec)\")\n",
    "    print(f\"‚Ä¢ Speed difference: {slowest_model} is {speed_ranking[slowest_model]/speed_ranking[fastest_model]:.1f}x slower than {fastest_model}\")\n",
    "    \n",
    "    # Performance vs speed trade-offs\n",
    "    best_f1_model = merged_data.groupby('Model')['F1_Score'].mean().idxmax()\n",
    "    print(f\"‚Ä¢ Best F1 model: {best_f1_model}\")\n",
    "    print(f\"‚Ä¢ Best trade-off: {most_efficient} (combines good performance with speed)\")\n",
    "    \n",
    "    return {\n",
    "        'model_stats': model_stats,\n",
    "        'speed_ranking': speed_ranking,\n",
    "        'efficiency_ranking': efficiency_ranking,\n",
    "        'document_stats': doc_stats\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Runtime analysis functions updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b045fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RUNTIME ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check if we have a selected pipeline folder\n",
    "if 'selected_pipeline_folder' not in globals() or not selected_pipeline_folder:\n",
    "    print(\"‚ùå No pipeline folder selected. Please run the pipeline folder selector cell first.\")\n",
    "else:\n",
    "    # Set up the main file path\n",
    "    main_file = Path(selected_pipeline_folder) / f\"{Path(selected_pipeline_folder).name}.json\"\n",
    "    \n",
    "    if not main_file.exists():\n",
    "        print(f\"‚ùå Pipeline info file not found: {main_file}\")\n",
    "        print(\"Available files in the pipeline folder:\")\n",
    "        for file in Path(selected_pipeline_folder).glob(\"*.json\"):\n",
    "            print(f\"  - {file.name}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Using pipeline folder: {selected_pipeline_folder}\")\n",
    "        print(f\"üìÑ Pipeline info file: {main_file}\")\n",
    "        \n",
    "        # Extract actual runtime data from annotations\n",
    "        model_runtime_data = []\n",
    "        document_runtime_data = []\n",
    "\n",
    "        # Get runtime data for each model-document combination\n",
    "        for doc_file in main_file.parent.glob(\"*.json\"):\n",
    "            if doc_file.name != \"pipeline_info.json\" and doc_file != main_file:\n",
    "                try:\n",
    "                    with open(doc_file, 'r', encoding='utf-8') as f:\n",
    "                        doc_data = json.load(f)\n",
    "                        doc_name = doc_data.get('document_name', doc_file.stem)\n",
    "                        \n",
    "                        for annotation in doc_data.get('annotations', []):\n",
    "                            model_name = annotation.get('model_name', 'Unknown')\n",
    "                            llm_runtime = annotation.get('llm_runtime_seconds', 0)\n",
    "                            model_runtime = annotation.get('model_runtime_seconds', 0)\n",
    "                            \n",
    "                            model_runtime_data.append({\n",
    "                                'model': model_name,\n",
    "                                'document': doc_name,\n",
    "                                'llm_runtime_seconds': llm_runtime,\n",
    "                                'model_runtime_seconds': model_runtime\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not process {doc_file.name}: {e}\")\n",
    "\n",
    "        # Convert to DataFrame for easier analysis\n",
    "        runtime_df = pd.DataFrame(model_runtime_data)\n",
    "\n",
    "        if not runtime_df.empty:\n",
    "            # Aggregate runtime by model\n",
    "            model_totals = runtime_df.groupby('model')['model_runtime_seconds'].sum().sort_values(ascending=False)\n",
    "            \n",
    "            # Aggregate runtime by document\n",
    "            doc_totals = runtime_df.groupby('document')['model_runtime_seconds'].sum().sort_values(ascending=False)\n",
    "            \n",
    "            # Overall statistics\n",
    "            total_runtime = runtime_df['model_runtime_seconds'].sum()\n",
    "            avg_per_model = runtime_df.groupby('model')['model_runtime_seconds'].mean()\n",
    "            avg_per_doc = runtime_df.groupby('document')['model_runtime_seconds'].mean()\n",
    "            \n",
    "            print(f\"Total Processing Time: {total_runtime:.2f} seconds ({total_runtime/3600:.2f} hours)\")\n",
    "            print(f\"Average per Model-Document: {runtime_df['model_runtime_seconds'].mean():.2f} seconds\")\n",
    "            print(f\"Total Model-Document Combinations: {len(runtime_df)}\")\n",
    "            \n",
    "            print(\"\\nTOP 5 SLOWEST MODELS (Total Time):\")\n",
    "            for i, (model, time_sec) in enumerate(model_totals.head(5).items(), 1):\n",
    "                print(f\"  {i}. {model}: {time_sec:.2f}s ({time_sec/60:.1f}m)\")\n",
    "            \n",
    "            print(\"\\nTOP 5 FASTEST MODELS (Average Time per Document):\")\n",
    "            fastest_avg = avg_per_model.sort_values(ascending=True)\n",
    "            for i, (model, time_sec) in enumerate(fastest_avg.head(5).items(), 1):\n",
    "                print(f\"  {i}. {model}: {time_sec:.2f}s avg\")\n",
    "            \n",
    "            print(\"\\nTOP 5 MOST TIME-CONSUMING DOCUMENTS:\")\n",
    "            for i, (doc, time_sec) in enumerate(doc_totals.head(5).items(), 1):\n",
    "                print(f\"  {i}. {doc}: {time_sec:.2f}s ({time_sec/60:.1f}m)\")\n",
    "            \n",
    "            # Store runtime data for potential visualization\n",
    "            model_runtime_summary = model_totals\n",
    "            document_runtime_summary = doc_totals\n",
    "            \n",
    "            print(f\"\\nRuntime analysis complete! Found timing data for {len(runtime_df)} model-document combinations\")\n",
    "        else:\n",
    "            print(\"No runtime data found in annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1066fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"DETAILED MODEL RUNTIMES PER DOCUMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the runtime data we collected in the previous cell\n",
    "if 'runtime_df' in locals() and not runtime_df.empty:\n",
    "    # Create a pivot table for better visualization\n",
    "    runtime_pivot = runtime_df.pivot_table(\n",
    "        index='document', \n",
    "        columns='model', \n",
    "        values='model_runtime_seconds',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Display top 10 documents by total runtime\n",
    "    doc_total_times = runtime_df.groupby('document')['model_runtime_seconds'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Time-Consuming Documents:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (doc, total_time) in enumerate(doc_total_times.head(10).items(), 1):\n",
    "        hours = total_time / 3600\n",
    "        print(f\"{i:2d}. {doc}\")\n",
    "        print(f\"    Total: {total_time:.1f}s ({hours:.1f}h)\")\n",
    "        \n",
    "        # Show breakdown by model for this document\n",
    "        doc_models = runtime_df[runtime_df['document'] == doc].sort_values('model_runtime_seconds', ascending=False)\n",
    "        print(\"    Top 3 slowest models:\")\n",
    "        for j, row in doc_models.head(3).iterrows():\n",
    "            model_time = row['model_runtime_seconds']\n",
    "            pct = (model_time / total_time) * 100\n",
    "            print(f\"      - {row['model']}: {model_time:.1f}s ({pct:.1f}%)\")\n",
    "        print()\n",
    "    \n",
    "    # Show model performance consistency\n",
    "    print(\"\\nModel Performance Consistency:\")\n",
    "    print(\"-\" * 35)\n",
    "    model_stats = runtime_df.groupby('model')['model_runtime_seconds'].agg(['mean', 'std', 'min', 'max']).round(2)\n",
    "    model_stats['cv'] = (model_stats['std'] / model_stats['mean']).round(3)  # Coefficient of variation\n",
    "    model_stats = model_stats.sort_values('cv')  # Sort by consistency (lower CV = more consistent)\n",
    "    \n",
    "    print(\"Most Consistent Models (lowest coefficient of variation):\")\n",
    "    for i, (model, stats) in enumerate(model_stats.head(5).iterrows(), 1):\n",
    "        print(f\"{i}. {model}\")\n",
    "        print(f\"   Avg: {stats['mean']:.1f}s, CV: {stats['cv']:.3f}\")\n",
    "        print(f\"   Range: {stats['min']:.1f}s - {stats['max']:.1f}s\")\n",
    "    \n",
    "    # Store the pivot table for potential use in visualizations\n",
    "    model_document_runtimes = runtime_pivot\n",
    "    print(f\"\\nData summary: {len(runtime_df)} model-document combinations analyzed\")\n",
    "    \n",
    "else:\n",
    "    print(\"No runtime data available. Please run the previous runtime analysis cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Runtime analysis and dashboard functions ready!\")\n",
    "\n",
    "def create_actual_runtime_dashboard():\n",
    "    \"\"\"Create runtime dashboard using actual timing data from annotations.\"\"\"\n",
    "    \n",
    "    # Check if we have a selected pipeline folder\n",
    "    if 'selected_pipeline_folder' not in globals() or not selected_pipeline_folder:\n",
    "        print(\"‚ùå No pipeline folder selected. Please run the pipeline folder selector cell first.\")\n",
    "        return None\n",
    "    \n",
    "    # Check if runtime data exists from previous analysis\n",
    "    try:\n",
    "        # Reference the global runtime_df created in the previous cell\n",
    "        runtime_data = runtime_df\n",
    "        if runtime_data.empty:\n",
    "            print(\"‚ùå Runtime data is empty. Please run the runtime analysis cell first.\")\n",
    "            return None\n",
    "    except NameError:\n",
    "        print(\"‚ùå No runtime data available. Please run the runtime analysis cell first.\")\n",
    "        return None\n",
    "    \n",
    "    # Set up the evaluation results path\n",
    "    eval_results_path = Path(selected_pipeline_folder) / \"llm_evaluation_results.json\"\n",
    "    if not eval_results_path.exists():\n",
    "        print(f\"‚ùå Evaluation results not found: {eval_results_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load evaluation results for F1 scores\n",
    "    eval_results = load_evaluation_results(eval_results_path)\n",
    "    eval_df = create_dataframe_from_results(eval_results)\n",
    "    eval_lenient = eval_df[eval_df['Evaluation_Mode'] == 'Lenient'].copy()\n",
    "    \n",
    "    # Merge runtime data with evaluation results\n",
    "    eval_summary = eval_lenient.groupby(['Document', 'Model']).agg({\n",
    "        'F1_Score': 'mean',\n",
    "        'Precision': 'mean',\n",
    "        'Recall': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Merge with actual runtime data\n",
    "    merged_data = runtime_data.merge(\n",
    "        eval_summary, \n",
    "        left_on=['document', 'model'], \n",
    "        right_on=['Document', 'Model'], \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    if merged_data.empty:\n",
    "        print(\"‚ùå Could not merge runtime and evaluation data\")\n",
    "        return None\n",
    "    \n",
    "    # Dynamic color palette for all available models\n",
    "    color_palette = [\n",
    "        '#E74C3C',   # Red\n",
    "        '#3498DB',   # Blue\n",
    "        '#2ECC71',   # Green\n",
    "        '#F39C12',   # Orange\n",
    "        '#9B59B6',   # Purple\n",
    "        '#1ABC9C',   # Teal\n",
    "        '#F1C40F',   # Yellow\n",
    "        '#E67E22',   # Dark Orange\n",
    "        '#95A5A6',   # Gray\n",
    "        '#34495E',   # Dark Blue Gray\n",
    "        '#16A085',   # Dark Teal\n",
    "        '#8E44AD',   # Dark Purple\n",
    "        '#C0392B',   # Dark Red\n",
    "        '#2980B9',   # Dark Blue\n",
    "        '#27AE60',   # Dark Green\n",
    "        '#D35400'    # Dark Orange Red\n",
    "    ]\n",
    "    \n",
    "    # Get unique models and assign colors dynamically\n",
    "    unique_models = merged_data['model'].unique()\n",
    "    model_colors = {}\n",
    "    for i, model in enumerate(unique_models):\n",
    "        model_colors[model] = color_palette[i % len(color_palette)]\n",
    "    \n",
    "    # Create dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Actual Processing Time by Model (seconds)',\n",
    "            'F1 Score vs Processing Time',\n",
    "            'Model Efficiency (F1 Score per second)',\n",
    "            'Processing Time Distribution by Model',\n",
    "            'Top 10 Slowest Document-Model Combinations',\n",
    "            'Model Performance vs Speed Trade-off'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]],\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    # Get model totals and averages\n",
    "    model_totals = merged_data.groupby('model')['model_runtime_seconds'].agg(['sum', 'mean']).reset_index()\n",
    "    model_totals.columns = ['model', 'total_time', 'avg_time']\n",
    "    model_totals = model_totals.sort_values('total_time', ascending=False)\n",
    "    \n",
    "    # 1. Total processing time by model\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=model_totals['model'],\n",
    "            y=model_totals['total_time'],\n",
    "            marker_color=[model_colors.get(model, '#95A5A6') for model in model_totals['model']],\n",
    "            name='Total Time',\n",
    "            hovertemplate='<b>%{x}</b><br>Total Time: %{y:.1f}s (%{customdata:.1f}m)<extra></extra>',\n",
    "            customdata=model_totals['total_time'] / 60\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. F1 Score vs Processing Time scatter\n",
    "    for model in merged_data['model'].unique():\n",
    "        model_data = merged_data[merged_data['model'] == model]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_data['model_runtime_seconds'],\n",
    "                y=model_data['F1_Score'],\n",
    "                mode='markers',\n",
    "                name=model,\n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    color=model_colors.get(model, '#95A5A6'),\n",
    "                    opacity=0.7\n",
    "                ),\n",
    "                text=model_data['document'],\n",
    "                hovertemplate=f'<b>{model}</b><br>Document: %{{text}}<br>Time: %{{x:.1f}}s<br>F1: %{{y:.3f}}<extra></extra>',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Model efficiency (F1 per second)\n",
    "    model_efficiency = merged_data.groupby('model').apply(\n",
    "        lambda x: (x['F1_Score'].mean() / x['model_runtime_seconds'].mean()) * 1000  # F1 per 1000 seconds\n",
    "    ).reset_index()\n",
    "    model_efficiency.columns = ['model', 'efficiency']\n",
    "    model_efficiency = model_efficiency.sort_values('efficiency', ascending=False)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=model_efficiency['model'],\n",
    "            y=model_efficiency['efficiency'],\n",
    "            marker_color=[model_colors.get(model, '#95A5A6') for model in model_efficiency['model']],\n",
    "            name='Efficiency',\n",
    "            hovertemplate='<b>%{x}</b><br>F1 per 1000s: %{y:.3f}<extra></extra>'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Processing time distribution by model (box plot)\n",
    "    for model in merged_data['model'].unique():\n",
    "        model_data = merged_data[merged_data['model'] == model]\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=model_data['model_runtime_seconds'],\n",
    "                name=model,\n",
    "                marker_color=model_colors.get(model, '#95A5A6'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 5. Top 10 slowest combinations\n",
    "    slowest_combinations = merged_data.nlargest(10, 'model_runtime_seconds')\n",
    "    time_in_minutes = slowest_combinations['model_runtime_seconds'] / 60\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[f\"{row['model'][:15]}...<br>{row['document'][:20]}...\" for _, row in slowest_combinations.iterrows()],\n",
    "            y=slowest_combinations['model_runtime_seconds'],\n",
    "            marker_color=[model_colors.get(model, '#95A5A6') for model in slowest_combinations['model']],\n",
    "            name='Slowest Combinations',\n",
    "            hovertemplate='<b>%{customdata}</b><br>Time: %{y:.1f}s (%{y:.1f}m)<extra></extra>',\n",
    "            customdata=[f\"{row['model']} - {row['document']}\" for _, row in slowest_combinations.iterrows()]\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Performance vs Speed trade-off (average by model)\n",
    "    model_summary = merged_data.groupby('model').agg({\n",
    "        'F1_Score': 'mean',\n",
    "        'model_runtime_seconds': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=model_summary['model_runtime_seconds'],\n",
    "            y=model_summary['F1_Score'],\n",
    "            mode='markers+text',\n",
    "            text=model_summary['model'],\n",
    "            textposition='top center',\n",
    "            marker=dict(\n",
    "                size=15,\n",
    "                color=[model_colors.get(model, '#95A5A6') for model in model_summary['model']],\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            name='Model Average',\n",
    "            hovertemplate='<b>%{text}</b><br>Avg Time: %{x:.1f}s<br>Avg F1: %{y:.3f}<extra></extra>',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        title=f'Actual Runtime Performance Dashboard<br><sub>Based on real timing data from {len(merged_data)} model-document combinations</sub>',\n",
    "        title_x=0.5,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Model\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Total Time (seconds)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Processing Time (seconds)\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Model\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"F1 per 1000 seconds\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Model\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Processing Time (seconds)\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"Model - Document\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Processing Time (seconds)\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Average Processing Time (seconds)\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"Average F1 Score\", row=3, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nRUNTIME DASHBOARD SUMMARY\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Total combinations analyzed: {len(merged_data)}\")\n",
    "    print(f\"Fastest model (avg): {model_totals.iloc[-1]['model']} ({model_totals.iloc[-1]['avg_time']:.1f}s)\")\n",
    "    print(f\"Slowest model (avg): {model_totals.iloc[0]['model']} ({model_totals.iloc[0]['avg_time']:.1f}s)\")\n",
    "    print(f\"Most efficient: {model_efficiency.iloc[0]['model']} ({model_efficiency.iloc[0]['efficiency']:.3f} F1/1000s)\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create the actual runtime dashboard\n",
    "actual_runtime_fig = create_actual_runtime_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
