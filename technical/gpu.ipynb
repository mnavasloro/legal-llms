{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1debdb41",
   "metadata": {},
   "source": [
    "### Quick GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f3bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 6.0%\n",
      "RAM Usage: 22.0%\n",
      "GPU Usage: 0%\n"
     ]
    }
   ],
   "source": [
    "def check_system_resources():\n",
    "    import psutil\n",
    "    print(f\"CPU Usage: {psutil.cpu_percent()}%\")\n",
    "    print(f\"RAM Usage: {psutil.virtual_memory().percent}%\")\n",
    "    \n",
    "    # Check if running on GPU\n",
    "    try:\n",
    "        import subprocess\n",
    "        gpu_info = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], \n",
    "                                capture_output=True, text=True)\n",
    "        if gpu_info.returncode == 0:\n",
    "            print(f\"GPU Usage: {gpu_info.stdout.strip()}%\")\n",
    "        else:\n",
    "            print(\"GPU: Not available or not in use\")\n",
    "    except:\n",
    "        print(\"GPU: nvidia-smi not found\")\n",
    "\n",
    "# Call before processing:\n",
    "check_system_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f58d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CHECKING GPU AVAILABILITY\n",
      "========================================\n",
      "‚úÖ NVIDIA GPU detected:\n",
      "Fri Aug  8 14:54:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.24                 Driver Version: 566.24         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 2000 Ada Gene...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P8              7W /   45W |     543MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      7776    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     11216    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12660    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     13192    C+G   ...m Files\\Zscaler\\ZSATray\\ZSATray.exe      N/A      |\n",
      "|    0   N/A  N/A     15060    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16596    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16884    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     17060    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19592    C+G   ...n\\138.0.3351.121\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     19876    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     21868    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "üîç CHECKING OLLAMA STATUS\n",
      "========================================\n",
      "‚úÖ Ollama is running\n",
      "Available models:\n",
      "NAME                                     ID              SIZE      MODIFIED     \n",
      "gemma3:4b                                a2af6cc3eb7f    3.3 GB    3 weeks ago     \n",
      "gemma3:1b                                8648f39daa8f    815 MB    3 weeks ago     \n",
      "qwen2.5:14b                              7cdf5a0187d5    9.0 GB    3 weeks ago     \n",
      "qwen3:8b                                 500a1f067a9f    5.2 GB    3 weeks ago     \n",
      "codellama:7b                             8fdf8f752f6e    3.8 GB    2 months ago    \n",
      "gemma3:12b                               f4031aab637d    8.1 GB    4 months ago    \n",
      "GandalfBaum/llama3.1-claude3.7:latest    0d109f949553    4.9 GB    4 months ago    \n",
      "chevalblanc/claude-3-haiku:latest        beb64f2b8dbf    10 GB     4 months ago    \n",
      "incept5/llama3.1-claude:latest           4ba850d59c62    4.7 GB    4 months ago    \n",
      "llama3.3:latest                          a6eb4748fd29    42 GB     4 months ago    \n",
      "deepseek-r1:8b                           28f8fd6cdc67    4.9 GB    4 months ago    \n",
      "llama3-gradient:8b                       5d1398df5b8b    4.7 GB    4 months ago    \n",
      "mistral:latest                           f974a74358d6    4.1 GB    4 months ago    \n",
      "\n",
      "Ollama version: ollama version is 0.9.6\n"
     ]
    }
   ],
   "source": [
    "# Check current system and Ollama configuration\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def check_gpu_availability():\n",
    "    \"\"\"Check if GPU is available and what type\"\"\"\n",
    "    print(\"üîç CHECKING GPU AVAILABILITY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check NVIDIA GPU\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ NVIDIA GPU detected:\")\n",
    "            print(result.stdout)\n",
    "            return \"nvidia\"\n",
    "        else:\n",
    "            print(\"‚ùå nvidia-smi not found or failed\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå nvidia-smi not found\")\n",
    "    \n",
    "    # Check AMD GPU\n",
    "    try:\n",
    "        result = subprocess.run(['rocm-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ AMD GPU (ROCm) detected:\")\n",
    "            print(result.stdout)\n",
    "            return \"amd\"\n",
    "        else:\n",
    "            print(\"‚ùå rocm-smi not found\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå rocm-smi not found\")\n",
    "    \n",
    "    print(\"‚ö†Ô∏è No GPU detected or drivers not installed\")\n",
    "    return None\n",
    "\n",
    "def check_ollama_status():\n",
    "    \"\"\"Check current Ollama configuration\"\"\"\n",
    "    print(\"\\nüîç CHECKING OLLAMA STATUS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Check if Ollama is running\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Ollama is running\")\n",
    "            print(\"Available models:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"‚ùå Ollama not running or accessible\")\n",
    "            \n",
    "        # Check Ollama version\n",
    "        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Ollama version: {result.stdout.strip()}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Ollama not found in PATH\")\n",
    "\n",
    "# Run checks\n",
    "gpu_type = check_gpu_availability()\n",
    "check_ollama_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpu_ubuntu():\n",
    "    \"\"\"Test GPU usage on Ubuntu\"\"\"\n",
    "    print(\"\\nüß™ TESTING GPU ON UBUNTU\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test with a small model\n",
    "    try:\n",
    "        print(\"Pulling test model...\")\n",
    "        result = subprocess.run(['ollama', 'pull', 'gemma3:1b'], \n",
    "                              capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        print(\"Testing inference...\")\n",
    "        result = subprocess.run(['ollama', 'run', 'gemma3:1b', 'Hello world'], \n",
    "                              capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Test successful!\")\n",
    "            print(f\"Response: {result.stdout}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Test failed: {result.stderr}\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ö†Ô∏è Test timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Uncomment to test\n",
    "test_gpu_ubuntu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell to diagnose GPU issues on Ubuntu\n",
    "def diagnose_gpu_ubuntu():\n",
    "    \"\"\"Comprehensive GPU diagnosis for Ubuntu\"\"\"\n",
    "    print(\"üîß COMPREHENSIVE GPU DIAGNOSIS (Ubuntu)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Check hardware detection\n",
    "    print(\"1. HARDWARE DETECTION:\")\n",
    "    print(\"-\" * 25)\n",
    "    try:\n",
    "        result = subprocess.run(['lspci | grep -i nvidia'], shell=True, capture_output=True, text=True)\n",
    "        if result.stdout:\n",
    "            print(\"‚úÖ NVIDIA hardware detected:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"‚ùå No NVIDIA hardware found\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking hardware: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # 2. Check driver installation\n",
    "    print(\"\\n2. DRIVER INSTALLATION:\")\n",
    "    print(\"-\" * 25)\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ NVIDIA drivers installed and working\")\n",
    "            # Extract driver version\n",
    "            lines = result.stdout.split('\\n')\n",
    "            for line in lines:\n",
    "                if 'Driver Version:' in line:\n",
    "                    print(f\"Driver version: {line.strip()}\")\n",
    "                    break\n",
    "        else:\n",
    "            print(\"‚ùå nvidia-smi failed - drivers not properly installed\")\n",
    "            print(\"Install with: sudo apt install nvidia-driver-535\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå nvidia-smi not found - drivers not installed\")\n",
    "        return False\n",
    "    \n",
    "    # 3. Check CUDA installation\n",
    "    print(\"\\n3. CUDA INSTALLATION:\")\n",
    "    print(\"-\" * 25)\n",
    "    try:\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ CUDA toolkit installed\")\n",
    "            # Extract CUDA version\n",
    "            for line in result.stdout.split('\\n'):\n",
    "                if 'release' in line:\n",
    "                    print(f\"CUDA version: {line.strip()}\")\n",
    "                    break\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è CUDA toolkit not found\")\n",
    "            print(\"Install with: sudo apt install nvidia-cuda-toolkit\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è nvcc not found - CUDA toolkit not installed\")\n",
    "        print(\"This might be needed for Ollama GPU support\")\n",
    "    \n",
    "    # 4. Check Ollama GPU environment\n",
    "    print(\"\\n4. OLLAMA GPU ENVIRONMENT:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    gpu_vars = ['OLLAMA_GPU', 'OLLAMA_GPU_LAYERS', 'CUDA_VISIBLE_DEVICES']\n",
    "    for var in gpu_vars:\n",
    "        value = os.environ.get(var, 'Not set')\n",
    "        print(f\"{var}: {value}\")\n",
    "    \n",
    "    # 5. Check Ollama service configuration\n",
    "    print(\"\\n5. OLLAMA SERVICE:\")\n",
    "    print(\"-\" * 25)\n",
    "    try:\n",
    "        # Check if running as service\n",
    "        result = subprocess.run(['systemctl', 'is-active', 'ollama'], capture_output=True, text=True)\n",
    "        if result.stdout.strip() == 'active':\n",
    "            print(\"‚úÖ Ollama running as system service\")\n",
    "            \n",
    "            # Check service environment\n",
    "            result = subprocess.run(['systemctl', 'show', 'ollama', '--property=Environment'], \n",
    "                                  capture_output=True, text=True)\n",
    "            print(f\"Service environment: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Ollama not running as system service\")\n",
    "            \n",
    "        # Check manual process\n",
    "        result = subprocess.run(['pgrep', '-f', 'ollama'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Ollama process found\")\n",
    "        else:\n",
    "            print(\"‚ùå No Ollama process running\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking service: {e}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run diagnosis\n",
    "diagnose_gpu_ubuntu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12666cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell to configure GPU properly\n",
    "def configure_ollama_gpu_ubuntu():\n",
    "    \"\"\"Configure Ollama for GPU on Ubuntu\"\"\"\n",
    "    print(\"\\nüîß CONFIGURING OLLAMA FOR GPU (Ubuntu)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Set environment variables for current session\n",
    "    print(\"1. Setting environment variables...\")\n",
    "    gpu_env = {\n",
    "        'OLLAMA_GPU': '1',\n",
    "        'OLLAMA_GPU_LAYERS': '-1',\n",
    "        'OLLAMA_KEEP_ALIVE': '5m',\n",
    "        'CUDA_VISIBLE_DEVICES': '0'  # Use first GPU\n",
    "    }\n",
    "    \n",
    "    for key, value in gpu_env.items():\n",
    "        os.environ[key] = value\n",
    "        print(f\"   {key}={value}\")\n",
    "    \n",
    "    # 2. Create systemd service override (if running as service)\n",
    "    print(\"\\n2. Creating systemd service override...\")\n",
    "    service_override = \"\"\"[Service]\n",
    "Environment=OLLAMA_GPU=1\n",
    "Environment=OLLAMA_GPU_LAYERS=-1\n",
    "Environment=CUDA_VISIBLE_DEVICES=0\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"   Service override content:\")\n",
    "    print(service_override)\n",
    "    print(\"   To apply manually, run:\")\n",
    "    print(\"   sudo mkdir -p /etc/systemd/system/ollama.service.d/\")\n",
    "    print(\"   sudo tee /etc/systemd/system/ollama.service.d/gpu.conf << EOF\")\n",
    "    print(service_override)\n",
    "    print(\"   EOF\")\n",
    "    print(\"   sudo systemctl daemon-reload\")\n",
    "    print(\"   sudo systemctl restart ollama\")\n",
    "    \n",
    "    # 3. Add to bashrc for persistence\n",
    "    print(\"\\n3. Adding to ~/.bashrc for persistence...\")\n",
    "    bashrc_content = \"\"\"\n",
    "# Ollama GPU configuration\n",
    "export OLLAMA_GPU=1\n",
    "export OLLAMA_GPU_LAYERS=-1\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "\"\"\"\n",
    "    print(\"   Add this to ~/.bashrc:\")\n",
    "    print(bashrc_content)\n",
    "    \n",
    "    return gpu_env\n",
    "\n",
    "# Configure GPU\n",
    "gpu_config = configure_ollama_gpu_ubuntu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f57899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell to restart Ollama with GPU\n",
    "def restart_ollama_with_gpu_ubuntu():\n",
    "    \"\"\"Restart Ollama with GPU configuration\"\"\"\n",
    "    print(\"\\nüîÑ RESTARTING OLLAMA WITH GPU\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Stop any running Ollama processes\n",
    "    print(\"1. Stopping existing Ollama processes...\")\n",
    "    try:\n",
    "        subprocess.run(['pkill', '-f', 'ollama'], capture_output=True)\n",
    "        print(\"   ‚úÖ Stopped existing processes\")\n",
    "    except:\n",
    "        print(\"   ‚ÑπÔ∏è No existing processes found\")\n",
    "    \n",
    "    # Wait a moment\n",
    "    import time\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Start Ollama manually with GPU environment\n",
    "    print(\"2. Starting Ollama with GPU environment...\")\n",
    "    \n",
    "    # Set GPU environment variables\n",
    "    env = os.environ.copy()\n",
    "    env.update({\n",
    "        'OLLAMA_GPU': '1',\n",
    "        'OLLAMA_GPU_LAYERS': '-1',\n",
    "        'CUDA_VISIBLE_DEVICES': '0'\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Start ollama serve in background\n",
    "        print(\"   Starting 'ollama serve' with GPU env...\")\n",
    "        proc = subprocess.Popen(['ollama', 'serve'], env=env)\n",
    "        \n",
    "        # Wait a bit for startup\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Check if it's responding\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            print(\"   ‚úÖ Ollama started successfully with GPU environment\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   ‚ùå Ollama not responding: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error starting Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "# Restart Ollama (uncomment to run)\n",
    "restart_ollama_with_gpu_ubuntu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43837522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell for detailed GPU testing with monitoring\n",
    "def test_gpu_with_monitoring():\n",
    "    \"\"\"Test GPU with real-time monitoring\"\"\"\n",
    "    import threading\n",
    "    import time\n",
    "    \n",
    "    print(\"\\nüß™ TESTING GPU WITH MONITORING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Start GPU monitoring in background\n",
    "    monitoring = True\n",
    "    gpu_activity_detected = False\n",
    "    \n",
    "    def monitor_gpu():\n",
    "        nonlocal gpu_activity_detected, monitoring\n",
    "        print(\"   üìä Starting GPU monitoring...\")\n",
    "        \n",
    "        while monitoring:\n",
    "            try:\n",
    "                result = subprocess.run([\n",
    "                    'nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu',\n",
    "                    '--format=csv,noheader,nounits'\n",
    "                ], capture_output=True, text=True, timeout=2)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    for i, line in enumerate(result.stdout.strip().split('\\n')):\n",
    "                        if line:\n",
    "                            gpu_util, mem_used, mem_total, temp = line.split(', ')\n",
    "                            if int(gpu_util) > 0:\n",
    "                                print(f\"      üî• GPU {i}: {gpu_util}% util, {mem_used}MB/{mem_total}MB, {temp}¬∞C\")\n",
    "                                gpu_activity_detected = True\n",
    "                            elif int(mem_used) > 500:  # Model loaded but not active\n",
    "                                print(f\"      üí§ GPU {i}: Model loaded ({mem_used}MB), waiting for inference...\")\n",
    "                \n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå GPU monitoring error: {e}\")\n",
    "                break\n",
    "    \n",
    "    # Start monitoring thread\n",
    "    monitor_thread = threading.Thread(target=monitor_gpu)\n",
    "    monitor_thread.daemon = True\n",
    "    monitor_thread.start()\n",
    "    \n",
    "    # Test inference with timing\n",
    "    try:\n",
    "        print(\"\\n   üöÄ Testing inference with gemma3:1b...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = subprocess.run([\n",
    "            'ollama', 'run', 'gemma3:1b', \n",
    "            'Extract one key fact from this text: The case was filed in court in 2020.'\n",
    "        ], capture_output=True, text=True, timeout=120)\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Stop monitoring\n",
    "        monitoring = False\n",
    "        time.sleep(1)  # Let monitoring finish\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"\\n   ‚úÖ Inference completed in {inference_time:.2f} seconds\")\n",
    "            print(f\"   üìù Response: {result.stdout[:200]}...\")\n",
    "            \n",
    "            # Determine GPU usage\n",
    "            if gpu_activity_detected:\n",
    "                print(\"   üéØ GPU ACTIVITY DETECTED - GPU acceleration working!\")\n",
    "            elif inference_time < 15:\n",
    "                print(\"   ‚ö° Fast inference but no GPU activity detected - check GPU monitoring\")\n",
    "            else:\n",
    "                print(\"   üêå Slow inference and no GPU activity - likely using CPU\")\n",
    "                \n",
    "            return gpu_activity_detected\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ùå Inference failed: {result.stderr}\")\n",
    "            monitoring = False\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"   ‚ö†Ô∏è Test timed out - definitely using CPU\")\n",
    "        monitoring = False\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Test error: {e}\")\n",
    "        monitoring = False\n",
    "        return False\n",
    "\n",
    "# Run GPU test with monitoring\n",
    "gpu_working = test_gpu_with_monitoring()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
